dnt knw whats wrong with the key... probably the save options ( save as , open & save) had to use "save"  for it to connect in mobaxterm 


sed 's/^echo/#echo/'     *** trying to remeberwhich topic we used this
https://stackoverflow.com/questions/18880024/start-ssh-agent-on-login/18915067#18915067

https://www.bing.com/search?pglt=299&q=what+isthe+correct+path+to+save+my+key+in+linux&cvid=46133917af76474d893e4abb77139521&gs_lcrp=EgRlZGdlKgYIABBFGDkyBggAEEUYOTIHCAEQ6wcYQNIBCTIxMDg3ajBqMagCCLACAQ&FORM=ANNTA1&PC=U531
RSA Keys:
Private key: ~/.ssh/id_rsan
Public key: ~/.ssh/id_rsa.pub
ED25519 Keys:
Private key: ~/.ssh/id_ed25519
Public key: ~/.ssh/id_ed25519.pub
************************************************************************************************

APPLICATION DEPLOYMENT   ***tomcat1&2**
which kind of projects are we managing?
we are managing java based projects - projects that are developed in the java lanaguage
*****mee***  jboss/wildfly supports/used for enterprise application while tomcat supports jar/standalone & war/web applications






GIT2
41.16 ...       TRYING TO CLONE A repository and get this error; 
destination path 'paypal' already exists and is not an empty directory
                                        git clone https://github.com/landmarktechnology/paypal
SOLUTION: https://stackoverflow.com/questions/50361138/destination-path-already-exists-and-is-not-an-empty-directory
This error comes up when you try to clone a repository in a folder which still contains .git folder (Hidden folder).
meeee...... you can  make a new dir (mkdir projects) & change dir (cd projects) then clone the repo andcan further cd into the repo ( cd paypal) & ls to see its content.
     


Ticket :1:53:48 ******* in the remote repo in github, he also created a stage branch from the master branch, then updated the app.java file 
         got the pull req in the master baranch, accepted reQ 
     finally in the local repo, he run the git pull (git pull pay master) command to bring the changes from the remote repo to both the local rep & working env.

***** SOLUTION .....
I ran only git pull pay master & didnt run the same in the dev branch 
Then I tried to run the push command  "git push pay --all"  but it wasnt working
finally i cd into dev branch then run (git pull pay dev)   ****(guess I should have also run git merge dev , but NO, bc the changes was done in the stage branch in the remote repo & in the local repo we already pull it to the master branch, so i )
then "git push pay --all" worked.                                                                                                                                                       only need to pull it to the dev branch in the local repo too

**** THE ERROR *****
$ git push pay --all
To https://github.com/bibiblee/pay
 ! [rejected]        master -> master (non-fast-forward)
error: failed to push some refs to 'https://github.com/bibiblee/pay'
hint: Updates were rejected because the tip of your current branch is behind
hint: its remote counterpart. If you want to integrate the remote changes,
hint: use 'git pull' before pushing again.
hint: See the 'Note about fast-forwards' in 'git push --help' for details.



                                    TO  EDITING A FILE IN THE REMOTE REPO & TO RUN "git push pay --all" IN THE LOCAL REPO   (or mayb i can call it a pull conflict since theres smt called merge conflict)
   ##### I went further to experiment and with the try & error below i realised that ,in remote repo when you edit/add a line in a file(eg file name project) for eg in stage branch, even after confirming the pull REQ in master, u have to switch to the other branch (dev)  
& make sure to update it, if it says the dev branch is behind if nt in the local repo, when u pull, it will say update to date even when its not as seen in (1) below (mayb to clone is better but for now i dnt know)
ALSO, In the local repo, even after pulling in the dev repo, you have switch to the amster branch and pull before you can successfully run  git push pay --all



1) ***** NEXT:                                                                                                            
 In the remote repo, in stage i added a line in app.java 
confirmed pull request in master branch
in the local repo, in master i tried to run " git push pay --all" but it says error 

ekehc@DESKTOP-OQQ2EQ9 MINGW64 ~/paypal (master)
$ git push pay --all
To https://github.com/bibiblee/pay
 ! [rejected]        stage -> stage (non-fast-forward)
error: failed to push some refs to 'https://github.com/bibiblee/pay'
hint: Updates were rejected because a pushed branch tip is behind its remote
hint: counterpart. If you want to integrate the remote changes, use 'git pull'
hint: before pushing again.
hint: See the 'Note about fast-forwards' in 'git push --help' for details.

*** I switched to both dev and stage app.java was not updated 
so in stage branch i pulled, 
then again in master ran " git push pay --all" and it worked 
but when i switched to dev again and checked app.java, it was nt update
i tried to pull it says everything up to datei run "git push pay --all", it says everything up to date
but when i check app.java, its not ?


2)**** NEXT, 
In the remote repo, in dev branch, i see "this branach is 3 commit behind master so i clicked on it and confirmed merge
then in the local repo, i ran git push pay --all but got error

ekehc@DESKTOP-OQQ2EQ9 MINGW64 ~/paypal (dev)
$ git push pay --all
To https://github.com/bibiblee/pay
 ! [rejected]        dev -> dev (fetch first)
error: failed to push some refs to 'https://github.com/bibiblee/pay'
hint: Updates were rejected because the remote contains work that you do not
hint: have locally. This is usually caused by another repository pushing to
hint: the same ref. If you want to integrate the remote changes, use
hint: 'git pull' before pushing again.
hint: See the 'Note about fast-forwards' in 'git push --help' for details


3)*** NEXT
In the local repo, in dev branch, i ran git pull pay dev , & checked the app.java, it was updated
ran git push pay --all, but error again

ekehc@DESKTOP-OQQ2EQ9 MINGW64 ~/paypal (dev)
$  git push pay --all
To https://github.com/bibiblee/pay
 ! [rejected]        master -> master (fetch first)
error: failed to push some refs to 'https://github.com/bibiblee/pay'
hint: Updates were rejected because the remote contains work that you do not
hint: have locally. This is usually caused by another repository pushing to
hint: the same ref. If you want to integrate the remote changes, use
hint: 'git pull' before pushing again.
hint: See the 'Note about fast-forwards' in 'git push --help' for details.


******* NEXT, i switch to master , pulled and ran git push pay --all
and everything was ok

    
        GIT4
MERGE CONFLICT
i had a commit in stage branch and a diff commit in the master branch but when i tried to merge got an error " merge conflct so i had to merge maually




  i saw online **** TO PUSH to all repositories at once
https://stackoverflow.com/questions/6865302/push-local-git-repo-to-new-remote-including-all-branches-and-tags
To push all your branches, use either (replace REMOTE with the name of the remote, for example "origin"):

git push REMOTE '*:*'
git push REMOTE --all
To push all your tags:

git push REMOTE --tags
Finally, I think you can do this all in one command with:

git push REMOTE --mirror
However, in addition --mirror, will also push your remotes, so this might not be exactly what you want.



MAVEN
  Maven-local-repo was accidentally deleted by a Jr. Engineer  
but that was not an issue bc i had earlier backed up the artifacts in the custom maven repo
by configuring settings.xml by pasting " <localRepository>/home/ec2-user/.mlr</localRepository>"
 sudo vi /opt/maven/conf/settings.xml


MAVEN2  
**MEE* There was a change in one of the modules developed by our developers and i was able to carry on with our project by passing the "P flag" to select specific modules to build thereby carry on swiftly with our project& preventing time wasted
how can a specific module be built in maven-enterprise application
for example, developers have done a change in module one 

 by passing the PL flag               
mvn package -P1  for profile name 
eg
mvn package - P1 maven enterprise -web  ***** ie we want build only the maven enterprise-web package
OR
mvn package - P maven Enterprise -ear  ie we want build only the maven enterprise-EAR package


57:00
so We can use the -p flag to do a build
by changing the profile name

57:45
if there are many modules but cahnge in only one module 
we build by choosing that particular profile
by using the ? web flag


                                                                     TOMCAT
engr Amstrong said permission denied means you dnt have the right key permission, its not enough to have the key to the server you are trying to access, you should also hv the right permission
said we can use the same key for diff instances/servers but if the key has issue sthen it means all the servers will be affected so thats will they say use diff keys for diff servers

i resolved issues of 
***********using unauthorized keys or wrong key format     eg ed25519 instead of RSA  
****** wrong installation or configuration
*** Not installing in the right directory                                 Domain name system
****** unknown host issues  (ie Problems with the network connection or the DNS server itself can prevent it from resolving the hostname.)



TRANSFERRING FILES FROM MAVEN TO TOMCAT  (SOLVING PASSWD ISSUE & KEY FORMAT ISSUE) 

1)USING PASSWD                 ****unresolved...
 DePLOYING FROM SERVER TO SERVER (Maven to tomcat)    **** i was not successful got error..   error in libcrypto
                                                                  &  scp -i sea.pem target/tesla.war ec2-user@18.223.16.218:/opt/tomcat9/webapps/app.war
                                                                     ec2-user@18.223.16.218: Permission denied (publickey,gssapi-keyex,gssapi-with-mic) scp: Connection closed

2)USING SSH KEY    *** Resolved
[ec2-user@maven ~]$ ssh-keygen
generating public/private ed25519 key pair    ******** but later on i got error with the scp command to transfer the file bc this key is the wrong key format  
got this error; "error in libcrypto" ....  *** this mean wrong key format" ie "openSSH, eg id_ed25519" private key format instead of "rsa, eg id_rsa"
https://serverfault.com/questions/939909/ssh-keygen-does-not-create-rsa-private-key
 Solution:
1) download new rsa key =  ssh-keygen -m PEM (-t rsa)   **** i dnt think "-t rsa" is necescssary
2)keys with OpenSSH private key format can be converted using ssh-keygen utility to the old PEM format.
ssh-keygen -p -m PEM -f ~/.ssh/id_rsa     **** I ran this but it only requested to change pass phrase
OR
cd ~/.ssh
ssh-keygen -e -m PEM -f id_rsa > id_rsa.pub.pem      **** This said unsupported format

** applying option1 above)
[ec2-user@maven ~]$ ssh-keygen -m PEM -t rsa -b 4096       ***  ssh-keygen -m PEM -t rsa... but i dnt think (-t rsa) is necesary, -b 4096: Sets the key size to 4096 bits for stronger encryption, -t rsa: Specifies the RSA algorithm.
Generating public/private rsa key pair.
Enter file in which to save the key (/home/ec2-user/.ssh/id_rsa):          *** normally just click enter but to use with scp command ( ~/.ssh/id_rsa.pub )
Enter passphrase for "/home/ec2-user/.ssh/id_rsa" (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/ec2-user/.ssh/id_rsa
Your public key has been saved in /home/ec2-user/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:VLwU3yUJmjzOK3Z31fdk76c+rTw4cFCOJVVzRXMjm+E ec2-user@maven
The key's randomart image is:
+---[RSA 3072]----+
|         .ooo=+=B|
|         o+++.*=+|
|        ..=B.E.  |
|       . o+..   .|
|        S o.    *|
|          ...  ++|
|        o oo....o|
|       . o .ooo.o|
|             o==o|
+----[SHA256]-----+
[ec2-user@maven ~]$ cat /home/ec2-user/.ssh/id_rsa.pub
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDBBzOi2Ntn7I5XAnj1vLA9pcDD2R3vP2zQqmICC5qndLPsJrfgqjrQc+cejldfIeAMHj0c8I2FUTNzB/NcXZFz8vWLQGPtippC6+Uc/2jLmoa57giBUSkxKnPeOtm4JncLF1blBJZSD+QdbfIeasFiTVvGos01bUpEUe/+CDGgMXPd6RANuuPPl045kDhZ0YJUEQJX56PR
STQPuL1O48eZvF5jXgTS6vxnVfU/pNB/x0y++0YrV6DjE/OXE1ZqUmK97P5HyG25lEPMwQTtEoKjNqCw8D/GZvgIOG2QxQyEKyGUiiITXc/byZmQjXEkygdv0paq+y/aOD0XCyAfd9Kci82syk5HiJwdF6byiPTuSR58GBmdU3OGP5tpLQ3TSb4pxB4KRpM9xVXV/Or/2RKYQ/stB+jkr3JCHH2rYd5aP3reynIcW/CqIqK7YI
V6PQhmMEFikurVV/iTadQmiYM/sZ79Q2TTa4DQUFiDFSu4eeJCf8+Po0lkVef05xC5ULk= ec2-user@maven


  *** then tried to transfer and i didnt get "error in libcrypto)
[ec2-user@maven mwa]$ scp -i  ~/.ssh/id_rsa.pub target/tesla.war ec2-user@18.223.16.218:/opt/tomcat9/webapps/app.war
ec2-user@18.223.16.218: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
scp: Connection closed
*********** tried a whole day but nothing worked ; creted new maven & tomcat servers, made sure i used the same keys but nothing worked, just cudnt transfer with both passwd and ssh key


Converting keys with opensll format
https://phoenixnap.com/kb/ssh-permission-denied-publickey
https://stackoverflow.com/questions/36300446/ssh-permission-denied-publickey-gssapi-with-mic
The ssh-copy-id command transfers the public key to a remote server, enabling passwordless login. Syntax
ssh-copy-id -i ~/.ssh/id_rsa.pub ec2-user@maven      ***** ran this but permission denied
*** i dnt even think thats the issue bc each time i run the scp" command i get this ( as seen below): and it means that the key was added to my authorized keys as seen below as well from my "/sshd_config" files; i bliv its the format of my key   
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes                                                                                           my computer changes it once it saves it (i think the file shud be white not blue)
Warning: Permanently added '3.150.123.124' (ED25519) to the list of known hosts.
ec2-use@3.150.123.124: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
scp: Connection closed

/sshd_config; file
#HostKey /etc/ssh/ssh_host_rsa_key
#HostKey /etc/ssh/ssh_host_ecdsa_key
#HostKey /etc/ssh/ssh_host_ed25519_key
*****************************************************************************************************
Resolving "Authentication Agent Not Found" Error
https://www.bing.com/search?q=Could+not+open+a+connection+to+your+authentication+agent.&cvid

[ec2-user@maven ~]$ ssh-add ~/.ssh/id_ed25519              ************ tried to add but error
Could not open a connection to your authentication agent.
[ec2-user@maven ~]$ Could not open a connection to your authentication agent.
-bash: Could: command not found
[ec2-user@maven ~]$ ssh-agent -s
SSH_AUTH_SOCK=/tmp/ssh-XXXXXXIdAK0E/agent.104674; export SSH_AUTH_SOCK;
SSH_AGENT_PID=104675; export SSH_AGENT_PID;
echo Agent pid 104675;


https://www.bing.com/search?q=what+does+the+ssh-copy-id+command+do&qs
SSH ID refers to the SSH key pair used for secure authentication in SSH (Secure Shell) connections. It consists of a private key (kept secret) and a public key (shared with remote servers). These keys enable passwordless login by verifying
the user's identity cryptographically.
The ssh-copy-id command simplifies the process of setting up public key authentication by copying the public key to a remote server's ~/.ssh/authorized_keys file. This eliminates the need to manually edit files and ensures proper permissions.

Generating SSH Keys

To create an SSH key pair, use the ssh-keygen command:

ssh-keygen -t rsa -b 4096 -C "your_email@example.com"
-t rsa: Specifies the RSA algorithm.

-b 4096: Sets the key size to 4096 bits for stronger encryption.

-C: Adds a comment (e.g., your email) for identification.




https://www.bing.com/search?q=what+is+ssh+add+command&cvid
Using ssh-add in Linux
The ssh-add command is used to add private key identities to the authentication agent, ssh-agent. This allows you to manage your SSH keys more efficiently by loading them into memory, so you don't have to enter the passphrase every time 
you use the key.
Common Options for ssh-add

-l: Lists fingerprints of all identities currently represented by the agent. ssh-add -l

-L: Lists public key parameters of all identities currently represented by the agent. ssh-add -L

-d: Removes identities from the agent. If run without arguments, it removes the keys for the default identities. ssh-add -d

-D: Deletes all identities from the agent. ssh-add -D

-t life: Sets a maximum lifetime when adding identities to an agent. The lifetime may be specified in seconds or in a time format. ssh-add -t 3600 ~/.ssh/id_ed25519
  



sed 's/^echo/#echo/'     *** trying to remeberwhich topic we used this
https://stackoverflow.com/questions/18880024/start-ssh-agent-on-login/18915067#18915067




                       SONAR
when your trying to execute code quality report ie code quality analysis, it could result in some error messages like 
http error code 400 errors =client side
400/401/402/403
not authorized , when entering the wrong passwd
please check the properties sonar.login and sonar.paaword.


******** from bootcamp Dec26 
Interview :

 ANOTHER PROBLEM     ( ROLL BACKS, due to no code quality software )
we had a serious issue with one of our client where we managed a fintech appl, we kept having multiple rollbacks 
we will deploy the appl but the client will call back informing us that smt was wrong, there were multiple rolbacks , i was tasked to invetsigate what was the problem bc 
that doesnt give a good image abt our software cy, so i began researching, the build was successful, deployment has happend but thers still rollback
then i realise that in our pipeline, something was missing on the pipeline, a code quality checker, there was no tool that was been use to check for code quality to ensure 
that the applicatons we rae deploying ra deployed form quality sofware that the no. of testcases that developers are writing are commesorat to the no. of code that they are 
developer , so when i realised that was the problem , i immediately explained that to our team in one of our sprint review meeting and i instantly recommended that we shud
use sonarqube bc in my research that sonarqube will be a very good code quality checker and that was the problem we were facing and the recommendation was to use sonarqube 
so we use sonarqube .

and i was tasked again to know how sonarqube can be fully deployed in our env  and thats how i came up with the white paper on sonarqube setup in our env and frm that paper
i learned the steps on how to get sonarqube installed , whcih is very simple, u go to the sonarqube site, u download the latest version of the sonqrqube software via the wget
command and extract it and start sonar ,  sonar.sh start 
sonar starts,and once that is done we can now access it via the browser and frm ther we can now modify our project fie under the properties tag, we modify it with our sonarqube
aerver login credential details and frm ther sonarqube is now part of our pipeline, jenkins integrate with sonarqube for code quality analysis , we introduce sonarqube , we
strated setting up our own benchmark for code quality and as such today we dnt hv huge incidents where we hv to be rolling back on deployments bc before any deployment is done
we expect that the codes hv been qualified , its deployed from quality software, that is the problem we hv faced in our env and thats how we fixed it 


   
   WHAT HAVE YOU DONE THAT YOU ARE PROUD OF 


41:55
some  of our developers are writing code without intergrity analysis tool to check for code quality, so code quality wasnt integrated in the project so in our review meeting 
i recommended that inorder to avoid a lot of bugs in the code and a lot of roll back  in deployment, we need to add code quality in our pipeline 
i adviced to use sonarqube and we had some working sessions where i was able to school other members of our team on how useful sonarqube will be once integrated in our
project and thats how it was applied and when it was applied initially developers were not happy because they had to do a lot of fixes whenevr the committed poor quality code
but eventually developers became so happy because they are very proud of their job.



i worked in a project but no code quality was been integrated in the project and i observed that we were having a lot issues because whenevr developers commit their code &
the code is developed to production we were having a lot of failures, code breaks and bugs and we had to always rollback to fix issues 
i had adviced that we should add a tool to check for code quality and in the process i recommended sonarqube, i had working sessions with the team where in general training
was done and i was priviledged to be the one teaching members of our team on how to use sonarqube and that happened effectively but initially developers were not happy. 




 NEXUS

1.01..   CONFIGURING THE CONFIGURATION FILE 
ensuring that nexus is run by the nexus user  by runnin echo runa s user nexus  I this is bc the nexus file is owned by root user and we need to change that

400 err0r; repository does not allow updating assets  (so resolved by allowing redploy in the in deployment policy; repositoring settings) or vi in pom.xml and change version from "o.1" to "0.2"
401 ; unauthorized -- authentication failed.


QUESTION  .... ill change this  question to a problem ,initially when we are raedy for deployment, i allowed redeploy without updating/creating the new version and bc of that we couldnt roll back bc we didnt have the previous version  
is there a situation that require/necessitate the nexus repo not to allow update???? 
like when you had to changing the setting from disable to enable, why is that setting necessary even??? 
ANS 
You know we hv releasssses and snapnap
releases is when developerrs are done with the job, it has been tested and ready for production, most of the time if we are goin to deploy from there then we need to create a new vesrion bc we are ready for .deployment
thefore not allowing update means that if ur trying to redeploy or its like you hvv version 1.1 runnin, then you should go and crEate version 1.2 bc now you are set to deploy to production and if thers a problem you can roll back to 
version 1.1 but if you IF You dnt choose that option it means when you run mvn deploy, you upload and just update the same asset and in that case you cant rool back if thers a problem



USING THE WRONG REPOSITORY 
eg Using releases instead of snapshot



 From bootcamp Dec26 
58:00*********************** NEXUS
    i was tasked to upload some artifacts into the release repository and i realised for some reason i kept getting build fail and when i checked the logs i realised i was using 
the wrong version in my pom.xml, instaed of ensuring that the versioning is tagged to releases it was tagged to snapshot, so i quickly went to my pom.xml modified the version
used the corret version which was snapshot and once i had to rebuld again via my pipeline that failure never existed because it was now uploading artifacts into the snapshot
repository that was expected.



JENKINS

 . Troubleshooting jenkins build related problems:
     -- permissions issues, Unauthorized [4** error codes] , i manage the credentials such taht jenkins can be authenicated by github or nexus accordingly
                     e.g jenkins trying to connect with nexus/sonarqube/github

     -- Jenkins [GitHub/sonarqube/nexus/tomcat] 

     -- Jenkins agent/slave failing to connect to the master
        [java not install], wrong java version, AUTHENTICATION 

 What problems have you faced using/applying Jenkins in your Environment??
     1. builds failing
     2. jenkins service failing start so we need the complier, check if [java-11-openjdk] is installed
     3. slave/agent failing to connect to the master [bc of executables] mayb java is nt installed or trying to run multiple jobs at once
jenkins master slave architecture
to run multiple jobs at once,  we may have one jenkins master server connecting to to other jenkins slaves via executioners
      4. integration isues, integration failing
     3. firewall issues = ports [8080 / 8899] not open





AWS3
 Explain your experience with AWS Storage services
thers a service in aws called Lambda functions:

If we have upto  75% volume usage increase vol by 20%   
  
in my envi in one of my projects, i was able to use the lamba function, which was written to expand my project by 20% whenevr usage is 75% 
### 1:54so if  df -h / 100G/75G  ie avail 100 and used 75, increase by 20% --  so we then hv 120G


DISASTER RECOVERY
 we can also  write a lamba function , lamba function has what is called diasaster recover
if u work for landmark and we hv aws account1, & aws account2. in account1 we hv a database servers running ,  for account 2 (diaster recovery account, for our database servers backup)
because diaster can occur , eg if someelse is able to access d email&paswd & log into aws wit root access, thers a possiblity of a disaster bc dats a serious issue bc they wil hv 
access to all my servers bt wit account 1 & account 2, our disaster recovery is accont 2, wit our database serververs running in account 1, we back the databases in accout2
by creating a snapshot lifecycle policy using Lambda  which
create sonapshots, wich is a replica of account1 from our databases from account1 in account2 cross region , so we ar able to recover frm account2

1.58.00   So what we do is We created a snapshot lifecycle policy using Lambda  which
creates sonapshots from our databases from account1 in account2
snapshot is a replica of your database, its a backup
therefore if account1 is hacked, we already hv everything in account2 so we are able to recover from account2 
SO WIT THE HELP OF LAMBDA & snapshot lifecycle policy, this entire process was automated
so am proud of this project





AWS 7&8

MY TS EXPERIENCE: I tried installing tomcat with user script ie pasting
installation script while launching an inatance but it wasnt working, and the target grp keptindicating unhealthy bc d https link to install tomcat wasnt
the latest version, so i had to google tomcat download and tcopy the correct https link  
 https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.83/bin/apache-tomcat-9.0.83.zip



DOCKER 5&6
QUESTION
WHAT problem have you encountered in ur project before
when i joined my team, they had particularly entry point and CMD instructions that were written uisng shell form for that reason they were nt running as 
the parent process so they were killed unnoticed

WHAT IS SOME OF THE PROBLEMS U HAVE FCAED IN UR ENVR RECENTLY AS A DEVOPS ENGR.
we hv some of our docker files which we hv used to build and deploy applications and we had issues wher  applications were failing, endusers were complaining,
so i had to TS , ha sto check hw the docker files were written and i realized that our CMD were been passed in shell form and as a result, the applications 
process were been killed unnoticed and i quickly modify the dockerfile, changed it from shell form to executable form such that the application will run as a
parent process. 


QUESTION
do you summarize evrything u nid to define in the dockerfile, inside the application file already, is that why u v only 2lines in the docker file; FROM & COPY ?
ANS:  3:23:45
FOR e.g in doockerhub, this tomcat base image is running on an OS,, Click on the tomcat n select tag, we can search for apline,ubuntu,centos , e.g we search 
alpine, we can see a number of alpine images and for each we can see the tomcat is running on an alpine base image and this means that this wil cm only wit 
the lib that are sufficient for the said tomcat aplication to run althoug a normal OS  wil cm wit all lib and all thats needed
secondly:  if u go tru all our docker files, u wil see docker files with diff number of lines as well, u will see docker files wit more oda configuration also
in dockerhub in overview of tomcat, click on any of the files n we see the content like FROM, env variables

FROM amazoncorretto:8-al2-jdk

ENV CATALINA_HOME /usr/local/tomcat     .. tomcat home dir  defined as : usr/local/tomcat  
ENV PATH $CATALINA_HOME/bin:$PATH   ... path to start tomcat, in d tomcat home dir the command to start tomcat is found in bin, so $CATALINA_HOME/bin ie d path to d commands 
RUN mkdir -p "$CATALINA_HOME"     .... then this home dir is been created "$CATALINA_HOME" wich means  /usr/local/tomcat is being created
WORKDIR $CATALINA_HOME    .....   and the workdir is tomcat

######### so ders no problem here, the main stuff is to undertsnd the principle, onCe i understnad them, i can apply them to multiple environments wil be no 
big deal

3:28:00
######### like someone is saying sonarqube image has a good explanation abt it, so he went to sonaqure page .. 3.27.33
 he said there are going to be diff OS,u ar going to choose the OS u wnt to 
choose bt this sonarsoftware is running on a base image OS 




                                      KUBERNETES

                   ****Kub17:  run kubectl get pod, first before running kubectl get logs tdapp)




KUB 10&11
 troubleshooting issues from k8s setup/configuration or installation. ,, so v been able to trpubleshoot issues like trying to stup my kubeadm n its failing,
or cant add worker node to master node, v had issues like tht n hv been able to fix it by creatin a token in the master runing in the workernode n having them
join the cluster



KUB 15
    TROUBLESHOOTING KOPS INSTALLATION ERROR
when that happens we nid to export the kube config file again(i guess thats 10b in the inatallation file we used in kops installation): run kops export kubecfg $NAME --admin
now run
kubectl get all and we hv our cl8 running





kUB 16

NICE explantion for :  .. 2) Explain your experience in kubernetes?? .......  18:00
my question: when is hosted zones created

4:00     ................... watch it
you would be ask what you have been able to use k8 to solve in your environment
ITS A Common experience when it comes to k8, either they want u to explain your experience or they just want to discus some problems u may v encountered using k8 and hw you
were able to fix those problem. also when u apply k8 thersa lot of xperience u gather n der a lot of problem uve been able to solve, 
so the xpectation is for you to be able to state clearly the kind of problems uv been able to fix applying k8 in ur env



3)What problems have you encountered applying kubernetes in environment?
we kn the k8 mg8 containerized appl, if ders a problem in the app 
How can you troubleshoot application related issues in kubernetes??
our appl are running in pods so if ders a problem, we v a few things we can do , we also saw dat when it comes to docker 
now if u look at k8 and docker, if ders a problem in doker in our container we can run the below , also for k8 we run the command below
kubernetes manages containerised applications:
  kubernetes                                                    docker    :
  kubectl get pods                                          docker ps or docker ps -a      
  kubectl describe pods podName                             docker inspect containerName/ID  
  kubectl get svc/service  
  kubectl get endpoint/ep  
  kubectl logs podName                                        docker logs containerName/ID 
  kubectl exec podName  
exec to execute command inside the pod ..... 24:00                       docker exec containerName/id 
  kubectl top podName                                            docker stats/top   
we can also pipe n grep for errors
kubectl logs app-(app name) | gre errors



#############more xplanation  .................13:05
kubectl get po 
he tried to access his kops cl8 bt it didnt wrk so  he said 
when we v dis rror msg   to get it fixed we go to our kops cl8 setup n export the kube config file that is the file we used for kops inatallation
kops export kubecfg $NAME  --admin
then run 
kubectl get pod 
so if ders a problmr in our appl we can run that and we can also describe the pod 
kubectl describe po (pod name)


         IN K8 appl IS EXPOSED USING SERVICE DISCOVERY
 service discovery:
    webAppservice ---> 7webAppPods
lets assume i hv 7 webApppods
THEREfore i will create a webapp service wich will lb traffic to the 7pods
############for me to knw my webappservice is routing traffic to all of this pod i can run
kubectl get svc
impotrantly we can get the endpoints/ ep

#############   also  .................................................NICE .........................################################18.00
we run kubectl get deploy  to see the app we deployed
kubectl get ep 
WE SEE THE CURRENT EP is 2 
kubectl scale deply (app name) ----- replica 4
we run kubectl get ep again 
now we see Ep is 4
############### there fore we can rightly say that the service is routing traffic as expected
so i can be abl rto xpalin this in real time bc it is working as expected
#########we can aslo check kubestl get svc 
to see if we can accesss the app and also check online

********** we can also get logs for the appl
kubectl logs app-(app name) | gre errors
if ders an error the error msg is able to tell us 


we cud also have these errors ............27:00       our images are found in dockerhub or amazonECR
WE can run kubectl get events
in the process of pulling the image creating the container and
starting the container there could be a problem in btw this proceses
its either we are pulling the wrong image
we cud hv the right image but wrong authentication with the image pull sercert when pulling frm a private repo


  COMMON ERROR IN KUBERNETES
common errors in kubernetes:  
1)  pods are in pending  state 
  pullimage error / imagepulloff   
   WRONG  image: mylandmarktech/hello:20     
          mylandmarktech/hello:22   
2) authenticationerror:
  mylandmarktech/nodejs-fe-app:2  
  imagePullSecrets:
  - dockerhubcred
3)KOPS CLUSTER DEPLOYMENT ISSUES: like
  IAM user not authorise     ............. we nid this to create a kops cl8 in aws
    create an IAM role with required permissions/policy  
       VPCFULLACCESS/EC2FULLACCESS/S3FULLACCESS and attached
       to the kops control server 
    Attached required IAM policies to the IAM user/group and   
    run aws configure  

**meee, copied from below...
#crashloopbackoff 
  # pod repeatedly crashing
#ReadinessProbeFailed or 
#LivenessProbeFailed
#Insufficient resources
#persistent volume mount failures
#InvalidNamespace
#cni plugin error



    VPC LIMIT
  #vpc limit reached / :............ by default when u run a vpc tier account, the max numbe rof vpc u can crtae in one regionis 5, so when u reach this u can request#
    5 vpc running in production  
    Request additional vpc quota from aws  
  deploying a kops cluster [ a new vpc,s3, ec2-instances, ELB, ASG ]


                          NODE NOT READY (MG8 CLUSTER)

node notReady: when u deploy self mg8 cl8, u hv to deploy ur node , u hv to mk sure that ur nodes are ready bt  mg8 cl8 like  EKS we dnt worry abt d node nt being ready
  kops/kubeadm = kubernetes CNI [flannel, weave, calico] is not deployed our node will nt be ready
NODES not ready cud be bc of k8 cli, k8 netwrk interfaces nt installed 
  ensure that the kubelet service is running [systemctl start kubelet]   if not node wil nt be ready    
  node is stopped/terminated in aws , in this case restart it 
 OR  Place the nodes behind an ASG with a ELB for health check  
 *** use kops/eks over kubeadm    to manage such problem
#crashloopbackoff 
  # pod repeatedly crashing
#ReadinessProbeFailed or 
#LivenessProbeFailed
#Insufficient resources
#persistent volume mount failures
#InvalidNamespace
#cni plugin error

                                 ERROR CONVERTING YAML TO JSON
 
# error converting YAML to json
=============================

k8 manifest file can be written in json or yaml
bt when u run kubectl apply, it has to use the file 
e.g   ............. this is an error bc we dnt hv any kind as devops
kind: Devops
apiVersion: PODS    
metadata  
spec

kubectl api-resources | grep devops  
but no result
 bt grep for pod n we v result
kubectl api-resources | grep i pod


                                        COMMUNICATION ERROR
# what about issues when all pods are running but the applications are unable to communicate?
this cud be as a result of misconfigurations.
for example: for external access we use a nodeport service type
pod.yml   
-------
kind: Pod    
apiVersion: v1   
metadata:  
   name: myapp  
   labels:
      tier: be  
spec:
   containers:
   - name: myapp   
     image: mylandmarktech/hello   
     ports:
     - containerPort: 80

svc.yml  
-------
kind: Service
apiVersion: v1
metadata:
  name: websvc
  namespace: dev
  labels:
    app: web
spec:
  type: NodePort # ClusterIP
  selector:
    app: myapp # labels= tier:be
  ports:
  - port: 80
  - targetPort: 8080  # containerPort80/8080
  - nodePort: 40200   # 30000 - 32676
we dnt hv port range of 40200, its out of our service range

################################ so if u go for an interview and u are asked what is wrong with this script, why cant we route traffic what cud be the problem
 its just misconfiguration   , also our target port has to be the same with container port, also my selector has to match to my labels

*********** so if am given a scenario like this i identify problems and fixed them

so if u are asked did u encounter any problem using k8 ????
the answe is yes
did u suceesd in fixing them ?
most of the time yes





KUB 17 ELF
                               TO ACCESS APPLICATION THAT IS RUNNING

1) WHEN WE DONT HAVE MANY APPLICATION:
    ***** run kubectl get pod, first before running kubectl get logs tdapp)
when u want to access an applictaion that is running, in k8 we can check the logs
kubectl get pod
we see the pods that are running in the defaulf dev space
NAME                        READY     STATUS
hello-app-6hztm              1/1         running
tdapp-759f7c6687              1/1         running


we can check the logs of each of the appl
we can describe the pod
kubectl describe  tdapp-759f7c6687     
OR check/get the logs  of the pod
kubectl logs tdapp-759f7c6687     
we can see the appl was deployed using the spring booth frame work
if u look at the appl, you can visualize the logs
********************if ders anytin wrong with your appl you can check the logs it will tell whats happening

 2) WHEN WE HAVE MULTIPLE APPLICATIONS :  kubectl logs cluster-autoscaler-6cf74d69f7 -n kube-system    (kubectl logs app-(app name) | gre errors)                     
but when u have multiple appl and ther are issues,
kubectl get pod -A        * TO get all the pods in all the name spaces
we see all the pods running
NAMESPACE            NAME    
apm                 grafana
apm                 promethus
kube-system         cluster-autoscaler-6cf74d69f7
grafana pod is running

 so if we hv 100 pods we cant be running kubectl for all of the pods, that will be a tidious task
kubectl logs cluster-autoscaler-6cf74d69f7
ERROR form server not found clustr-autoscaler      ************************** this is bc we didnt check in the correct namespace (we nid to pass the namespace)
kubectl logs cluster-autoscaler-6cf74d69f7 -n kube-system
the logs are successfully generated 

*******************************************************************************************




Linkedin
Sudheer MedaramettlaSudheer Medaramettla

 Production Issues Every DevOps Engineer Faces (and How We Solve Them) üö®


üî• 1. High CPU / Memory Spikes

Symptoms: Pods or EC2s hitting 90‚Äì100% CPU, applications slowing down.

Root Causes: Memory leaks, unoptimized DB queries, infinite loops, traffic bursts.

How We Solve It:

Use monitoring tools (Prometheus, Grafana, CloudWatch).

Identify heavy processes (kubectl top pods, htop).

Restart or scale services, then work with devs to fix memory leaks.

Add auto-scaling rules for resilience.




üåê 2. DNS / Load Balancer Misconfigurations

Symptoms: Services healthy but users still see downtime.

Root Causes: Incorrect DNS TTL, failed health checks, misconfigured ALB/NGINX.

How We Solve It:

Validate DNS resolution (dig, nslookup).

Rollback recent LB/DNS changes.

Fix health checks, shorten TTL for faster recovery.

üì¶ 3. Deployment Failures / Broken Releases

Symptoms: New release goes live, app immediately crashes.

Root Causes: Missing environment variables, broken Docker image, dependency mismatches.

How We Solve It:

Rollback instantly with Helm/K8s.

Debug container logs (kubectl logs).

Use blue-green or canary deployments to limit blast radius.

Automate pre-deployment checks.


üîë 4. Secret & Credential Leaks

Symptoms: Keys in GitHub or logs, causing potential security breaches.

Root Causes: Poor secret management practices.

How We Solve It:

Rotate leaked credentials immediately.

Use Vault / AWS Secrets Manager / SSM Parameter Store.

Integrate tools like trufflehog, git-secrets into pipelines.

üìâ 5. Database Performance & Connection Issues

Symptoms: ‚ÄúToo many connections‚Äù, query latency spikes.

Root Causes: Unoptimized queries, missing indexes, poor connection pooling.

How We Solve It:

Monitor DB metrics.

Increase connection limits, add caching (Redis).

Scale DB vertically/horizontally.

Optimize queries with dev team.


‚ö° 6. CI/CD Pipeline Failures

Symptoms: Builds suddenly failing, blocking deployments.

Root Causes: Dependency updates, misconfigurations, low runner resources.

How We Solve It:

Debug logs and retry.

Clear build caches.

Add notifications (Slack/Teams).

Improve pipeline observability & resilience.


üîí 7. Security Vulnerabilities in Production

Symptoms: Scans reveal critical CVEs in containers or base images.

Root Causes: Outdated base images, missing patches.

How We Solve It:

Patch/rebuild Docker images.

Use lightweight & secure base images (distroless, alpine).

Automate scanning with Snyk, Trivy, Aqua.


üì° 8. Network Latency & Cross-Region Issues

Symptoms: Slow service-to-service communication.

Root Causes: Cross-region calls, misconfigured VPC peering, API throttling.

How We Solve It:

Deploy services in same region or use edge caching.

Implement VPC endpoints/PrivateLink.

Add retries and circuit breakers for reliability.



