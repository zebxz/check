1. Terraform   -- Provisioning
-------------------------------
1) Infrastructure as Code (IaC)
o Understand Problems with Traditional way of Managing Infrastructure
    - GUI/console   
    - commands  
o How IaC with Terraform Solves them

o Infrastructure as Code (IaC) VS Configuration Management
o Install Tools on Mac OS, Linux OS and Windows OS
o Command Basics
    terraform init
    terraform plan  
    terraform validate   
    terraform apply   
    terraform destroy   
    terraform import   
    terraform fmt 
    terraform show/ cat terraformstate.tf    
       remote backend like s3 
       simon / 
       mary / 
       kelvin     


o HCL Language Syntax

2) Terraform Top Level Blocks
o Fundamental Blocks
o terraform block
o providers block
     aws / azure / github / local  
o resources block
o Variable Blocks
o Input Variables
o Output Values
o Local Values
o Referencing Block
o Data sources Block
o Modules Block

 AWS:
   ec2-instances  \ VPC / EKS  / S3  / ETC.  
   installing terraform on windows and linux systems 
    define scope / 
    init / 
    writing & modifying terraform scripts [ vars.tf main.tf modules]   


2. Ansible  is a Configuration Management tool  

    appServers = 50  
    webservers = 40  
    dbservers  = 44 
    kubernetes = 5  

    deployment of applications in the 50 appServers 
    commissioning the 50 appServers 
    securing the  50 appServers
    installing tomcat in the 50 appServers
  tasks that ansible can perform/run on its hosts:
    FileMGT  
    userMGT  
    deployment  
    securityMGT  
    system monitoring  
    patching  
    packageMGT  
 How to perform the tasks and ansible workflow:
   modules 
   playbooks:
     variables  
     plays   
     tasks  
     handlers   
     modules [ yum / copy / service / template / apt / package / shell ] 
               command / setup / systemd / 
     roles    
   roles    
   inventory/host file  
     [ appServer ]
       10.10.0.55
       10.10.0.91
     [ dbServer ]
       10.10.0.77
       10.10.0.100

     [ k8s ]
       10.10.0.10
       10.10.0.30

app.yml  
=======
host

destroy options :
  terraform destroy --auto-approve  
  terraform destroy --target local_file.test      
  terraform destroy --target aws_instance.web         
How have you applied terraform in your environment/landmark??? 
***********************************************************************************************


    SO FAR;
I understand that in deploying an application that requires endusers to enter some information ,then the deployment requires 2 application & this is will
involve 2steps
STEP1) DEPLOYING THE 2 APPLICATIONS
a)the main application which is stateless bc no info is been entered into it therefor it doesnt require a piece of storage to help retain/remember such info
 b) A database where the information entered by endusers will be captured, this requires to create a piece of storage to help retain/remember such info

STEP2) CREATING A VOL/PIECE OF STORAGE FOR THE DATABASE
The types of vol includes:
awsElasticBlockStore, azureDisk, azureFile, configMap, emptyDir gcePersistentDisk
gitRepo (deprecated), hostPath, nfs, persistentVolumeClaim, secret
Docker 8&9
Docker Volumes
Volumes stored data in a part of the host filesystem managed by docker

    THe type of vol been created can either be persistent or non persistent
NON PERSISTENT VOL: eg bindmount
non persistent means it can be accidentally removed/deleted sudo rm -rf because its not managed by the application process eg docker process
 30:23 Non-Docker processes on the Docker host can modify, Bind Mounts volumes at any time unnoticed so we nid to introduce persistence vol 
PERSISTENT VOL:
 most of the types of vol are persistent including docker vol
However in Kubernetes, its recommended to create all volumes with a PVC  ** eg for NFS with PVC, online i saw its called NFS based PVC..
so far, i understand that when deploying an application that includes a database, its best to use the volume concept provided by the application eg docker,
use docker vol, for kubernetes; use kubernetes volume

**online**
A persistent volume is a storage resource in a cluster that has a lifecycle independent of any single pod. It provides persistent storage for stateful 
applications, ensuring that data is not lost even if the pods using it are deleted or rescheduled. 
Administrators provision persistent volumes, which can be backed by various types of physical storage like local drives or cloud-based systems. 


  SCALING
based on how we seperately deployed the 2 application; ie the main application with a nodeport service & the mongo db application with a clusterip service
i bliv we scale only the data base application, bc its the application concerned with inputing information to be stored, & thats why a piece of storage is
attached which is associated with how much storage space it has & hw much is been consumed. 

DOCKER vs KUBERNETES
1)In docker we created networks, & ensured the appl are created in the same ntwrk to ensure communication 
BUT
In kubernetes, its uses DNS resolution ie service discovery eg clusterip , nodeport, loadbalancer service.
2)Docker uses just docker vol concept WHILE  kubernetes uses kubenretes vol concept with PVC
3)In kubernetes we can scale using controler mangers & we can also auto scale using hPA/CAS/VPA BUT in docker swamp we can scale, for now nt sure we can autoscale
************************************************************************************************************************************************************


KUBERNETEES CLUSTER, EXAMPLES OF SERVICE DISCOVERY, DERVICE DISCOVERY & ENDPOINTS, API VERSION, CONTROLLER MANGERS & THEIR DIFF SELECTOR SUPPORT, 2 CLASSES
OF SELECTORS, HOW CAN SCALING BE AUTOMATED, THE DIFF AUTOSCALERS, 1)HPA/HOW HPA WORKS, 2)CLUSTER  AUTOSCALER, 3)VERTICAL AUTOSCALER, DEPLOYING OUR LEGACY 
APPLICATION; SPRING BOOT DOCKER, STEPS TO DEPLOY THE SPRING APPLICATION WITH ITS SERVICE, & THE MONGO DB WITH ITS SERVICE, REQUIRED FOR THE LEGACY APPLICATION
DEPLOYMENT, WHAT IS CODE SMELL, VOLUME CONCEPT IN KUBERNETES,  DEPLOY MONGO DB WITH HOSTPATH & NFS VOL, DEPLOYING MONGO DB WITH HOSTPATH VOL, 
DEPLOYING MONGO DB WITH HOSTPATH VOL, STEPS FOR Configuration of NFS Server, DEPLOYING MONGO DB WITH NFS VOL, PERSISTENT VOLUMES, ClAIM POLICIES, THE 
RELATIONSHIP BTW THE CONTAINER & THE PV & THE PVC, DEPLOYING MONGO DB WITH HOSTPATH PVC,  DEPLOYING MONGO DB WITH NFC BASED PVC.




******* While creating hostpath with PVC, i dnt knw why he repeated the same PV manifest twice before the PVC manifest BUT for this NFS with PVC, he didnt
                  When am practicing ill check if that was just an error, ie if it works without repeating the PV twice.




                                   ##########  STARTED HERE ########################
So we've been looking at k8 And one of the things that we discuss about k8, we realize that k8 is an orchestration.Is an open source orchestration tool.
That's k8, open source, or ochectration  tool which is used to orchestrate containerized applications.
So when applications are containerized using order tools like docker, we use k8 to orchestrate the containers, we use k8 to manage the containers.
So we also realize that k8 is not a replacement for docker.Okay, because we need, we stil need docker to containerize our application.
But k8 could  be a replacement for order orchestrates, like Doctor Swan.
Now, we saw the architecture of k8, and we realize that when we talk about k8, we are talking about a system that has, you know, it has control plane, and
work on node, that is the architecture.So , you know, there is a server that is controlling other, okay, servers, and we have the workeer   node where the
applications are running.Now we went ahead to discuss how do we deploy work load in  k8.How is work load deploy?
So we realize that generally, when we talk about k8 and work load, okay, is a very sensitive aspect to take note of when we talk about k8 and workload.
So E.G., we are managing containers, so containerized applications are deployed in k8 . we are deploying containerized application inside our k8 cluster
And for that to happen, We saw stuff where .We have Worker node. And, you know, that is our application are deployed in pods.That's where they are 
deployed.But now, should we just deploy our application directly in a port?The answer is no. So k8. So in k8, we have a k8 cluster, And this is a group of
one group of savers working together Okay, now we have a cluster, okay. And this cluster, we can can call it, k8s. Now, inside the inside the cluster, what 
do we have? We have nodes.Inside the notes, we have pods.And inside the pots, we have one containers.So this is the diagram. The cluster is made up of a 
group of note. The node is housing pods, and the pod is housing containers.
So now you know what is the smallest building block of k8 when it comes to objects and k8? What is the smallest building block? The smallest building block 
for k8 are pods? that is the smallest building block they are  pods.Because our continentalized application are okay. Deployed in pods,bt  we  notice that
if we deploy our application in pods, there's going to be a problem.why? How would that problem occur, e.g., the life cycle of a pod, okay, port.Has a short 
life cycle i. So the life cycle of the pod is short.Now, the second thing about the pod is that, if a pot dies, okay, the pot can not be restarted.So the pot
does not benefit from k8 Features. What are some advantages of k8?Some of the features of k8? What are some of the features of k8, k8 features that we saw?
We realize that k8 has self healing. What that means is that now, in k8, okay, now, dead port, dead ports are automatically.It's automatic, automatically 
restarted But this will Not happen if we're going to use PODS for deployment here.So if we use  pods for deployment now,there is a problem.pod has a short 
life cycle now, pod does not benefit from the entire features of k8.And we also saw that in k8, there is load balancing.Okay, and load balancing comes with 
service discovery, service discovery.Now, services discovery n load balancing. We can easily use a service to discover our applications that are running in 
the cluster.Now, by virtue of that, when we talk about service discovery and load balancing, what are some of the examples of services to take note of? .6:24



EXAMPLES OF SERVICE DISCOVERY

1)CLUSTER IP  2)  noodeport  service  3)loadbalancer  service  4)External name  
5)headless service

these Are the different services in communities.



                                           SERVICE DISCOVERY & ENDPOINTS
So with these, we can use any of these  to discover our application.
Therefore, we saw again, this diagram, where, with the help of a service, am I able to access, you know,access some  pods that are deployed in  my cluster.
Look at this diagram,  Look at this service. I have a cluster I-P service here okay now Let's.
Assume this my web service for my web application is able to route traffic and load balance traffic to all of this pods, to the group of pods so this 
service acts as a single DNS resolver.(domain name service) .. 8:00
What that means is that, now, if I want to access this pod, I want to call, okay, like, this pod will have, like, maybe Web port one.
Did you see that? Web port one?Now observe something This web pod one.,This pod two.Okay. Now if you want to access all of this pod, web, pod three.,web pod4
Now do I need to be calling Web for one Web, web port2,No, I-I don't have to do that so I can use one name to call all of this pod,
so which Name would i will use, the name of the service.So, because my servicename is, is constant.So DNS domain name is resolver.So  it, it is going to achieve 
domain name resolution.Okay, to all of the set of pods that are linked to this service now.So how do we call this pods in  k8 that are linked to a service?
     #####We call them What END POINTS? Okay, so this service has How many end point? How many end points? How many end points are there now? They are four
and point.
Now, for my scale, if need be can I scale?  if there is need to scale, am I able to scale to have maybe more pod, Can the number of end point increase?The 
answer is yes.The number of end point, okay, can increase.and once it increases
The same service is going to be achieving DNS resolution. The same service because that's what services are all about It's good to be achieving DNS resolution,.

If need be? Can we have more end point?, yes, if need be.
We can scale to how many replicas , right now 6
Okay. So one service, one service is routing traffic, to how many pods of the application six pods at the moment .  that’s one service.
This is what is going to be happening. So this is the feature of k8, as we have seen, a cluster, IP noteport, load balancer, external name, headless service.
And we are going to look at the ingress  as well as part of the options we can use for service discovery.  11:36

Therefore, if we if we should not deploy our our application as pods, how then should we deploy application, even though application are running as podst?
Should we use pods as an object to deploy k8? Should not be used as an object because we have, what we call k8 objects.
Okay. What are some of the objects that we know?Object number one is pods. Pod is an object?
Okay. Now, recommendation.we shouldn't use pods to deploy applications in k8.
 what should we use?  if you're going to deploy, use controller managers?

So we saw some controller managers. What are some of the controller managers that we have seen, which are part of the k8 object, okay?
Now, if you look at all of this controller my managers like pod for e.g , what is the API version for pod? API version for Pod? Well, what are their API
versions for pods?



                                    API VERSION
API version for pod  is V1.
What is the API version for replication? Controller? V1.
What is the API version for replica? Sets.. apps/V1.
What is it API version for daemon set, app/V1,   for deployments, app/V1.



  15.00                                      CONTROLLER MANGERS & THEIR DIFF SELECTOR SUPPORT

So do you realize that the replica set for pod and replication controllers, for E.G.,  differs from that of replica set?, daemon set n deployment? yes.

And this difference is captured in what we call Selector support.
for  E.G. replication controller. Which kind of selector does it have?
it Has equality based selector,. So this is like the old version  ….replication controller.- old version 
How about replica set on the others? Which kind of selector do they support Replica set?, daemon set ,  which kind of selector, and  deployment.
Which kind of selector does it have? Now this has what we call both equality base.& Set base selectors .. 16.50

                             2 CLASSES OF SELECTORS
 They have two classes of selectors.They have equality Base  selectors What else do they have apart frm  the equality base?
1)They have equality base. Selectors = key:value .... (key1: value1) ie key1  is equals to value1
2)it also has set base selectors.
We can hv a situation wher  a particular key  e..g key1 can hv multiple values like: one key is having multiple values like 
 key1:
               values:
                 value1  
                 value2  \
                 value3  

we also spoke abt replica sets ,,  with replication controller, and replica set, if you deploy your application, what happens?
We can scale them.Same with deployment, we can scale them so we can decide to have any number of replicas. So giving k8 support, one as a feature of k8, we
hv scaling. Could we skill in Docker? No, but in k8, we able to scale, so we can run command like, :
kubectl scale deployment/rc/rs --replicas 10   = manual scaling    .... 19:00
  kubectl scale ds --replicas 10   = .. but we can scale with daemon set bc daemon set ar used to deploy appl which we expect them to be hosted in each node
or a selectd number of nodes, that when wwe use daemon set.
so we hv seen that in k8 we are able to sclae ... .. and we can also automae scaling


2.00
************************* HOW CAN SCALING BE AUTOMATED?  ..........  INTERVIEW Questions.....
IQ: How can scaling be automated in kubernetes? 
   to sucesfully deploy auto scaling
By installing a guage API [: metric-server ] in our cluster and using *** mee** THE DIFF AUTOSCALERS 
      1. HorizontalPodAutoscaler[HPA] for pods   
       2. VerticalPodAutoscaler[VPA] for pods      ,, .........  so we can scale our nodes and pods
       3. CUSTER Autoscaler[CAS] for nodes  , we are scaling the number oF nodes 
************we can use one of these to automate scaling..
we need the guage bc we hv traffic coming into our cluster, and we hava a HPA, which we want that pod to automate the traffic for us. so we are saying 
that :HPA
min2/max50    ....ie replicas....  this is what we ar defing for our HPA , we ar also sayin that our target resource can be a deployment,RS/RC/STS
target resource 
deployment RS/RC/STS      ,, it could be any of these   .... now what is my target ultilization?
target ultilization  ... now when theHPA react? .. the HPA wil react when CPU ultilization or memUltilistaion is greater than 70% ,& thats when it wil scale
cpu Utilisation>70
memUltilisation >70
                                        26:56

                             1)HPA
HOW HPA WORKS
so our HPA is targeting one of d resource for e.g we use deployment & d deployment name is myapp,the deployment is suppose to control a set of pods &the 
deployment(myapp) is using a selector (match labels, if it is equality based) to know the set of pods that its suppose to control.,  to know the pods we ar
managing.. we wnt to automate it,, e.g we hv node1 and node9  and based on our replica:2  we hv mayapp1 on node1 and myapp2 on node9 so in our cluster we  
hv deployed another object in the cluster,which is called a metrix server whic is runin as a pod ,,so we hv a matrix server service and its also deployed as
a pod. the pod  is able to check hw much resource is been used here.. e.g it can check n tell us we ar using 10% memory, etc and when it gets TO  70%, ie if
ders a spike & we hv to process abt e.g 40m request, my app1 & myapp2 are receiving 20m reQ each due to Load been balanced automatically in k8, bt the 20m 
REQ is making use of up to abt 70% usage, our CPU usage exceeds 70% avg for all our pods,
our HPA will be triggered and one more pod will be automatically created,, this is an automatic addition and once the pod is created, more
traffic will be assigned to the pod.

34.18              

                           2)  CLUSTER  AUTOSCALER 
********if we are now processing 60m REQ, ie our nodes are at their max carryin capacity, but inside our cluster we hv deployed/install CAS 
n in the CAS we ar sayin that athe max number of nodes is min2/max40 .CAS is scaling the number of nodes. now when ar we goin to scale the node or when shud
the node be scaled??? for us to knw the proportion of our CPU that is been used, k8 wil use matrix server to check the vol of cpu for my node
 CAS/min2/max40 
target ultilization  ... 
cpu Utilisation>70
memUltilisation >70
so when our HPA wants the scheduler To scheedule more nodes bt all our nodes has insufficient resources so CAS wil be triggerred and it will add more node
to the cluster,& once the node is added new pods can be scheduled. this process is automatic
now the CAS is runin in our cluster n its runing as a pod and we hv the server which wil be checking hw much resources is our node consuming, 
our pod consuming, thats how it goes
... 37:25


3)
******** HOW WIL VERTICAL pod autoscaler work.//..... 
Let assume we want to deploy another application & we want to use the VPA to manage it
VPA resources:  under resources, we have request, how much resource can a pod requet
request:
cpu=500m  miliquo
mem=128Mi   miliquo
limits:cpu 900m
memory:512Mi
 based on the VPA, when a pod is created, this is the resource (the resources listed above) that is going to be assigned initially/the first time. resources
assigned to the pod wil be based on what is reQuested.assumin that the pod is caryin out some load,load is being assigned to the pod, at 500m it can process
a max of abt 20m requests bt if the number of rEQ is increasing,its goin to scale vertically ie increasing/growing upwards in size. the max cpu can increase
to is 900m , so again, more req can be processed to a max point of when cpu grows to 900m & it processin abt 40m REQ therfore, the same pod that was using 
cpu of 500m & processing 20m REQ has grown vertically to 900m and processing 40m REQ..  ... 43:00

metrics-server    
Resources / Limits / Requests / ResourceLimits  
all of this are what we discussd and today we want to deploy  our legacy application called spring boot mongo db and a database pod
Stateless applications deployment doesn't require to maintain it state   :
  Use deployment as the choice kubernetes object  
  ReplicaSets/ReplicationController:  
Stateful applications maintained their state. Examples are  :
  databases, Jenkins, 

*******************************
                                               
kubernetes
=========
  -- Kubernetes CLUSTER = Group of servers working together - k8s  
  -- k8s -- nodes -- pods[containers]  
pods for deployment:
  -- pods has short lifecycle      

kubernetes features:
   - scaling  
   - self healing capabilities  - dead pods are authomatically restarted 
   - service discovery & load balancing  :
         - ClusterIP / NodePort  / LoadBalancer  / ExternalName / HeadLess Service
         - ingress  

kubernetes objects:
   - pods[v1]  = we shouldn't use pod to deploy applications in kubernetes
                 use controllerManagers to deploy and manage pods/applications 
   - ReplicationControllers[v1]
   - ReplicaSets [apps/v1] 
   - DaemonSets  [apps/v1] 
   - Deployments [apps/v1] 

  selector supports:
   ReplicationControllers - old version = equality based selectors 
   ReplicaSets/DaemonSets/Deployments - selectors:
        - equality based selectors = key1:value1
        - set based selectors:
             key1:
               values:
                 value1  
                 value2  \
                 value3  

  kubectl scale deployment/rc/rs --replicas 10   = manual scaling   
  kubectl scale ds --replicas 10   = manual scaling   

IQ: How can scaling be automated in kubernetes? 
    By installing a guage API [ metric-server ] in our cluster and using 
       1. HorizontalPodAutoscaler[HPA] for pods   
       2. VerticalPodAutoscaler[VPA] for pods 
       3. CUSTER Autoscaler[CAS] for nodes  


metrics-server    
Resources / Limits / Requests / ResourceLimits  
==========================================================
                                    
All of this is to bring us to what we have covered.



44.00

            WE ARE DEPLOYING OUR legacy application.. **meee**


https://github.com/LandmakTechnology/spring-boot-docker         *** this application has a github repository, the application is called spring boot docker
-# Spring App & Mongod DB as POD without volumes         *** we are deploying a springboot application & a database pod
we will be deploying: a Stateless & a stateful application
1)Stateless applications deployment doesn't require to maintain it state   :   ***i bliv this is the main application itself
  Use deployment as the choice kubernetes object  
  ReplicaSets/ReplicationController:  
2)Stateful applications maintained their state. Examples are  :
  databases, Jenkins, where we create jenkins job , we hv to maintain its state
but generally databases are stateful applications


    ****WE  have this application that has been developed by our team of developers
mongo database:               48.00       
===============
 **** we have staefull aplications & stateless applications
  1. use StatefulSets as the choice kubernetes object for Stateful application deployments   
 however,
  2. USE ReplicaSets/ReplicationController/deployment StateLESS      
     application deployments              



                   ***mee*** DIFF service for the 2 application

      STEPS TO DEPLOY THE SPRING APPLICATION WITH ITS SERVICE, & THE MONGO DB WITH ITS SERVICE

so we ar goin to create 2files:
      *mee** one file for the application & one for the database & we wil deploy each of the files seperately with their own service manifest file
1) We create & deploy the springapp manifest file & a nodeport service for the application
    ***we need a nodport service so that: 
 we can hv external users and their traffic can get into the cluster either thru any of d worker nodes or the master node
   bc we v kubeproxy running ,the trafic doesnt leave d node n talk to d pods directly, it goes tru the service n once it recieves d msg it does dns 
         resolution, it routes the traffic n it lb to all d end points 
2) We create & deploy the mongo db manifest file & a clusterip service for the application
   now dis appl nids to write in the db so we ar deployin a db pod & for the communication to be established they wil communicate tru
a service, dis  is internal comm so we nid clusterip service, so we create a mongosvc;app:mongo
      so for springapp to talk to the mongo db it wil be resolved via the service name of the db  ... 1:26:40


                  
                       REQUIRED FOR THE LEGACY APPLICATION DEPLOYMENT
Why is required for this deployment, if you look at the database configuration & the appliaction,
if we decide to deploy this application some objects ar goin to be affected  ..... 49:01 
we wnt to deploy our stateless appli called springboot mongo n we also  deploy our stateful application wich is a mongo db, 
so we ar goin to create 2files: *mee** 1) file for d application & one for the database & we wil deploy each of d files seperately with their own service file
1)Stateless  
--- springapp.yml/            ***meee** this is the main application
2)Stateful 
--- mongo.yml/                   ** this is the database


49.30   
the way developers develop this application, it listens on port 8080
spring:
  data:
    mongodb:
      host: ${MONGO_DB_HOSTNAME}
      port: 27017       ******** the db listens on 27017
      username: ${MONGO_DB_USERNAME}
      password: ${MONGO_DB_PASSWORD}
      database: users
      authentication-database: admin
server:
  port: 8080    .... ******the spring appl listens on 8080


   ** 49.47,,.. for you to access a database server or application, is like if we were going to ssh, it goes with username/hostname
if u run this command:
   ssh username@hostname [ It will request for the password ]  
hostname is a variable.

lets assume we hv smt like this, whereby the variables are already given 
52.10                                                CODE SMELL
however if we v an e.g like dis wher d variables hv been given,it means its hardcoded then it becomes a problem bc we cant use it acros multiple env &can 
result in code smell if we are to run sonarqube code analysis/review
    mongodb:
      host: mongo
      port: 27017
      username: devdb  
      password: devdb123  
  ssh devdb@mongo   , we can run smt like this.. to ssh

53.00
so we want to deploy this application, lets start with the first one 
1)Stateless  
--- springapp.yml/ 

53.55 .... WE ARE DEALING WITH OUR legacy application
Deleted all previous resources


1. image: mongo         ******this is maintained by the mongo community, in docker hub, we can check for the image inf to get the env variable  1:16:30, just type mongo
2. image: mylandmarktech/spring-boot-mongo               ********this is maintained by us

repository to clone:
1. https://github.com/LandmakTechnology/kubernetes-notes
2. https://github.com/LandmakTechnology/kops-k8s
3. https://github.com/LandmakTechnology/kubernetes-manifests

######this is a springboot application that requires a database, it has has been developed by our developers usin the springboot framewrk
this was not hardcoded, our developers left the hostname, username,passwd as a variable, meaning that if i am deployin in the dev, uat, prod env, i can use
diff values based on my project requirement....   .. 53:15


                                    DEPLYING THE SPRING APPLICATION
*********the default deployment strategy in k8 is rolling update
now since am using the default strategy,i wnt put a value for strategy bc the default wil be used ie rolling update

springapp.yml: kams   
kind: Deployment  
apiVersion: apps/v1   
metadata:
   name: spingapp    
spec:
   replicas: 2  
   selector:
     matchLabels:
       app: spring    
   template:  
      metadata:
         labels:
            app: springapp    
      spec:  
         containers:
         - name: app  
           image: mylandmarktech/spring-boot-mongo  
           ports: 
           - containerPort: 8080
           env:
           - name: MONGO_DB_HOSTNAME  
             value: mongosvc  
           - name: MONGO_DB_USERNAME 
             value: devdb   
           - name: MONGO_DB_PASSWORD
             value: devdb@123   
---                                         #######the deployment also requires a service, to write a service manifest file is no big deal
apiVersion: v1
kind: Service
metadata:
  name: springappsvc                               1:22:30
spec:                         for us to be abble to acces athe  springapp pod, 
  type: NodePort      ,, we created  a service , nodeport service:nodeportsvc and the service name is called springappsvc n the service has a label; app:springapp
  selector:             since its a nodeportservice, we can hv external users and their traffic can get into the cluster either thru any of d worker nodes or the master node
    app: springapp       bc we v kubeproxy running ,the trafic doesnt leave d node n talk to d pods directly, it goes tru the service n once it recieves d msg it does dns 
  ports:                          resolution, it routes the traffic n it lb to all d end points.. now dis appl nids to write in the db so we ar deployin a db pod n 4 d 
  - port: 80                          comm to b est they wil communicate tru a service, dis  is internal comm so we nid clusterip service, so we create a mongosvc;app:mongo
    targetPort: 8080               so for springapp to talk to d db it wil be resolved via the service name of the db  ... 1:26:40

*********** now we deploy the springapp and the service called: springappsvc
kubectl apply -f springapp.yml

ubuntu@master:~$ kubectl get pod

                                    GET END POINTS
ubuntu@master:~$  kubectl get ep
NAME                    ENDPOINTS                      AGE 
springappsvc             10.36.0.1:8080,10.44.0.3:8080   67s

   ***************if i wnt ot communicate wit the pod, i can use the name of the sevrice: springappsvc or the service ip: 10.100.100.234 
such that if i curl, 
ubuntu@master:~$ curl 10.100.100.234       ..... i successfully get a response 
so the service is achieving dns resolution  ,,....... 1:07:50

we can access the appl externally since we ar using node port , 
ubuntu@master:~$ kubectl get svc 
NAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORTS    AGE
springappsvc   NodePort    10.100.100.234     none        80:32136/TCP 

*****all we nid to use is one of our node even the master node in this case 
ubuntu@master:~$ curl ifconfig.me
3.36.7561ubuntu@master:~$
ubuntu@master:~$
ubuntu@master:~$ curl 3.16.75.61:32136    ... or  in google 3.16.75.61:32136 , w ecan access the application
  ###we can see this appl is a springbootapp thet needs the mondodb  databse and we can automate it using jenkins,docker & k8 , and we are using k8 now, 
we already used docker to containerize
***** right now our database has nt been deployed n so ders no database that the appl is able to comm with.


                              DEPLOY THE DATABASE APPLICATION
*********8*************now we deploy our database
mongo.yml  = kams:  
=========
kind: ReplicaSet   
apiVersion: apps/v1  
metadata:  
  name: mongors    
spec:                      *********** we wnt 1replica so we dnt write it bc by default, one wil be created.
  selector:
    matchLabels:
      app: mongo     
  template:
    metadata:  
      labels:
        app: mongo    
    spec: 
      containers:
      - name: mongodbcontainer    
        image: mongo      
        ports:
        - containerPort: 27017    
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb                ********** if we hv a number of databaseports, to ahieve dns resolution
        - name: MONGO_INITDB_ROOT_PASSWORD 
          value: devdb@123
---
kind: Service   
apiVersion:  v1  
metadata:  
   name: mongosvc 
spec:                 the default service type in k8 is clusterip so even if i dnt write it, the clusterip type will be created. type:clusterip
  selector:
    app: mongo    
  ports:
  - port: 27017 
    targetPort: 27017  

#############to deploy the db      .........  1:10:10
ubuntu@master:~$ vi spring/mongo.yml n paste the  db manifest file
ubuntu@master:~$ kubectl apply -f spring/
replicaset.apps/mongors created
service/mongosvc created            ****** we see the service created 
deployment.apps/springapp unchanged
service/springappsvc unchanged
ubuntu@master:~$
ubuntu@master:~$ kubetcl get all 
we can see mongosvc is running  ...... n when we check in google we can see its running, so we are able to deploy our stateful appl n stateless appli
so this process can be automated
if we run kubectl delete pod mongors-966hc                 ,......1:33:40
we can see a new pod is being created automatically.. n this is permitted bc it has a controller manger which is our mongors but if u run: kubectl delete rs mongors, db wnt
although it was automTICALLY CREATED, we cant get back data that was previously created                       be recreated bc D replication controller MANAGer managing the.
for us to be able to capture data we hv to look at volume concept in k8                                      db pod has been deleted  ... 1:42:50
                                                                ***  bt goin further in d video he kefpt deletin the rs each time he had to deploy a manifest 
                                                                                                   file  like 


---1:15:48                                                                                    for NFS and persistent vol
1. image: mongo         ******this is maintained by the mongo community, in docker hub, we can check for the image inf to get the env variable  1:16:30, just type mongo
         *** The mongo image got over 1B downloads
2. image: mylandmarktech/spring-boot-mongo               ********this is maintained by us

  This is its environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
           env:
           - name: MONGO_DB_HOSTNAME  
             value: mongosvc  
           - name: MONGO_DB_USERNAME 
             value: devdb   
           - name: MONGO_DB_PASSWORD
             value: devdb@123   


==========================



WE ARE GOING TO BE LOOKING AT VOLUME CONCEPT IN KUBERNETES    ***mee** we want to add vol/a piece of storage to our database
                                       IMP REPOSITORIES TO CLONE
IMP REPOSITORIES TO CLONE
repository to clone:
1. https://github.com/LandmakTechnology/kubernetes-notes
2. https://github.com/LandmakTechnology/kops-k8s
3. https://github.com/LandmakTechnology/kubernetes-manifests


                                         DEPLOY MONGO DB WITH HOSTPATH & NFS VOL

  ***meee**
We want to deploy the mongo db with 2 diff volume types
1)Hostpath volume
2)NFS volume.. this will require creating a NFS server


        
                                   CREATING HOSTPATH VOLUMES .................

****IF U ARE TRYING to deploy a stateless appl we hv to mk use of k8 volumes  ...  1:3540
since we are deployin a database we nid to ensure that a piece of storage is created..
we ar goin to see hw to use vol for example:  
.. and the only deployment that will be affected is our mongors , so we wil deploy it agin this time around With volume

 1)                              DEPLOYING MONGO DB WITH HOSTPATH VOL  
    *dnt know why we didnt create a manifest file to create a service,is it bc earlier we created the mongodb with a clusterip or the service is jst nt required
   or he just skipped that part
but that illustration is completely independent of this vol illustration & also the pvc illustration, 

kubernetes volumes:
===================
mongovol.yml  
kind: ReplicaSet   
apiVersion: apps/v1  
metadata:  
  name: mongors    
spec:
  selector:
    matchLabels:
      app: mongo     
  template:
    metadata:  
      labels:
        app: mongo    
    spec:       .. ******  under spec, we add volumes
      volumes:        *********vol is a list so we can v more than one vol e.g we can v, name: vol2 ,hostpath:  and path can jst be /data n also multiple mount point
      - name: mongodbhostvol                **i bliv hostpath can be like docker vol &/or bind mount, wher we can need only d vol name or create a mount point
        hostPath:
          path: /mongodata        *****if the db pod was created on node9, a mountpoint will be created on node9 and it wil be called /mongodata
      containers:
      - name: mongodbcontainer    
        image: mongo      
        ports:
        - containerPort: 27017    
        volumeMounts:    ******* we wnt to mount the vol  ,,,  we ar creatin a host path vol  .. 1:41:10
        - name: mongodbhostvol           *******name of vol been mounted
          mountPath: /data/db    ******** in the file in dockerhub we see data is stored in /data/db therfore in our db container, data will be stored /data/db
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD 
          value: devdb@123

########we ar deployin our appl and making use of volumes
ubuntu@master:~$  vi spring/mongovol.yml
ubuntu@master:~$  kubectl apply -f spring/mongovol.yml
replicaset.apps/mongors created
ubuntu@master:~$  
#########now if the db pod is deleted, once it is created we still hv access to the inf created before bc the pod is created on the same node it existed b4 it was deleted.


kubectl taint nodes node9 key1=value1:NoSchedule     [taint the node]
kubectl taint nodes node9 key1=value1:NoSchedule-    [untaint the node]

###### e.g if  smt happens to node 9 or we taint node9 where our db is created , the db pod has been recreated on node1, what will happen to our stored data ???, it wil 
result in data loss because we are using hostpath volume :
  hostPath volume will store data ONLY on the node where the POD is scheduled .. ie inf entered while d db pod was on d node cant be transfered to anoda node if d db is
  This can result data lost and data inconsistency                                                         recreated on anoda node



CREATING NFS VOL 
######## ******************    2:01:20  ..   netwoRK FILE SYSTEM OR EFS

NFS    
if we introduce any of these, then we are goin to create an nfs server & ther wil be a mount poinT on d node& nfs server then data is captured in both the db
container and the nfs such that evEN if the container is recreated on another node, once the container syncronizes with the nfs server all the data that was
captured in the Nfs server wil reflect in the container and this wil result in data consistency

      volumes:
      - name: mongodbhostvol    
        hostPath:
          path: /mongodata 
      volumes:
      - name: mongodbhostvol    
        nfs:
          path: /mongodata 


when it comes to vol concept, ther are several vol options we can look at like:
awsElasticBlockStore 
azureDisk
azureFile 
configMap 
emptyDir g
cePersistentDisk
gitRepo (deprecated) 
hostPath
nfs 
persistentVolumeClaim 
secret
***Docker 8&9... Docker Volumes
Volumes stored data in a part of the host filesystem managed by docker

 
                              STEPS FOR Configuration of NFS Server
A)===========================
Step 1: 
Create one Server for NFS
2:08:31
go to aws and create a server for nfs
the server has to be launched in the same vpc like the other servers ie k8 servers, like the master node .. *** mee** like all the servers in the cluster..
can create a security grp for nfs with the REQ ports open like  ssh  port ffrm anywher, nfs 2049 
then connect the server in mobaxterm

######***********************then Install NFS Kernel Server in 
Before installing the NFS Kernel server, we need to update our system’s 
repository index with that of the Internet through the following apt command as sudo:

***********************Configuration of NFS Server
  NFS = Network file system 
        distributed file system 
        shared file system 

Step 1: Install NFS Kernel Server
Before installing the NFS Kernel server, 
  we need to update our system’s repository index with that of the Internet 
  through the following apt command as sudo:

$ sudo apt-get update ( in the nfs server created in aws)   

The above command lets us install the latest available version of a software through the Ubuntu repositories.
Now, run the following command in order to install the NFS Kernel Server on your system:

$ sudo apt install nfs-kernel-server -y   ...   2:13:20

Step 2: Create the Export Directory

sudo mkdir -p /mnt/share/       ** this is a shared dir , can run ll /mnt/share/  to see who owns the dir

=# As we want all clients to access the directory,
    we will remove restrictive permissions.
sudo mkdir -p /mnt/share/
sudo chown nobody:nogroup /mnt/share/         ... now the dir is owned by nobody and d dir has read write permissions
sudo chmod 777 /mnt/share/

Step 3: Assign server access to client(s) through NFS export file   ***** in dis file we ar goin to decide who wil v access to our nfs server, to read n write

sudo vi /etc/exports

=#/mnt/share/ <clientIP or Clients CIDR>(rw,sync,no_subtree_check,no_root_squash)
 #Ex:
    and paste the below in the file then go to my vpc in aws n copy the CIDR (ipv4CIDR) N PASte it in place of the * , for access within the vpc but if i dnt
use CIDR, it 
means am allowing access frm anywher
/mnt/share/ 10.0.0.0/24(rw,sync,no_subtree_check,no_root_squash)    *acess within vpc

/mnt/share/ *(rw,sync,no_subtree_check,no_root_squash)         ******** accesss frm anywher


Step 4: Export the shared directory

$ sudo exportfs -a

sudo systemctl restart nfs-kernel-server

Step 5: Open firewall for the client (s) PORT 2049


 B)  NEXT STEP
Configuring the Client Machine  .. ie client are my worker nodes, i can also instal IT on my master, so we installed on both master n worker node1&9, or all
        nodes in d cluster  so on my master &worker node1,9, we are just going to update n instal nfs client wich is nfs common, thats all..
        rather than doing this installation manually, we could hv used ansible to automate the process
==========================      
                            
Step 1: Install NFS Common
Before installing the NFS Common application, we need to update our system’s repository index with that of the Internet through the following apt command as
sudo:

$ sudo apt-get update && sudo apt-get install nfs-common -y

$ sudo apt-get install nfs-common


nfs server: public n private ip 3.17.179.30  / 10.0.0.76
*******************************************************************
When used as a Persistent Volume, an EFS volume can be mounted with ReadWriteMany access mode, allowing multiple Pods on different nodes to read and write
to the same volume simultaneously.
EFS is suitable for applications requiring shared file access, such as content management systems or distributed applications.

                            
2)                DEPLOYING MONGO DB WITH NFS VOL    **dnt know why we didnt create a manifest file to create a service, i bliv clusterip
----   the only thing that wil change here is the nfs server address and remove hostpath cos d vol type we are using is nfs, then everytin remains the same
 *dnt know why we didnt create a manifest file to create a service,is it bc earlier we created the mongodb with a clusterip or the service is jst nt required
   or he just skipped that part
but that illustration is completely independent of this vol illustration & also the pvc illustration, 


mongo-nfs.yml  
==================
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongors
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      name: myapp
      labels:
        app: mongo
    spec:
      volumes:
      - name: mongodbvol
        nfs:
          server: 10.0.0.76          ********using the private ip bc they ar in the same vpc ie both my nfs server & k8 nodes
          path: /mnt/share
      containers:
      - name: mongodbcontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
        volumeMounts:
        - name: mongodbvol
          mountPath: /data/db

************TO DEPLOY
vi spring/mongo-nfs.yml n paste manifest file
kubectl delete rs mongors  ... ie the db
kubectl apply -f spring/mongo-nfs.yml
kubectl get po -o wide    ......  ... we see the pods n the node they are running on

######### with this nfs deploymnet we dnt have issues with data loss / inconsistency .............2:33
***********************can chech the power point script as well 

############WE CAN USE EFS instead of NFS, but we used NFS without any issues here

kubectl taint nodes node9 key1=value1:NoSchedule     [taint the node]
kubectl taint nodes node9 key1=value1:NoSchedule-    [untaint the node]



                                 PERSISTENT VOLUMES 
**online**
A persistent volume is a storage resource in a cluster that has a lifecycle independent of any single pod. It provides persistent storage for stateful 
applications, ensuring that data is not lost even if the pods using it are deleted or rescheduled. 
Administrators provision persistent volumes, which can be backed by various types of physical storage like local drives or cloud-based systems. 

===================
this are simply a piece of storage in your cluster
 the ideal appraoch to store data in k8 is by using persiastent vol bc it is managd by the kube service
just like docker vol is manged by docker and thats why we use persistent vol in docker as well

*** Based on someones question: the persistence vol could either be host path, NFS and other vol options 
with persistent vol, the vol itself is managed by the kube service
the nfs is kind of s storage option that we can use to store data like we saw hostpath wher data wil only be hosted by one particular node in the clusted bt
in the nfs data is distributed to all the nodes and that leads to data constitency,, howeve rif u create a nfs and it is nt created under a persistent vol, 
bc it is the recommended approach to store data in k8 bc when u create a persistent vol it is manged by the kube service , therefore the vol is independent 
of the livecycle of the pod that is consuming the vol so if the pod goes down we can stil retieve the data



deploy a kubernetes using Kops  


PV --> It's a piece of storage (hostPath, nfs,ebs,azurefile,azuredisk) in k8s cluster. 
PV exists independently from from pod life cycle form which it is consuming.

PersistentVolumeClaim -->
   It's request for storage(Volume).Using PVC we can request(Specify) 
   how much storage u need and with what access mode u need.

Persistent Volumes are provisioned in two ways, Statically or Dynamically.:  ... 2:35:40  ... power point script

1) Static Volumes (Manual Provisionging)
    A k8's Administrator can create a PV manually so that pv's can be available for PODS which requires.
    Create a PVC so that PVC will be attached PV. We can use PVC with PODS to get an access to PV. 

2) Dynamic Volumes (Dynamic Provisioning)
     It's possible to have k8s provision(Create) volumes(PV) as required. 
     Provided we have configured A storageClass [sc].
     So when we create PVC if PV is not available Storage Class will Create PV dynamically.        

PVC: If pod requires access to storage(PV),it will get an access using PVC. 
     PVC will be attached to PV.

PersistentVolume – the low level representation of a storage volume.
PersistentVolumeClaim – the binding between a Pod and PersistentVolume.
Pod – a running container that will consume a PersistentVolume.
StorageClass – allows for dynamic provisioning of PersistentVolumes.

PV Will have Access Modes:
============================
ReadWriteOnce=RWO – the volume can be mounted as read-write by a single node = EBS 
ReadOnlyMany=ROM  – the volume can be mounted read-only by many nodes
ReadWriteMany=RWM – the volume can be mounted as read-write by many nodes = NFS 


Claim Policies
================
A Persistent Volume Claim can have several different claim policies associated with it including
Retain – When the claim(PVC) is deleted, the volume(PV) will exists.
Recycle – When the claim is deleted the volume remains but in a state where the data can be manually recovered.
Delete – The persistent volume is deleted when the claim is deleted.

The claim policy (associated at the PV and not the PVC) is responsible for what happens to the data when the claim is deleted.

2:37:13
**********************************THE RELATIONSHIP BTW THE CONTAINER & THE PV & THE PVC    *** i bliv its the db container..
inside my cluster, i wil create persistent vol, now my db nids this vol n the vol can only be assigned using persistent vol claim ,,, PVC, with the PVC we 
are able to mount we ar able to create a mount point btw the container n the persistent vol, the PVC permits the container to claim the persistent vol
*************with this system ders no data loss if db container goes down bc persistent vol is anoda k8 object whic is independent of my pod lifecycle
##########we wil use retain claim policy most of the time


##########################################DEPLOYING MONGO DB WITH HOSTPATH PVC..................2:44:44
2.48.45 if any of this object is not created, it will not work, the container will not run.

*i bliv.. this firt manifest file is where we create d 'pv & hostpath' & we see  its 'kube' like in Docker; Bind mount, we created mount point which is d vol
mongo-pv-hostpath.yml   
---------------------                   ********** i dnt knw why he repeated the same PV manifest twice before the PVC manifest
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/kube"             ************  WE SEE the hostpath is manged by the kube   .......2:53:11

---
                ************************** CREATING mongodb with pV ,   ** mee*** this is the vol
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
    path: "/kube"
---                  ************************ cREATING THE claim policy ie the PVC
apiVersion: v1                     PVC: If pod requires access to storage(PV),it will get an access using PVC. PVC will be attached to PV
kind: PersistentVolumeClaim
metadata:
  name: pvc-hostpath
spec:
  storageClassName: manual
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi                2.48.20***its claiming 100Mi from the 1G of the pv ie the first manifest file above
---                                ***************CREATING     Mongodb manifest file using Replicaset 
# Mongo db pod with PVC
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongors
spec:
  selector:
    matchLabels:
      app: mongo
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongo
     spec:
       volumes:
       - name: mongodb-pvc
         persistentVolumeClaim:
           claimName: pvc-hostpath
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: mongodb-pvc
           mountPath: /data/db

**************************************  Vi mongo-pv-pvc.yml and paste        .... this is all manual provisionong
kubectl apply -f mongo-pv-pvc.yml
created  , our db pod is running n we can write data  
the recommendation is always standard,  persistent vol is created &managed  such that if the pod using the vol goes down, the vol stil exists so the vol is managed by k8 
by k8 , so the ideal appraoch to store data in k8 is by using persiastent vol by it is managd by the kube service
just like docker vol is manged by docker and thats why we use persistent vol in docker as well


 
**********************************   DEPLOYING MONGO DB WITH NFC BASED PVC...........
                                          we chnaGE whats necessary in the file                                                
mongo-pv-pvc-nfs.yml   
===================
--- ** we dnt nid to deploy the persistent VOL part of the script here like we did above when we mounted persistent vol bc we are using NFSas our VOL Type
**went tru this part of d video now in 2025, bt didnt hear him say this, so i probably wrote this myself in 2024 based on my understanding then & its wrong
    **** While creating hostpath with PVC, i dnt knw why he repeated the same PV manifest twice before the PVC manifest BUT for this NFS with PVC, he didnt
                               When am practicing ill check if that was just an error, ie if it works without repeating the PV twice.


apiVersion: v1
kind: PersistentVolume   
metadata:
  name: pv-nfs-pv1         
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: <nfs server ip>  10.0.0.76
    path: "/mnt/share"
---
# Mongo db pod with PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nfs-pv1
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Mi
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongors
spec:
  selector:
    matchLabels:
      app: mongo
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongo
     spec:
       volumes:
       - name: mongodb-pvc
         persistentVolumeClaim:
           claimName: pvc-nfs-pv1
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: mongodb-pvc
           mountPath: /data/db

kubectl delete rs mongors
mongors deleted
vi spring/pv-pvc-pfs-momgos.yml and paste
kubectl apply -f spring/pv-pvc-pfs-momgos.yml and paste

**************after we deployed this nfs vol  , bc synchronization has taken place we can see data captured previously in the nfs  bc its the same mountpoint 
that was created earlier these are persistent vol and that is the choice that is being used in deployment 
kubernetes volumes  :
==========================

QUESTION

CAn we use NFS TO scale inorder to meet the wrk load??
if u are refering to hw to hv multiple read replicas 
we wil be seeing hw to replicate the db so we can hv multiple read replicas

why didnt we use stateful set for deployment ?
we will use it dnt worry

******i dnt understand persistent and nfs
the nfs is kind of s storage option that we can use to store data like we saw hostpath wher data wil only be hosted by one particular node in the clusted bt
in the nfs data is distributed to all the nodes and taht leads to data constitency,, howeve rif u create a nfs and it is nt created under a persistent vol, 
bc it is the recommended approach to store data in k8 bc when u create a persistent vol it is manged by the kube service , therefore the vol is independent 
of the livecycle of the pod that is consuming the vol so if the pod goes down we can stil retieve the data
the persistence vol could either be host path, NFS and other vol options 
with persistent vol, the vol itself is managed by the kube service


if u master k8 u are at the center of ur game 











