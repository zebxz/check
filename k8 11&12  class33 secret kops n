CONFIG MAP AND SECRETS , DEPLOYING A K8 CLUSTER USING KOBS
ways to pass secret values so they are more secured.               ....... questiosns
****************************************************************************************************

there aew 2kinds of appl we generally deploy in k8
stateless
stateful   : we always need to maintain its state bc it captures data and we nid to ensure the data is stored and retrievable and vol concept in k8 is what we can use to achieve
that process

we also realize that why we v dif option when it cm to vol der are some other imp aspect to consider like:
diff vol option:
secret
nfs
hostpath
azuredisk
configmaps

4:48
env variables are  like secret variable ,, because the inf is stored in our source code mgt and so we shouldnt pass the env variable like this 
 env:
           - name: MONGO_DB_HOSTNAME
             value: mongosvc
           - name: MONGO_DB_USERNAME
             value: devdb
           - name: MONGO_DB_PASSWORD
             value: devdb@123


6:29      CONFIG MAP
######  HOW CAN WE DEPLOY THE ENV variable/file in a more secured way

kubernetes volumes :
==========================
==========================  by using:
Config Maps & Secrets
======================
We can create ConfigMap & Secretes in the Cluster using command or also using yml.
********************ConfigMaps:
  are used to passed non confidential information in key:value  pair that weren't 
  hardcoded/included in the Dockerfiles/code by Developers
   E.G we can use config map to pass stuff like 
    HOSTNAME
    USERNAME 
also if developers are required nt to code, we are then required to use config maps to pass the variables

e.g we are tryin to deloy an appl and we are using a kind of a tomcat image  n in that tomcat image hw do we access n manage tomcat
we hv a file called tomcat users.xml, so  we couldcreate a config map for our tomat users
   e.g  lets assume we hv diff users in diff , with diff rolesenv we are not hardcoding, these are dynamic variables
tomcat-users.xml  [  ]  
      dev = <user username="tomcat" password="tomcat" roles="admin-gui,manager-gui"/>
      uat = <user username="paul" password="admin123" roles="admin-gui,manager-gui"/>

****** we can now use config map such that when we wnt to deploy for each env we knw we hv diff users , in each of that env we can use configmap n secret for that purpose
bc we ar managing our appl n it has diff values for diff env 

tomcat-users.xml  
    <?xml version='1.0' encoding='utf-8'?>
      <tomcat-users xmlns="http://tomcat.apache.org/xml"
                      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                      xsi:schemaLocation="http://tomcat.apache.org/xml tomcat-users.xsd"
                      version="1.0">
       
       <user username="tomcat" password="tomcat" roles="admin-gui,manager-gui"/>
    </tomcat-users>


****************************Secrets:
  are used to passed confidential information in base64 format e.g 
    PASSWORD
    SSH_PRIVATE_KEYS  
    dockerHub LOGIN password  
    tls certificate  
          
 *******************************OPTIONS FOR CREATING  CONFIGMAP:
1)Create ConfigMap Using Command:
  kubectl create configmap springappconfig --from-literal=db-username=devdb 
  kubectl create configmap springappconfig --from-literal=db-hostname=mongosvc 

should we use imperative or declarative approach to create config maps? we shud use file/declarative
2)
Create ConfigMap Using files/declarative approach:
yaml:
=====
kind: ConfigMap  
apiVersion: v1     
metadata:
  name: springappconfig   
data:
  db-hostname: mongosvc   
  db-username: devdb  
  db-password: devdb123       ************shud we create pw using configmap, ofcourse we can do that bt it is nt recommended;
                                                           ill just put here devd123, am creating pw here using configmap bt dis is nt recommended
      for us to create PW, WHICH OBJ SHUD WE be using ? kind , so we wnt shud we use to create secret, we can create secret using command like this:
Secret Using Command: 
kubectl create secret generic springappsecret --from-literal=mongodbpassword=devdb@123 

******************USING A FILE TO CREATE A SECRET
                  THIS IS A SECRET obj we are creating ............19:00
---                                                 
apiVersion: v1
kind: Secret
metadata:
  name: springappsecret
type: Opaque
stringData:   # We can define multiple key value pairs.
  mongodbpassword: devdb@123


kubectl api-resources                         16.15
kubectl api-resources | grep -i configmap           .... to get the api version for config map  

*************************************CREATING CONFIG MAP AND SECRET
***********************kubectl get secret  ............. to see our secrets
kubectl get cm   .... to get config maps

vi secret.yml    and paste  ,   

kind: ConfigMap  
apiVersion: v1     
metadata:
  name: springappconfig   
data:
  db-hostname: mongosvc   
  db-username: devdb  
  db-password: devdb123
-----
apiVersion: v1
kind: Secret
metadata:
  name: springappsecret
type: Opaque
stringData:   # We can define multiple key value pairs.
  mongodbpassword: devdb@123

....................to create the secret object9 : kubectl apply -f secret.yml
configmap/springappconfig created
secret/springapppsecret created
###########now we hv our secret n configmap for our configuration

we wnt deploy our springapp, we we hv our secret n configmap so we can mk some changes in the env of our manifaet file 
**************************************22:00
kind: Deployment
apiVersion: apps/v1
metadata:
   name: spingapp
spec:
   replicas: 2
   selector:
     matchLabels:
       app: springapp
   template:
      metadata:
         labels:
            app: springapp
      spec:
         containers:
         - name: app
           image: mylandmarktech/spring-boot-mongo
           ports:
           - containerPort: 8080
           env:
           - name: MONGO_DB_HOSTNAME               
             valueFrom: 
               configMapKeyRef:                        *************we dnt pass any values,
                  name: springappconfig
                  key: db-hostname                     it wil read the value frmthe config map that hv been created
           - name: MONGO_DB_USERNAME
             valueFrom: 
               configMapKeyRef:
                  name: springappconfig
                  key: db-username
           - name: MONGO_DB_PASSWORD   ************ even though we can create PW using configmaps but for best practice we use secret cos secret is for confidential data
             valueFrom:                   the value is coming frm the secret for best practice
               secretKeyRef:
                  name: springappsecret
                  key: mongodbpassword 

******************* now we can delete the previous springapp we deployed from previous class and deploy this file
vi spring.yml
kubectl apply-f spring.yml
now we can write data in the data base bc for us to deploy this appl we are making use of multiple objects  .....31:20

*****************************    APPLICATION DEPLOYMENT
if we are going to deploy an application what is involved????
LETS ASSUME the we are deploying the database as a stateful set , database stateful set or as we saw as a replica set
what else are we deploying as part of the database, we nid a storageclass, wich we wil be sseeing verysoon , thats part of vol, also persistent vol, pvc , this is one appl
we are deploying, we cud nid configmaps and secret ,  so one deployment requires all of this.. we'l also nid to deploy a service to mk this database discoverable
Stateful applications mongodb :
  - statefulsets/RS  
  - storageClass 
  - PersistentVolume 
  - PersistentVolumeClaim
  - configMaps 
  - Secrets   
  - service = ClusterIP   
stateless applications:  
  - deployment       .....recommended obj is deployment 
  - configMaps 
  - Secrets   
  - service - most of the time this could be NodePort / LoadBalancer     .......35:40

#######pls take note this is  very imp this is a key aspect  to underline and thats one thing that we are covering to mk sure u have a gud understanding when it cm to 
configmap n secrets


****************
WE AHVE ANOTHER EXAMPLE OF CONFIG MAP, THAT I WANT TO CREATE A CONFIGMAP WITH EXTERNAL VOLS, Create anoda configmap obj like this:
---
ConigMap As Volume Example
=========================
-# ConfigMap with file data
---  e.g am deploying an appl, i cud create a java webapp config like this, wher this tomcatuser.xml file,  am passing this as configmap, so u can deploy a configmap like
this and we ar goin to hv this config map in the system and if u want to access ur tomcat on the browser u can use this new pw and username to access it

apiVersion: v1
kind: ConfigMap
metadata:
  name: javawebappconfig
data:
  tomcat-users.xml: |
    <?xml version='1.0' encoding='utf-8'?>
      <tomcat-users xmlns="http://tomcat.apache.org/xml"
                      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                      xsi:schemaLocation="http://tomcat.apache.org/xml tomcat-users.xsd"
                      version="1.0">
       <user username="devadmin" password="tomcat123" roles="admin-gui,manager-gui"/>
    </tomcat-users>



***************************
unbuntu@master:~$  kubectl get sc   ..... to get storage class

unbuntu@master:~$ kubectl get svc
NAME               TYPE              CLUSTER-IP              EXTERNAL-IP      PORTS          AGE
mongosvc          ClusterIP         10.110.26.165               none
springappsvc       NodePort         10.100.100.234              none           80:32136/TCP 

we can edit the service
unbuntu@master:~$ kubectl edit svc springappsvc 
we saw the manifest file and he changed the service type from nodeport to loadbalancer

unbuntu@master:~$ kubectl get svc
NAME               TYPE              CLUSTER-IP              EXTERNAL-IP      PORTS          AGE
mongosvc          ClusterIP         10.110.26.165               none                                 what are some of the changes here?
springappsvc       LoadBalancer       10.100.100.234             pending      80:32136/TCP     WE stil hv the samenodeport no. here****it is waiting for an external ip wich wil nt cm  ....38:58
                                                                                   & this is the problem we ar goin to be facin with self manged k8 cluster.... 39:05

***************** DEPLOYING A K8 CLUSTER USING KOBS    ...... 40:50  ,, read  the  file in github account

       KOBS IS A SOFTWARE used to deploy a production ready k8 cluster
here is our github account wich we'l be using to create a kobs cluster
kobs is ifaac

kops:
Ticket07: Install a kops cluster using company documentation
          https://github.com/LandmakTechnology/kops-k8s

**************lets see hw to manage kops .  ..... DEPLOYING OUR KOP CLUSTER .......... 44:00
1) create ubuntuEC2instance in AWS
he used the master node we already created
##################When u ar craetin user in ubuntu server its diff frm  redhat

2) CREATE KOPS user
  sudo adduser kops
  sudo echo kops ALL =NOPASSWD:ALL | SUDO TEE /ET          ..... TO GRANT sudo right
SUDO SU -KOPS
2a)  instal AWS cli  ............     see the file in the github link for full steps 
 ******************* we are tryin to install kobs , it is a software taht wil help us to deploy a production ready k8 cluster
4) we already hv kubectl installed 
5 create iam role

6) aws s3 ls  .. to list the buckets but we access denied
he had to go to aws to create iam role for an aws service 50:00
then clicked on the master node, under action,security, modify iam role, and assigned the  iamrole we jsut created and update it
now when we run aws s3 ls , we no longer get permission denied bc we hv assigned iam role to the server.
###### now we create the bucket   .... aws s3 mb s3://kops33   (kops33 bucket name, i bliv)    ............53:59

6b) vi bashrc  and paste the name of crated bucket n exit then
source . bashrc to refresh the file
now if we run 
kops@master:~$  echo $NAME    ,, WE get a response
class33.k8s.local
kops@masteer:~$ echo $KOPS _STATE_STORE
s3://kops33
kops@masteer:~$    aws ls s3   .......   to ensure it exists 
it exists

7)  ssh-keygen      ............  57:27
8)57:40
the rule for kobs is that whatever u are craeting must end with .k8s.local
we can scroll down to see everything/objects kobs will create , including a vpc , autoscaling grp fro master n worker node, lb for the api server,even some aws resources, 
vpc, subnet route gatway, evrytin wil be created
**********is this similar to what we saw in terrafrom as terraform plan???  in terraform when you do terrafrom plan it wil tell u evrtin tht WIL BE Created
####copy the sshkey so that we can ssh to our master node  ....1:01:39

Suggestions:  we can edit    ..........1:01:47 
 

9) kops update cluster ${NAME} --yes
10
Suggestions:
 * validate cluster: kops validate cluster --wait 10m
 * list nodes: kubectl get nodes --show-labels
 * ssh to the master: ssh -i ~/.ssh/id_rsa ubuntu@api.class33.k8s.local

10b)   1:14:55 
   kops export kubecfg $NAME --admin
kubectl get node  ,,  we get a response

************ so we hv deployed our kops cluster 

how many clusters do we hv now?
now we hv kubadbm and kops cluster  , we v deployed a self manged cluster using kops
1:18:05 *********lets go to our github and deploy a particular application


1:19:47
vi app.yml
# Complete Manifest Where in single yml we defined Deployment 
#& Service for SpringApp & PVC(with default  StorageClass),
#ReplicaSet & Service For Mongo.
apiVersion: apps/v1                     ******* springapp deployment
kind: Deployment
metadata:
  name: springappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: mylandmarktech/spring-boot-mongo
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
        - name: MONGO_DB_HOSTNAME
          value: mongo 
---
apiVersion: v1             ************ service
kind: Service
metadata:
  name: springapp
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
  type: LoadBalancer
---
apiVersion: v1                     ************ persistent volclaim, we are nt creating persistent vol
kind: PersistentVolumeClaim
metadata:
  name: mongodbpvc 
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: apps/v1
kind: ReplicaSet                ***************** replicaset for mongo database rs
metadata:
  name: mongodbrs
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: pvc
         persistentVolumeClaim:
           claimName: mongodbpvc     
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: pvc
           mountPath: /data/db   
---
apiVersion: v1               ************** service for mongo called mongo
kind: Service
metadata:
  name: mongo
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort: 27017

1:22:30........... we ar nt creatin persistent vol  bt let me show u smt b4 we deploy this appl

kops@master:~$ kubectl get pvc
nO resourecs found in defaullt namespace   ... there is no pvc
kops@master:~$ kubectl get sc
we hv a default storage class  ... with aws EBS provisioner, so when u craeted using kops,what do u realise? kops created EBS vol for us, this did nt happen when we deployed
kubeadm cluster and since ther was no storage class u must manually proviison it, manually provision ur pv and pvc bt when ther is a storage class thatcan be automatic,that
becomes dynamic 

kops@master:~$ kubectl apply-f app.yml  ...... 1:24:57
created 
kops@master:~$ kubectl get po   , it screating the pods
created 
kops@master:~$ kubectl get pv            the pv has been mounted,  a mount point has been craeted, it has been bound to our db pod
 it has created the persistent vol since der is a default storage class
kops@master:~$ kubectl get pvc   , thsi persistent vol claim has been mounted, so a mount point 
cretaed
kops@master:~$ kubectl get svc
we can see it created a lb in aws 
when we used kubeabdm, when we did a similar task using kubeabdm it wasnt able to create a lb in aws, it is stil show pending, waitin for external ip......... 1:26:30
*********  he put the appl ip in google n we are able to write data

the lb that has been created is manged nt self managed bc lb created in aws is manged by aws

managed or self managed  LB 
if i create a self mangaed lb, wit the self managed LB, i may create a server n in the server, i wil install a software like ngnix afterwich i wil deter hw service is routed
self managed =
   NGINX   
                            1:31:00
but in a manged cluster, aws wil craete  the lb for u and in the background the LB which is our service will be performing dns resolution n routing traffic accordingly to
the backend port of the containers craeted on the nodes, this is what is goin to happen 
we can check lb in aws and all of this stuff was automatically generated, we can see the target instance(the backend instance, i bliv the containers ip), its routing traffic
to these servers and thats where our pods are running

but now, so this lb hw is it able to knw wher the traffic is been routed????
ns lookup means nameserver look, what is the name server that is linked to thais load balncer (below is our lb link frm aws)
************* nslookup 6773018340fe4c89b2cbc41d4db7611-1100378697.us-east-1.elb.amazonaws.com
when u run this in ur browser :6773018340fe4c89b2cbc41d4db7611-1100378697.us-east-1.elb.amazonaws.com , it is linked to a particular nameserver

 1:36:00  .....   frm an infrastruture perspective
How does traffic get to the PODS/containers running in kubernetes.
we v 50m users whaen they try to access this appl, they are typing the lb dns name , based on our deployment we v deployed a cluster wich has private n public subnet 
in our public subnet der is an Elb , SO endusers are trying to access the appl tru the ELB guy. insid ethe cluster we v node1 and 9 , frm an infrastruture perspective, we
v the frontend and we hv the backend , now  when ensuders tyep the lb url in the background a command wil be executed, the reQ  frm endusers wil query global dns servers 
searching for the nameserver so in d background nslookup is searching for the nameserver, this is taking mini seconds, some of d global dns service providers it wil check is 
godaddy, google,aws route53, once nslookup is executed it wil immediately route traffic  via the ELS url n then traffic gets into our cluster  .........1:56:00
(&inside our cluster we v a service called springSVC
n its routing traffic to backend pod , now any service u create in k8 must create a cluster service ip as well, he ran kubectl get svc n we cud see a clusterip attached to
the loadbalancer service that was created when we deployed the springapp service above,) so frm the elb, traffic gets into the cluster n for it to get to the cluster it wil
hv to get to the node n since its now in the cluster it wil identify which service the traffic is meant for n via the service traffic gets to the pods, bc traffic cannot go
to the pod directly the pods are discovered via the service n as such traffic wil get to the backend pod accordinly and as part of the spring app  we also deployed a database
pod , for the db we hv another service.. .. all our servers are runing in private subnet master n worker while ELB is in the  public subnet


*********************DNS = Domain Name service to create A rcords  
TO avoid giving this to a client :6773018340fe4c89b2cbc41d4db7611-1100378697.us-east-1.elb.amazonaws.com

we wnt to create a record , the cord can say:  .....*********** in aws we can achieve this with a service called route53 ............1:43:00
app.com ---> 6773018340fe4c89b2cbc41d4db7611-1100378697.us-east-1.elb.amazonaws.com
so instaed of the long link we use the alias name app.com
kk --->  https://github.com/LandmakTechnology/kops-k8s 
landmarkapp.net---> 6773018340fe4c89b2cbc41d4db7611-1100378697.us-east-1.elb.amazonaws.com
https://github.com/LandmakTechnology/kops-k8s
remote add kk https://github.com/LandmakTechnology/kops-k8s 

1:50:00 explanation




    2:05
 IQ: Explain your experience in kubernetes? 
I have over 6 years experience in kubernetes performing the following;
- setting up a multi-node self managed kubernetes cluster using kubeadm    ,kubeadm this is self managed
- setting up a multi-node production ready kubernetes cluster using  kops     ,, kops is also self managed bt it comes wit other aws services
- setting up a single-node self managed cluster using minikube and Docker Desktop for testing. (if u want to test certain appl you can deploy them on a single node cluster
- setting up a multi-node managed production ready k8s cluster
  using amazon eks  ******* may in a video i can show us how to
*****if ther are ant issues in these appl am i able to fix them? 
- troubleshooting issues from k8s setup/configuration or installation. ,, so v been able to trpubleshoot issues like trying to stup my kubeadm n its failing, or cant add
worker node to master node, v had issues like tht n hv been able to fix it by creatin a token in the master runing in the workernode n having them join the cluster
- maintaining, monitoring and upgrading the cluster components E.G   :
  scheduler, etcd, controllerManagers, kube-proxy, kubectl, kubelet,
  container-D, Kubernetes-cni[weave, flannel], kubectl-csi, apiServer  
  kops export kubecfg $NAME --admin 
- deploying applications and workloads using kubernetes objects:
    - pods/ReplicationControllers/ReplicaSets/DaemonSets,
      Deployments/StatefulSets/PersistentVolume/ConfigMaps and
      secrets
- using deployment as a choice kubernetes objects for stateless apps  
- using replicasets, volumes with persistenvolumes for Stateful apps  
- using statefulsets to deploy Stateful applications  
- rollouts and rollbacks of Deployments  ********** we can deploy new versions of the appl bc we knw what to do
- deploying applications using controllerManagers [RC/RS/DS/STS/deploy]
- setting up Jenkins-kubernetes integration pipeline for full automation ,, we wil also look at this
- deploying both Stateful applications and Stateless applications  
- making use of objects like; PV,PVC and dynamic storage classes to 
  persist data for Stateful applications [mongodb/ES/prometheus/jenkins]
- using configmaps and secrets for a secured application deployment   
- using probes for Health checks configuration in our deployments     
- using RBAC/namespaces/IAM for a secure access in the k8s. 

#### these are some few things i shud be able to talk about ..........

scheduler for scheduling pods on nodes  , this is the function of the scheduler n if for some reasn the sscheduler is nt running we wont be able to deploy any application
we can also check the sheduler pod :
ubuntu@master:~$ kubectl get pod -n kube-system    :  .... the kubectl has very imp pod that is used to run the cluster
NAME            READY       STATUS          RESTARTS             AGE
etcd-master      1/1      running            ....... we can see the etcd pod is running, if its nt running ther wil be a problem, it means we wont be able to have record bc it
                                       is our key value store, whatevr is happenin in d cl8 is stored here,like if u create a pod here, the record of the pod is found in etcd
 kube-apiserver-master     running                the apisever is the main administrative component of the cl8, the entrypoint to d cl8, if its nt running we cant mk api calls
                                               bc we cant run any of the kubectl commands, these are calls we ar making, so the apiserver needs to be healthy.

kube proxy .. if  its nt running all the networking will fail if the proxy is not working
kuectl : if it is nt installed, we can make api calls
kublet : if its nt running appsl cannot be deployed, its the main agent runing in our worker node that comm with the master
cointainerD:  k8is managing containers, if our container runtime like containerD is nt installed, we cant create and stsrt containers
cni: container netwrk interface, e.g weave ntwrk, flannel. if these netwrk interfaces are nt running, our nodes wil nt be ready
csi: container storage interface: if its nt running we cant deploy our stateful appl
********* all of these we hv to check if they ar working or nt and have been able to check all of these  2:14:00


we still hv to cover;
- helm  
its a package manger for k8 
such that we can run : [ helm create springapp ]  : to create the springapp
    
schedulling:
- scheduling in kubernetes  [ NodeSelectors / NodeAffinity / PodAffinity ] 
- EKS  
- EFK   
- Prometheus/Grafana  

 we wil look at hw to instal eks cluster

  



parameteres to consider when it comes to scheduling 
1)the default is resource 
2)node selector bt thers a problem with selector so we have
3) node afffirmity  : this is like selecting a node with some flexibility .. :with this we hv prefferred n required
  this is the new generation of node selector
4) pod affirnity: 

2:22:28
when an api call is made fro pods to be made,d scheduler wil wil schedule the pod on the node with sufficient resources but when all the nodes dont have sufficient resources
(this is the default expectation) the pod wil be in pending state..
the default is resource thata re available but ther are other options:
node selector : for e.g i wnat to create a pod wich is my fronend app,in this deployment, if i want to ensure thatbthe pod is only scheduled on the node that is optimized for 
frontend appl, i can select the node by using node selector, so i will select it based on the label, so the scheduler wil schedule the pod on the right node i want but if 
the pod i want doesnt have sufficient resource and thers anoda pod that has sufficient resource the scheduler wont schedule it bc the selection doesnt match that label therfor
my pod wil be left in pending state.

3)node afffirmity  : this is like selecting a node with some flexibility .. :with this we hv prefferred n required
e.g i can say i prefer u host my frontend appl on node2 but with node affirnity, if node2 doesnt hv sufficient resource the node wil stil be scheduled bc it now depend on the
constraint, bc if the constraint says it is preferred to schedule this pod on node2 bt if node2 lack sufficient resources we can schedule the pod on any other node with 
sufficient resources... this is affirnity
one other thing is possible here, we have preferred during scheduling and;
required during education   .................. bc with affirnity its like selecting a node with some flexibility

4(pod affirnity: we can decide that is one pod is in us1east, the other one shud go to useast2 etc
based on our topology, it says that let the pod be distributed accross diff availabilty zones , therfore if we ar deploying replicas, all of them wil nt be in one AZ, IT WIL
be distributed.


####        ######## he wil stil send the video


we still have helm
if you run:
helm create springapp ], it will create charts
lets assune u wnat to deploy an application, charts is like manifest files all i will do is just cahnge some information
 charts-manifest files [ 
it wil create a deployment for springapp: 
        deployment.yml,
if it reqs service , it wil create
service.yml,
if it reqs ingress, it wil create 
ingress.yml,
horizontal auto scale hpa.yml,
if my deployment reQS security, it wil create 
rbac.yml 

so evrytin that is needed  so once u run helm create, it wil create all my chart and nd all i may need to chnage cud be the pod,the tag, the imgage name bt everytin chart
wil be created for me 
helm is a package manager for k8, we wil look at it in nxt class




1:09:21 
when we use kops
kops create cluster : since our target is aws cloud, it gets into the aws cloud and in the aws cloud this command execute/achieves , it creates a series of resources e.g vpc
n subnets, re.g subnet1& subnet2 , LB, autoscaling grp managing some worker noder, masternode, we didnt need to create server manually 
with terraform we cud cretae resources in a cloud provide



2:36:35      QUESTIONS
1)FROM THE CL8 WE created using kobs i unsderstood it but, i knw we shh tru d ec2 insatnce, the server wher wecarried out the whole operation is der a way we can ssh into the
master node using mobaxterm or vscode remote host
****ANS: we did nt nid to sh into the master node bc we created a kops sever bt tk note that the kops server is nt our k8 master node, so frm the kops server we cud jst 
remotely mange our kops cl8, we did nt nid to ssh into the kops bt if we had to do that then we can stil do that

student : cos i knw we did nt create a key pair dasts why am asking, hw do we then do it ?
ANS: WE did nt nid to create a key pair bt we craeted sssh keys while deploying our kops command , when we ran the ssh gen key command and we had exported that key into the
kops as a secret n we can use the key.

 2) when u tal abt hw the appl comm in the cl8 like d springapp n the dtabase,, hw does dis comm is it tru the programm that mayb the springapp like the  one we were using
like when u put ur data automatically u see it showing in the database,,,,, is it the programmer that do the database that wil create the interface?   
ANS:  
the fact is that we are using k8 to orchestrate containerized appl,in our team we v programers who are writing code they hv written d software that wil permit users to write 
their name , dob number ,, they hv done that in the backend, we as devops engr we hv been able to build that code, create pipelines n we v been able to containerize the appl 
by creatin an image and pushin the image to dockerhub,, in k8 we can pull that image n deploy the appl, onec it is deployed we v an appl that is running which reQ usr to 
enter their inf and thos info needs to be captured in a database ..... how can it be captured ina database???? that becaomes a serious issue that is why we are now deploying
a db appl called mongo in this case,,, 
HW DOES THAT APPL COMM WITH EACH OTHER IN K8 ??? it is done via service discovery and within the cl8 we use the cl8 ip service.

STUDENT: so frm what ur saying, ders a reference in the code that will link the info straight to yhe database ryt
ANS: .. of course

3) i nw for k8 our container runtime is containerD, is ther  a particular reason why we using it,, is der a preference??
ANS: 
we learnt docker bc we use docker to containerize appl , once we use docker, we write the docker file, with the help of the file, we build images, the images are our appl
that have been packaged, now these packages we hv shared n distributed them to our image registry like docker hub for example, now der are 2tins involved here: containerizin
and deploying the appl ,, for u to deploy the appl it means that u hv to create the conatiner n start the container.. so the containerD in k8 is simply doing 2thing: it 
create and start the container that is all bc thats why containerD is running in the workernode that create n start the container so in the past we used to sue docker to 
achieve that purpose but k8 has deprecated dockcer tahts why we no londer use docker, we sue conatinerD

4) when u ran kops u put a pW, HW do u knw the PW to use 
ANS:
when i was creatin my kops user , user add kops
when u craete a user with ubuntu, ubuntu wil request for u to assign a PW to the user automatically , so when u ar createin a user, it wil REQ for u to assign a PW as well

5) whne u use kops to provison ther was an autoscaling grp that was craeted , hw does the autoscaling grp works, how is it dif frm the k8 horizontal pod autoscaler
ANS:    
...... very imp question, i thin i missed that when i was explaining.....let me go bk and explain that





