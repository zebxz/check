 

 
***** linux 1............  1:43:00  the remote host is the ip address of our server

please i cant remember that for now, can we come back to that later

I Didnt print
Sand 
Gateway 
Eks


****meee*********
we use maven for build and tomcat for deployment having integrated it with nginx webserver for loadbalancing & routing traffic from endusers to the appservers 
we bring in sonar for security, then nexus aas a repository for backing up our artifacts and jenkins to automate the end to end process (CI/CD)
Most importantantly or in addition to that; because security is inheerent in our environment we utilize AWS for various levels of security like ........
furtheer more we use docker & kubernetes for .....
and finally Ansible and terrfaorm for ..........


jenkin 7&8
someone is aking we hv nt deployed to tomcat ,, we are going to be deploying containerized app, tomcat was an introduction
  1:12:18 all of these is nt happening in one agent , mvn package will tk place in maven build server, sonar,the code quality analysis will talk to a 3rd party server wich is sonarqube server

2:04:00  we can use jenkins for non java applications and any application as expalined in jenkins shared lib video


LINUX , scripting, git , maven, tomat/ngnix tomcat, sonar, nexus, jenkins , aws, docker, k8, terraform, ansible, k8/helm 
In our env, we use LINUX OS for file,process,package,security mgt, GIT FOR VERSIONING, MAVEN FOR BUILD, JENKINS automates the end to end process, 
AWS for cloud computing services/resources, DOCKER for containerization ie deploying light weight containers, DOCKER SWAMP for container orchestration, KUBERNETES for
container orchestration/MGT, Orchestrate means to arrange.




*************************





 **** onLINE*** 
AI Overview
Kubernetes clusters can be categorized based on various characteristics, including their deployment environment, architecture, and networking configuration.
1. Based on Deployment Environment:
On-Premises Clusters: These clusters are deployed and managed within an organization's own data centers, offering full control over infrastructure but
requiring significant operational overhead.
Cloud-Based Clusters: These are hosted and managed by cloud providers (e.g., Google Kubernetes Engine (GKE), Amazon Elastic Kubernetes Service (EKS), Azure 
Kubernetes Service (AKS)). They offer scalability, managed services, and reduced operational burden.
Hybrid Clusters: These combine elements of both on-premises and cloud-based deployments, allowing workloads to run across different environments.
2. Based on Architecture and Availability:
Single-Zone Clusters: All nodes are located within a single geographical zone, making them vulnerable to zonal outages.
Multi-Zonal Clusters: Nodes are distributed across multiple zones within a single region, improving availability and resilience against zonal failures.
Multi-Regional Clusters: Nodes are spread across different geographical regions, providing the highest level of availability and disaster recovery 
capabilities.
Multi-Cluster Architectures: Involve managing and orchestrating multiple independent Kubernetes clusters, often for reasons like isolation, compliance, or
scaling across diverse environments. These can be:
Cluster-centric: Focus on managing individual clusters as distinct units.
Application-centric: Focus on deploying and managing applications across multiple clusters.
3. Based on Networking Configuration:
IPv4-only Clusters: Configured to assign only IPv4 addresses to pods and services.
IPv6-only Clusters: Configured to assign only IPv6 addresses to pods and services.
Dual-Stack (IPv4/IPv6) Clusters: Configured to assign both IPv4 and IPv6 addresses, enabling communication using either protocol.
4. Based on Local Development/Testing:
Local Clusters: Tools like Kind (Kubernetes in Docker), Minikube, or K3s allow developers to run lightweight Kubernetes clusters on their local machines for
development and testing purposes.
These categories are not mutually exclusive, and a Kubernetes cluster can exhibit characteristics from multiple types simultaneously. For example, a 
multi-zonal cluster could be deployed in a cloud environment with a dual-stack networking configuration.


**********************************************************************************************************************************************



KUB 3&4
DISADVANTAGES OF DOCKER    ****bc of the disadvantages of docker, we therefore use tools like docker swamp, kubernetes & open sheet.     

 we can use software like docker to deploy applications ,to deploy n mange containers bt is it recommanded, shud we use 
docker ??? we shud nt and u are going to see that shortly, we shud rather use container managers or ochestration tools like docker swarm kub n open sheet.
when we studied dodcker, we deployed appl using docker bt what are the limitations using docker to deploy applications, we ar going to look at that shortly
7: 43 bt these ar the contarization software  ... check

with docker i cannot v more than 1 replica of webapp for e.g u are deploying ur appl n ders a spike , no i cant v multiple replicas in docker , e.g: so if u decide to run
docker run -name webapp -replicas 5 , ie trying to get the number of replicas u wnat, it wont work.. docker doesnt support it
also, if my engine goes down , my contaiiners will stop running 
also docker doesnt support a scenario wher i can v multiple serveers in a cluster  e.g docker engine1 (with container), docker engine2(with containers) such that if one 
engine is down the 2nd engine can support it ,, docker doesnt v such sys bc the docker ntwrk dose nt support MULTI HOSTING where we can v a cluster of servers, 
docker netwrking sys doesnt support multi hosting and thats a big problem ,, the docker system doesnt v an overlay ntwrk wher u can v a cluster of server,
no overlsy ntwrk support and d worst is that when appl ar deplloyed, to expose them to endusers is very limited and we wil see hw it is diff with kub...
bc in docker we do port forwarding bt we wil see what kub brings to the table  .. i bliv port forwarding is how we expose the application (on your internal 
netwrok) to the internet


    HOW Did WE DEPLOY A WORK LOAD IN DOCKER ??
we had to use docker resources or objects to deploy applications like docker file we use it to build images n we can now deploy our appl wit the help of 
ntwrks, vols, using the docker-compose file.
........docker resources/objects use to deploy applications:
  Dockerfiles/images/networks/volumes/docker-compose.yml/etc. but the docker compose file doesnt have the 'deployment strategy' so we didnt hv the option of 
replicas


DOCKER 1&2
 APPROACH USED TO DEPLOY WORKLOAD IN DOCKER 

WE DEPLOY workloads using Imperative and declarive approaches    

1-#Containerize application Imperative approach =  Command
   LIKE:
docker build -t teslaimage:1 .
docker run -d -p 80:8080 --name webapp teslaimage:1

2-declarive approach= DOC7.. makes use of deployment script *ie(vi to create eg spring.sh & put the image,db,netwrk command) & compose files/less commands 
Docker Compose is a tool for defining/declaring and running multiple containerised micro-services applications.
-# Using compose file     *** but this compose file doesnt have the 'deployment strategy' like docker swamp & kubernetes
  **while d docker compose file for docker includes netwrk, d compose file for docker swamp does nt include netwrk, because an overlay netwrk has been initialized


Docker 10
2.58.56****** WHEN U RUN DOCKER SWAM init, 
By default it creates an overlay ntwrk, the overlay ntwrk is what permits your docker server to communicate in a cluster like this bc in normal docker der 
is no overlay ntwrk for that reason, you cannot v docker servers in a cluster like this bc node1 cannot talk to node2 in normal docker
so once u v the overlay ntwrk, when u create ur containers , when you create ur services, those services can talk to each other irrespective of whether they
ar created in node1 or node10 bc of the overlay ntwrk n the DNS registration is done using the service names

            OVERLAY  NETWORK
whenever we ar deploying in a swarm, our containers wil be created in the overlay netwrk..overlay ntwrk means that u can communicate wit dif nodes, the master
can communicate with multiple nodes using the overlay netwrk, with overlay the container in node1 can talk with the conatinters in all the other nodes.


DOCKER 10
docker swarm has 2modes: REPLICA MODE & GLOBAL MODE
1)Replicas ---> it will deploy based on replicated number    (we v used this mode to deploy above)
***** this is the default mode & with this mode, it wil deploy base on the number of replicas that you want     ... 2:11:24
2)Global--- its like daemon set in kubernetes, its good for deploying logmgt container for monitoring
with the global mode for e.g, if u v a cluster that has 14servers or 14 nodes(3master + 11 workers)
if u v a 14 node cluster n u want to deploy an appl using the global mode what wil happen is that a replica wil be created in each of this node 


KUB 1&2
HOW WE DEPLOY APPLICATION IN K8
########### hw we deploy applictaions in k8 , which is a very key aspect when it comes to my understnading of k8, 
 OBJECTS USED FOR DEPLOYMENT
kubernetes resources/objects used to deploy application include/RUN WORK LOads:
 1)  Pod : ***pod is nt recommemded bc we cant scale bc the pod template doesnt hv the option to create replicas like controller managers, so when a pod dies 
or                                                                                             its nt recreated and that is dangerous.
2)controllerManagers:   ***Deploying with controller managers we are able to scale
      Replication Controller; uses only; matchlabels which is Equality based 
        ReplicaSet; uses both matchlabels which is Equality based &  matchexpression which is set based:, can rol out new version but cant roll back
        DaemonSet,  creates a pod in each of the nodes.. good to use to deploy logmgt for monitoring purpose
       StatefulSets, 
       Deployment,   has strategy like 'Recreate' OR 'rolling update' which is the default strategy, can roll out new version & can roll back
       Volume, Job  

 3)                                     TYPES OF SCALING
Manual scaling (Controller managers) OR Auto scaling the 'replica' using either of these objects:(horizontal/vertical/cluster autoscaler)

    



      

*****************************************************************
KUB 3&4*    POD
in k8, the pod lifecycle is very short so if we ar going to deploy appl, we shud nt use pod
We should not create pods directly to deploy applications.  **kub5**     lacks self-healing capacities
If a node  goes down in which pods are running, Pods will not be rescheduled.
We have to create pods using controllers which manages the POD life cycle.
controllerManagers: ReplicationControllers , ReplicaSets, Deployments, DaemonSets


    **mee**  WHAT A MANIFEST FILE CONTAINS/DETAILS FOR EACH CONTROLLER MANAGER & OTHER K8 OBJECTS/CONCEPT
1)**Mee*** therefore for Rc, in the manifest file, kind will be 'rc' & we will also hv a template for pod inside the same manifest file & in the RC section,
we have selectors & spec,for spec; we will indicate the number of replicas but if we dnt,it wil create 1 replica & if we need to create service, we wil also 
have a template/section for service
2)for RS, in the manifest file, kind will be 'rs' & in the replica set section, we have selectors & spec, for spec, we will indicate the number of replicas,
& we also have a template for pod inside the same manifest file & if we need to create service, we wil also have a template/section for service
3) for daemon set, kind will be Daemon set; in the Daemon section, for spec, we dnt indicate no. of replicas bc because daemon schedules pods on all the nodes
so we dnt need RC/RS, because there is no replica template, i bliv thats why we cant scale with daemon set. & we also have the template for pod, & service.
4) Deployment rolls out/deploys a replica set therefore; in the manifest file, in daemon section, we have spec & selectors, for spec; we 
 we will indicate the strategy type eg rolling update or recreate, but if u dnt indicate the strategy rolling update is the default strategy & if u dnt
indicate the replica, i replica will be deployed & we also hv the pod template & service template
      ***controller managers end****
 5)                        AUTOSCALING
****The manifest file states the minumum& maximum number of replicas of the application/pod to maintain at all times.

6)          KUBERNETES VOLUME CONCEPT
In k8, we can create volumes with or without PVC    ***for it to be persistent, its adviced to use PVC
1)   HOSTPATH
 a) Without PVC; the hostpath created is like a mount point just like in 'DOCKER bind mount', we hv to create a mount point & its not persistent
 b)With PVC; the hostpath created is the PVC,  its mg8 by kubernetes so its like 'Docker vol' we created in docker , its persistent bc its mg8 by docker 
   it has 4 manifest files  .. but we can use ansible
2)    NFS OR EFS 
     a) Without PVC; the NFS/EFS created is like  external vol 'EBS' we created in Doker, its persistent but i think not mg8 by k8
      b) with PVC; the NFS/EFS created is mg8 by kubernetes so its like 'Docker vol' we created in docker , its persistent bc its mg8 by docker 
            it has 3manifest files... but we can automate with ansible



                   ***meee***        STEPS FOR OCHESTRATING CONTAINERS
1)After installing either self managed (multi node preferred), OR  paid to manage
2)create namespace, add label in pod template, so service can make pod discovered/accessible,
3) create service type either clusterip or nodeport (using service template)
4)chose a controller manager, k8 object for deployment prefered 'DEPLOYMENT' &Its rolling update strategy, although he said smt about blue green techniques, bt didnt apply it.
5) Resources request & limit ie cpu/ memory for the pod/application
6) Autoscale the controller manager
7) Volumes & NFS
the NFS server has to be launched in the same vpc like the other servers ie k8 servers, like the master node . *** mee** like all the servers in the cluster..
can create a security grp for nfs with the REQ ports open like  ssh  port ffrm anywher, nfs 2049 
Configuration of NFS Server ( WE CAN USE ANSIBLE TO AUTOMATE THE PROCESS)


                   CONTROLLER MANAGERS eg RC,RS
1.42,00   Cretaed pods using Replication controller, when we delete them, we see they are RECREATED.
all the pods have been rescheduled/recreated bc my pods are been manged by a 
 Replication controller n the RC has a label app:web n the RC has a selector that has selected the label, once it is
 selected, the pod labels hv defined that the replicas is 3, so at all times, the RC must maintain d desired state of my appli which is 3 

***my own explanation, the pod template has a label & the RC section has a defined the number of replicas & the RC also has a selector that will select the 
label of the pod template & as such, the RC will maintain the defined number of replicas for the app/pod at all time.


advantages of namespace
****clusterRole &clusterRolebinding is for the entire cluster 


 ****in manifest files;
pods have label .. ie key value pair
ReplicationControllers has selectors which is used to be able to connect to the pod label


LABELS & SELECTORS EXPLAINED
****SERVice makes pods accessible using labels & selectors

**************************************************************************************************************



KUB 1/2 & 3/4.. is all about types of k8 cluster to we can install, installing d multi node cluster, creating namespace, how applications are deployed ie usng
pod as a object OR controller managers like RC, RS but we shudnt use pods bc of the short life cycle and inability to scale, also we hv the way we deploy 
workload either using the imperative/command approach or declarative/manifest file approach.
we have then the service discovery ; we have clusterip service & nodeport service.. clusterip is for internal comm within the same ns BUt to est comm from a
diff ns we hee FQDN  while nodeport is for externall comm ie been able to
comm into the cluster from outside , Then to use k8 to pull frm docker/private reg need to create secret login for dockerhub.
KUB 3/4,5,6 & 7, We explored the diff k8 objects used for deployment

 



   SUMMARIZED FOR INTERVIEW

KUB 1&2
XPLAIN THE K8 ARCHITECTURE 
kubernetes  is a open source, or ochectration  tool  used to Orchestrate and managed containerised Applications
k8 is a grp of servers that are working together to be able to ochestrate containerize applications, its made up of a control plane & worker node, where the
applications are running & our applications are deployed in pods & the control planee comes with components line the API server which is the main
adminstrative point and the entry point into the cl8 & it does generally authentication & authoriztion & it ensures that whatevr Req GETS into the cl8 is 
been made by an authorized & authenticated individual, apart of the API server, we hv the etcd that persists data in the cl8, it acts as a database ,whatevr
reQ is been made by whoever engr that inf is stored and persisted in the etcd and frm there we also hv the scheduler, which is incharge of scheduling  eg if
pods are been REQuested to be scheduled the sheduler will schedule that considering the nodes that hv available resources for such a pods to be hosted & if
thers any other constrain all the constraint are been factored in we also hv the controler managers that is able to ensure that at all times we hv a number
of nodes running ,pods running ,deplyments are runing as expected.
we also hv the worker nodes comes with the kublets which is the pry node agent, we equally hv the kube proxy, which is incharge of netwrk 
also the container run time, there are diff types of container run time that you can use, docker used to be one of them , also container-D, container runtime
is any software in your node that is able to create and start containers bc k8 is all abt managing containerized appl n in our env we use containerD as our 
container run time, we cud hv as well choosen other run time but we are using containerD

Also the k8 architecture is such that calls are made into the cl8 and that happens using the k8 client like kubectl or k8 UI form there we can mk certain API 
calls either to create containers, namespaces, security like RBAC and for any of these to happen, authentication must happen via the kube config file
so thats the k8 architecture


KUB 3&4
FOR autHORIZATION;
authorisation via RBAC: Roll base access control, 
this is under kub security , u hv to be authenticated n authorized to perform that task
once u are authentictaed n authorized , the task is being executed bt if u werent succesfully authorized, u wil get permission denied/ authentication failed

After sucessful auth/author, 
in docker we deploy our containers directly in containers, in kub they are deployed in what is called pods
so pod is the smallest unit in kub
so u wnat to deploy an appli, dis is what happens, the API server receives this information either frm UI or kubctl, n immediately it receives it, it will
persist the data in our key value store ie etcd,, (i bliv UI is using imperative/declarative & kubctl is the way we create static pod using the 'etc/' dir)
now u wnat to deploy your appl which are deployed in pods .... in your cluster, we hv the worker nodes that comes with the kublets which is the pry node agent
therfore, the control plane is communicating with the worker node via this pry node agent ,once it commun wit the worker node ,it wil be checking for resouce
availability u wnat to deploy a pod , this is a scheduler, this pods requires abt 800mb bt when it comes to availabilty: my node1 got abt 700mb of memory, 
node9 got abt 8000mb of memory we hv d scheduler, hes going to schedule the pods based on resource availabilty & other factors, all other factors are constant.
At this time the scheduler will do automatic scheduling on node9 which has enough enough resource availability.now when the pod, lemme cal it 
webpod is scheduled on node9, inside the webpod containers nid to be created now the container runtime wil create n start the container, so now i v my 
webcontainer inside the pod.this is the kub architecture, very powerful 
now i v my appl scheduled on the pod, and we nid to deploy multiple replicas , in this case we v 2replicas, the scheduler has scheduled the 2replicas on the 
2diff nodes.therfore 2replicas of the pod have been deployed
inside this cluster we wnat to ensure that we can access this pod, for the pod to be made accessible we ar going to make what is called a service in kuber.
35:34
so u v a service , this service act as a LB
E.G I V A webSVC, what wil happen is that if u wnat to access this appl running in this pod in kub, it is going to be made discoverable using the service
so if u wnat to access the appl, it wil be routed via the service, we call this service discovery, u wnt to discover ur appl, the service is involve
now this entire netwroking wher appls can accessed with relative ease is been suported by a ntwrk agent kube-proxy.

###########if u can xplain dis kub architecture, u ar as good as done 37:26



CLASS 33 vide 5&6.... 52:30 .... i will watch the video later...........
************* for each of the version it goes tru this steps: we create the packages, then run docker build to contianersrize the appl and ship the image with
docker push to the image registry 

this is an extract of the entire CI/CD process  .......7:58 ....(qusetion n answer vidoe 5&6)
b4  we containerize, containerize, b4 we ar creatin images, we had gone tru code quality analysis, and depending on the type of project, der are projects were
we execute sonarqube analysis b4 even creating packages wit mavin,, so if u do that u are good to go, so dis is jst a summary  bc we didnt hv nexus or sonar 
qube connected in this k8 but on our CI/CD pipeline, we hv developers write code, developing new update of the appl, and once they do that they commit to 
github and we can connect github to soanat cloud wer once der is a new commit, sonar cloud wil run code quality analysis, its only when that process passes,
wher our sonarqube or sonar cloud report indicates that it has gone tru all our quality threshold, evrytin is fine , der are no vulnerability in the code, no 
smell, bug n issues , then once we v realised that then we now bring in maven wich then creates packges , once maven is done we upload artifacts into nexus
and frm ther we can now build our image using docker, once image is built we use k8 4 deployment.

INORDER FOR US to update the new versionin on the deployment session on k8, that is done by webhook or pull ACM 
*************SOME ONE SAID ,, with the help of the webhook created in github wich is triggered when thers a new comit, github  then notify jenkins to do a
build ...32:17

*******i will share the jenkins kubernetes integration, it covers evrytin am tryin to xplain to u now, ,, a lot of folks have had jobs by just goin tru that
video, hw all this tools connect to ecah other




**************************************************************************************************************


******if we dnt pass replica, its goin to create one...

*me*Docker is 'compose file' deploy with command 'Docker compose up -d' WHILE kubernetes is 'manifest file' deployed with command 'kubectl apply -f pod.yml'


                           TYPES OF OCHESTRATION TOOLS
Container Orchestration Tools --> :
   Docker Swarm, Kubernetes, OpenShift



START

KUBERNETES
IT is an ochestration engine, an open source platform for managing containerized applications... kubernetes is responsible for container deployment, scaling, descaling of
containers and lB.
actually, kubernetes is nt a replacement for docker bt can be considered as a replacement for docker swamp , kuber is basically more complex than swarm and REQ more work to 
deploy.. born in google written in go and go lang
when we talk abt kub, we classify an all imp tool bc of its imp  of to me as a devops engr, its so important that one of the jobs i can do is a kubernetes administrator 


KUB 1&2 .... 1.43.50
kubernetes uses the kubectl client or the UI to run workloads.




TK NOTE OF THIS TREE
kubernetes:    kub has a cluster , 
   cluster: it is a grp of nodes
   nodes:  in the nodes ther are pods 
   pods:  in the pods ther ar containers ..  
   containers :  

therfore in kub we can say that nodes are the subset of the cluster , pods are subset of the nodes and containers are the subset of the pods
cluster ---> nodes ---> pods  ---> containers  :
********therfore if u are going to deploy any appl in k8, it is going to be deployed in a pod

PODS:
====
POD --> Pod is the smallest building block use to deploy applications in k8s.
Pod represents running processes. Pod can contains one or more containers.
These container will share same network, storage and any other specifications.
Pod will have unique IP Address in k8s cluster. 

Pods
 SingleContainerPods --> Pod will have only one container.       98%
 
 MultiContainerPods(SideCar) --> POD with two or more containers. 2%  
e.g, we can v the  the application container and like a sidecar container
       application Container :
          e.g  webapp    , e.g u v a container running and u wnat to ensure this appl doesnt v any problem, so u attach a logmgt container so that whatsover is 
       SideCar containers:                                                                             happening in the appl, this log is reporting it
            e.g  logMgt  container   ... like a container that is collecting log from ur application,  can als have like a utility container for supplies



 cONTROLLER MANAGER.. >>>>>>>>>
E.G IF U deploy ur appl and you want 3pods to be deployed , lets assume 2pods shud be running, if 1pod is down, the controller manger has been instructed to create 2pods, if
one pod goes down, it will automatically create the pod, those are the function of controller mangers
*** Meee.. it knows what to do at any given time, eg one pod is down, it automatcally brings up another one.



fEATURES:    *** OF K8
1)the first feature it has is that it supports what is called automatic scheduling ie it provides advance scheduler to launch contianers on node
2) it has self hidden capabilities, rescheduling, replacinging and rescheduling containers which are dead and the bauty abt it is that if u deploy a wrong application u can 
go back to what was running before so it has automATED ROLBACKS AND rollouts 
so kuben supports rollouts and rollbacks for the desired state of containerized application therfore, if u had a version running and u v a deploy a new version n ders a 
problem, u can rollback 
3) it has horizomtal scaling and LB , kub can be scaled up and down d appl as per REQuirment
4) service discovery & LB
5)WITH kuber DERS no nid to worry about netwrkn n communicatn bc kub wil automatically assign ip address to containers n a single dns name for a set of containers that can LB
TRAFFIC inside the cluser .... 5:00
containers get their ips so u can put a set of containers behind a single L Balancer ... this is a very key aspect service discovery n LB , hw powerful is that 
6)it also supports storage ochestration, u can mount the storage system of your choice , u can either opt for local storage or choose a public cloud 


          KUBERNETES CLUSTER & ITS COMPONENT

kubernetes architecture:  why are we doing kub and why is kub imp in my devops journey , that in all thy knowings u must knw kub
                                ############  kubernetes comes with a cluster in the cluster we v :
controller noder or controlPlane/MasterNodes: this has some components to tk note of:
  apiServer             : this is the main entry point into the cluster, it performs admin task in the cluster
  etcd                  : the key value store ,,, this is like a database
  scheduler  : 
  controllerManagers   : 
workerNodes           :  this is wher appl are running and here we can v node1, to node9  . the components of the nodes includes:
  kubelet             : this is the main agent that runs on the worker node. its the pry node agent
  pod                 : is the smallest unit in kub, in kub contaners are deployed in what is called pods
  container runtime   : and d container runtime we ar going to be seeing is = [Container-d] 
 kube-proxy           : the netwrk agent
21:30
############### in kub if u wnt to perform any task e.g u want to deploy an appl , for that deployment to happen ther are a few things that kub uses, we call it d client
kub client (how u can access ur cluster)  for e.g we hv docker client , the docker cli
22:11
kubernetes-client      : we also hv kub cli , command line interface ,, command line interface here is kubectl
  kubectl              : if u are authorized, u can run 
     kubectl create/delete/get/describe/apply/run/expose 
  so we v the cli which we can use to make some api calls and we also v other APIs 
  ui- DASHBOARD  
  api  ,, others APIs
Or it has a UI wich is called  a dashboard ,, since we started this programe wher v we see any dashboard , to deploy in jenkins we were able to go to the browser,  der was 
a nice dashboard wher we cud create and run jobs
Now in kub, to perform any task in ur cluster,  for docker we cud do stuff like docker build, docer run n our docker client was docker bc all the commands started wit docker
our kub cli is kubectl ... this is our kub server, u want to run a job in kub , u run commands like kubectl run to deploy an appl, what happens is that kubctl is like a msg
that is sent into ur cluster , dis msg comes to ur cluster, u wnt to deploy an appli, so it cud either b frm ur dashboard, or  ur cli. then regrdless of wher the msg is 
coming from, d API server receives the msg on behalf of our cluster,d API sevrer performs admin task n when u execute dis command, this will get you tru authenticaton,so  u
nid to be authenticated n authorization,, 2things wil happen now u v executed kubctl run, wich means u wnt to deploy an appl, for dis to happen, kub nids to authenticate n
authorize u and the authentication and authorization process wil tk place using a file called .kubeconfig file . this is the file dat authenticates you

kubeconfig [.kube/config ] file will authenticate the admin    ....(ur tryn to mk an APi call, tryn to dd smt to b done) so u must be authenticated) 39:34
                               the caller admin/Developer/Engineer 




 AUTHENTICATION & AUTHORIZATION

FOR autHORIZATION;
authorisation via RBAC: Roll base access control, 
this is under kub security , u hv to be authenticated n authorized to perform that task
once u are authentictaed n authorized , the task is being executed bt if u werent succesfully authorized, u wil get permission denied/ authentication failed

WHO CAN DO WHAT, who can perform what task .. ,, THIS WILL BE SUPPOERTED BY KUB SECURITY  .. 40: 26
kubernetes security - RBAC:
  Developers [ Paul, Joyce, Chidi ] 
  Engineers  [ James, Dominion, Janet ]    .... we can assign them diff functions

authentication via kubeconfig : 
authorisation via RBAC:

  AUTHORIZATION IN AWS
         to be authorized to create an aws EKS cluster in aws, i can create an IAM Role for EKS , this wil permit us to create and manage an EKS cluster
it wil be the same thing for Azure AKS, goodle GKE and Ibm cloud

************* so if i v like 10clusters that i wnt to manage, wich i wnat to be observing hw dey are performing at all times then i need to instal the rancher 
sfware so i can use it to access those clusters



                                           2TYPES OF KUBERNETES CLUSTER YOU CAN INSTALL
1) Single node cluster 
2) Multi Nodes Kubernetes Clusters   .. This is our focus....


                             WAYS TO MANAGE KUBERNETES CLUSTER
you can deploy your cluster using, either a manged or a self managed sfware
ideally, companies will prefer to go for managed bc it will give them more time to focus on the app , bc if u nid to focus on the architecture, it wil mean
that u wil need to spend more time ensuring that ur cluster is running than managing containerized appl,,  bt more time shud be dedicated to appl management.


the first of its kind which we can install is : 
1. Self Managed Kubernetes [k8s] Cluster = IaaS--EC2  :  thess ar infrastructure as a service , we can launch of k8 cluster on a grp of ec2 instances, using kubeadm
    kubeadm --> We can setup multi node k8's cluster using kubeadm.  ... kubeasm is a sfware that is used to launch a k8, both d control plane n worker node
    kubespray --> We can setup multi node k8s cluster using kubespray , it uses ansible playbook
     (Ansible Playbooks Used internally by kubespray).
WITH  Self Managed Kubernetes [k8s] Clusters both the    
     controlPlane: [apiServer, etcd, scheduler, Controller Managers] 
      and 
     workerNodes: [  kubelet, containerRuntime-Container-d, kube-proxy]  
  are managed by the Admin/Kubernetes/DevOps Engineers

*************** so if u have a self managed cluster u hv to ensure that the control plane wit all its components re healthy n functioning and also the components of the 
woker nodes are all running and healthy, it is ur function

we v anoda class of clusters wich are  manged by 3rd party, u will nt bother abt the mgt
2. Managed k8s Cluster  (Cloud Services) = PaaS  : 
   The controlPlane is managed by a cloud provider or third party.  
   The controlPlane and all it components are managed by the Cloud provider 
   ***************  howerever,   The workerNodes are managed by the Admins or engineers

        
         TYPES OF CLOUD manged service:    ***I bliv for ochestrating contaners
   EKS --> Elastic Kubernetes Service(AWS)
   AKS --> Azure Kubernetes Service(Azure)
   GKE --> Google Kubernetes Engine(GCP)
   IKE --> IBM K8s Engine(IBM Cloud)
    Kubernetes Cluster = k8s  

########### so u can deploy your cluster using, either a manged or a self managed sfware
ideally, companies will prefer to go for managed bc it will give them more time to focus on the app , bc if u nid to focus on the architecture, it wil mean
that u wil need to spend more time ensuring that ur cluster is running than managing containerized appl,,  bt more time shud be dedicated to appl management.

*******************we also v another clas of cluster that can be installed ... KOPS

    
KUBERNETES DISTRIBUTION TOOL

****ONLINE
Kubernetes distributions and tools: Several open-source tools and distributions assist in deploying and managing self-managed Kubernetes clusters, offering 
varying degrees of automation and opinionated configurations. Examples include:
Kubeadm: A tool for bootstrapping a minimum viable Kubernetes cluster.
Kops (Kubernetes Operations): A tool for building, operating, and maintaining production-grade Kubernetes clusters in the cloud.
Rancher: A complete software stack for managing multiple Kubernetes clusters.


3. KOPS: is a software use to create production GRADE/ready k8s in AWS and  
         azure for the kops beta version  
         It creates a highly available kubernetes services (he said cluster, mayb a mistake) in Cloud like AWS.
            KOPS will leverage Cloud Sevices like: .. so when u deploy wit kops, it comes along with all dis services n its very esay to craate a kops cluster 
              vpc, 
              AutoScaling Groups, 
              LoadBalancer, 
              Launch Template/configuration
              ec2-instances nodes [workerNodes and masterNodes]



                             A TOOL WE CAN USE TO MANAGE MULTIPLE CLUSTER 
Rancher: - Using Rancher we can deploy both managed and self managed k8s CLUSTER
           Rancher serves as a glass to access and manage multiple k8s  
           from the rancher dashboard [UI]  - rancher dashboard  ,,,, frm rancher i can create a cluster in EKS/AKS/GKE/IKE 
           authentication and authorisation: EKS/AKS/GKE/IKE  ... bt u v to be authorized




DEPLOY/INSTALL MULTI CLUSTER

DEploy master node:
in mobaxterm connect to the server then set hostname to master  and sudo i to switch to root user
in the installation script first, we v to 
we nid to diasable swap n kernel setting,so we wil disable swap memory so that we can enhnace performance in our k8 cluster , then nxt, we ar adding kernel
details n instal container-d, first instal container-d n its dependecies bc our runtime (to b able to start&runour conatiners) is nt docker bt it is container
-d, once it is installed, we will proceed to start container-d  afterwhcih we wil instal kublet, kubeabm and kubectl,, after that we wil start the kublet
service i will run this as a script and i nid to be root user
Initialize Kubernetes control plane by running the below commond as root user.
sudo kubeadm init
****************** but am a root user, so i dnt nid sudo. ... just run kubeadm init 
and for us to be able to initialize this control plane the req port must be opened. 
************************** SUCCESSFUL... now i can process as a regular user

to launch nodes/Wokers:   ***************

8)  Generate the master join token on the master node
so am going to generate the master token on the master node
kubeadm token create --print-join-command                                                                               1:14:44
once the token is created,



K8 BEST PRACTICES FOR LARGE CLUSTER

lets look at k8 BEST PRACTICES FOR LARGE CLUSTERS     *********** so if u have a very large cluster you can click on this link, u wil get more info
https://kubernetes.io/docs/setup/best-practices/cluster-large/
For very large clusters NB: dnt create :
   No more than 5,000 nodes
   No more than 110 pods per node
   No more than 150,000 total pods
   No more than 300,000 total containers


WE WANT TO DEPLOY WORK LOAD IN k8
***ticket002
========
************************************************************    Deploy workloads in kubernetes;
kubernetes resources/objects used to deploy application includes:
kubernetes Orchestrate and managed containerised Applications  
This applications run as containers  
These containers are housed in pods  
Pods are housed in nodes    
nodes are housed in the cluster   
********therfore if u are going to deploy any appl in k8, it is going to be deployed in a pod

now we v created our k8 cluster ,how are we able to deploy appl using this cluster ,, ????????????? dats some of the things we wil be able to look at:


    OBJECTS USED FOR DEPLOYMENT
kubernetes resources/objects used to deploy application include/RUN WORK LOads:
   Pod :
or    
 controllerManagers:
      Replication Controller
      ReplicaSet
      DaemonSet
      StatefulSets
      Deployment
      Volume
      Job       


when u deploy appl in k8, u v to make the appl accessible ie exposing appl/accessing appl
Exposing/accessing applications = Service Discovery:
    Service Types:  it will route service to our appl n their replicas ,
    ClusterIP :  it performs LB  , its used for internal communication inside the cluster.
    NodePort
    LoadBalancer
    ExternalName  
  ingress 
  networkPolicy 




                                 NAMESAPCE     ***i bliv its kinda similar to label & restriction in docker swamp
1:34:30  **********************generally for this deployment to tk place even in k8, we will make use of a namespace
how DO WE ENSURE THE APPL ARE ABLE TO COMMUNICATION  & all of that , ders a concept in k8 called:
Namespace:
  It is a virtual cluster inside your cluster  ,,, we can create a name space for dev stage, uat stage , prod stage , sales ... dependn on d project u are managing
     [ dev / uat / prod ],                            if u create a namespace for e.g, u wnat a situatn wher what dev ar doin does nt affect what is runin in uat &prod
     [sales, accounts, cs, payroll]                so we can use name spaces for isolation, we'l isolate d dev env frm uat, frm prod using namespaces





 APPROACH USED TO DEPLOY WORKLOAD IN K8 ????????????????

WE DEPLOY workloads using Imperative and declarive approaches    

-#Create Name Space Using Imperative approach =  Command
   LIKE:
kubectl create namespace <nameSpaceName>
    kubectl create namespace dev  

-declarive approach =  makes use of files and less commands  
-# Using Declarative Manifest file 

 Use the declarive approach to deploy workloads in kubernetes:
  Manifest files = kams       ........ we use manifest files to deploy in ddeclarative approach
  Manifest files are written in yaml/yml language 

pod.yml  : in this yml language, it deals with stuffs like :
key:value  pairs     
dictionary: number of key:value pairs 
list:

                        KEY VALUE PAIR
hw do we consider keyvalue pair 
**************for e.g when u want to create a manifest file, it has this acronym, comes :   where 'comes' stands for kind, api version, metadata and spec
kind is a key value pair bc for example ; kind is pod , this is a key value pair, it has a key & a value  ,,, so key &value ,, kind:pod
for e.g , key value pair could be  name and the value is simon
key:value  pairs 
name: simon   
 

K8 BEST PRACTICES FOR LARGE CLUSTER

lets look at k8 BEST PRACTICES FOR LARGE CLUSTERS     *********** so if u have a very large cluster you can click on this link, u wil get more info
https://kubernetes.io/docs/setup/best-practices/cluster-large/
For very large clusters NB: dnt create :
   No more than 5,000 nodes
   No more than 110 pods per node
   No more than 150,000 total pods
   No more than 300,000 total containers




 APPLICATIONS IN OUR REGISTRY   ... ..... these are all images we can use to depploy our appli in k8
in our image registry , we have applications like 
Docker images = dockerHub other registries:
-- python-web-app ,  nodeweb-app,  net-webapp 
   mylandmarktech/hello,  nginx,  mysql,  mongo  
   jenkins,  sonarqube, nexus            



KUB 3&4
 SET CONFIGURED NAMESPACE TO CURRENT NAMESPACE
originally, the default namespace is the 'current namspace'
*Mee*by default, kubectl get po' lists the pods in the configured ns, BUT you hv to run the 'kubectl config set-context' command, to set the ns you configured/
created to be the current ns


  LABELS & SELECTORS EXPLAINED
****SERVice makes pods accessible using labels & selectors

********************* for a service to deiscover any pod, its going to be using labels, so what is the label of the pod
so we v labelas n we also v selectors...  so ders a label that says app is mapped to hello, its under the pod, so we v Pod label, so 
under service, we ar goin to v sevice selectors
in my k8 cluster der wil b many sevices,  hw wil the service be able to identify this pod,, it nids to create a selector that matches this pod like that 

***online
No, a manifest file's label does not have to match the container name, but they are often related and must match for certain tools to function correctly. 
For example, in Kubernetes, a manifest's metadata.
labels are used by other resources like a Deployment's selector to identify which pods to manage, not to name the container itself. 
 


SERVICE DISCOVERY
ServiceDicovery:
==============
*********the first service type is:
ClusterIP is the default kubernetes service type that support
communication within the cluster.    *** Used for internal communication
************** to communicate within our cluster, we will use a clusterip service type



What is FQDN?               ..........          45:21   in kubernetes by default we create containers in the same namespace to enable them comm wit each oda, however, 
FQDN = Fully Qualified Domain name.                                                    
If one POD need to access service & which are in different names space we have to use the FQDN of the service.
***We cant use curl' to est 


**we cant est communication btw containers in diff name space using the service name  BUT we can if use the service name if the containers are in the same ns


     CANT USE THE SERVICE TO COMM BTW CONTAINERS IN DIFF NS 
             1) errror... the 'web pod' in the default namespace TRYING TO use the service name to COMM with the 'hello container in the dev namespace  
root@web:/usr/local/tomcat#  curl hellosvc
curl: (6) could not resolve host: hellosvc    ,,,  wheni use just the service name am unable bc they ar in diff namespaces, so u hv to use the FQDN when d 
containers ar running in  diff namespaces.
            

                       USING SERVICE NAME TO COMM WITH CONTAINERS INSIDE THE SAME NS
       2)successful:  the 'web pod' in the dev namespace TRYING TO use the service name to COMM with the 'hello container in the dev namespace  
container
root@web:/usr/local/tomcat#  exit              54:10  ####remember earlier we config our default namespace to be dev n now we didnt indicate default so its 
in dev lik hello
ubuntu@master:~$  kubectl exec -it web bash          **this is the 2nd web container we created in the dev namespace n using just the service name am able to comm
root@web:/usr/local/tomcat# curl hellosvc        ****now am accessing a container that are in the same namespace .. in the same namespace wit just d service name it wil work n 
successfully communicated                                                                 also with the FQDN it wil also work ..


 DISADVANTAGES OF POD   ... 55.50
********We deleted the 'web container' but it wasnt recreated bc when u use pod to deploy k8 objects, pods cannot be recreated and it cannot be scaled
e.g we cannot decide to have more replicas of a container eg 'hello' if we ar going to be using pod
in k8, the pod lifecycle is very short/has a defined life cycle so if we ar going to deploy appl, we shud nt use pod    .....
(we shud use controllers like Replica Sets, Deployment, Deamon sets to keep pod alive)
We should not create pods directly to deploy applications.
it has a defned life cycle becuase for eg; If a node  goes down in which pods are running, Pods will not be rescheduled.
We have to create pods using controllers which manages the POD life cycle.
controllerManagers:
  ReplicationControllers , ReplicaSets, 
  Deployments,  DaemonSets  



STATIC PODS***************

    Static Pods are controlled by the kubelet service  
If we delete a static pod, So long as the manifest file still exists, it will continue to recreate the pod, the kublete service wil restart the pod
To permanently delete a static pod you must have to delete the manifest file used to create the static pod



                                              WORKLOADS
A workload is an application running on Kubernetes consisting of a single 
component or several components that work together inside a set of pods. 
In Kubernetes, a Pod represents a set of running containers on your cluster.


                                   k8 POD LIFE CYCLE
Kubernetes pods have a defined lifecycle. 
For example, once a pod is running in your 
cluster and the node hosting the pod fails then pods running on the node
will fail. Kubernetes treats that level of failure as final. 
You would need to create a new Pod to recover,even if the node later becomes healthy.
****Therfore we nid to use controller mangers such that if this  node goes down a controler manger will ensure that this pod is rescheduled on another node 


USING MANIFEST FILE TO CREATE A REPLICATION controller, that wil be recreating a particular pod
                        includes manifest/template for ReplicaControllers, template for Pod, & template for service (Nodeport)



                **mee**              NODEPORT/ENDPOINTS/KUBEPROXY  (this is service discovery to take note of)
Nodeport service can process external traffic,
WE connect to d nodeport service from outside d cluster by using the nodeIP & the node port, SO frm outside the cluster we ar able to acces d app using nodeport
This is how it works: our nodes have ip address (nodeports) so when endusers type any of the ip address ie trying to access the appl, the nodeport service will
be routing service to either of the pods we have.
the kubeproxy we have in our cluster will immediately identify that this traffic is meant for a particular service n so it wil be routed 
immediately to the service concerned & frm the service, which  now has endpoints ie our pods/containers, so traffic gets to the endpoints 
***i bliv the endpoints are possible bc of the pod/container label which the service selector matches with.
1:39:44 ###################### # ##########READ UP
Kubernetes Objects
NodePort - Exposes the service on each Node's IP at a static port. A ClusterIP service, to which the NodePort service will route, is automatically created. 
You'll connect to the NodePort service, from outside the cluster, by using "<NodelP>:<NodePortâ€º".

2.09.19                                  NODE PORT COMES WITH CLUSTERIP 
***********nternally, i created a nodeport service for python but that nodeport  service also has a cluster IP assigned to it, so if i wnat to access my 
python appl, i can curl, using the clusterip ,to  access the appl internally



 HOW WE DEPLOY APPLICATION IN K8
########### hw we deploy applictaions in k8 , which is a very key aspect when it comes to my understnading of k8, 
 OBJECTS USED FOR DEPLOYMENT
kubernetes resources/objects used to deploy application include/RUN WORK LOads:
   Pod :
or    
 controllerManagers:
      Replication Controller, ReplicaSet, DaemonSet, StatefulSets, Deployment, Volume, Job       


      WHERE APPLICATIONS ARE DEPLOYED IN K8
where are applications deployed in k8?/??......     we shud be able to est the fact that applictaions/containers are running in pods   
so pods can be deployed/managed by using:
                                     
              HOW POD IS MANGED
pods can be deployed/managed by using:
 1. pods as a kubernetes objects  
 2. controllerManagers kubernetes objects , thers a list of k8 objects when it comes to controller managers, we hv seen the first controller manager k8 objects
   *** now we want to talk about replica sets.. 



    REPLICA SET
ReplicaSet = RS :   ...  ****************HOW TO USE REPLICA SET TO deploy our application    ......1:53:50
==========
What is difference b/w replicaset and replication controller?
RS is the next generation of replication controllers, **kub5** with replica set, we can roll out new version of the application but we cant rollback
The only difference as now is the selector support.

          ***types of selectors:
1)matchlabels which is Equality based:
key == value(Equal Condition)     ****me**use of labels
2) matchexpression which is set based:
  key in [ value1, value2, value3 ]   


      DIFF BTW RC & RS
RC
 Supports ONLY equality based selectors.   *mee** we saw the use of labels..
selector:
   matchLabels:   -# Equality Based    ***use of labels
    key: value
    app: javawebapp
    tier: fe    
    client: tesla
WHILE
RS -->  
Supports eqaulity based selectors and/or set based selectors. (it can be either one) ,(set based is that we can hv keys in multiple volumes)
 matchExpressions: -# Set Based  ,   under selectors: it can either be  matchlabels which is Equality conditions OR matchexpression which is set based where we can have
   - key: app              ********* we can hv key, operator, values , so 'app' alone can match to javawebapp,myapp and fe  ... .....1:56:50
     operator: in
     values:
     - javawebpp
     - myapp  
     - fe  


KUB 8&9                       WE have 2 classes of selectors
   ***Equality base & Selector based selectors
Replica set, daemon set and  deployment. has what we call both equality base.& Set base selectors .. 16.50
BUT Replica controller has only Equality based selector


    CREATE SECRET
Create a secret that will authenticate kubernetes to pull images from dockerHub/nexus/jfrog  



KUB5 .. Class 34


with deployment we can run kubectl undo deployment but we cant undo replicaset
to undo replica set, it will have to be destroyed before it can be recreated and we will see hw deployment resolves theta issue and that is why we use 
deployment as an object over replica set

Deploy an app which must hv a pod running in each node = we use: daemonSet
e.g of diff app that shud hv a pod in each node    e.g logmgt/ logshipper
deploy an appl with scaling option = we  use : rc/rs/deployment/statefulsets
***********we cant use pod for  deployment with scaling  capabilities.... although the appl runs inside the pod, we shud nt use pod directly to deploy appl.
we shud nt use pod as an object for deployment



DAEMON SET
daemonset is a controller manger that permit a pod to be scheduled on each node
we deployed/provisioned a cluster:
   nodes [ node1/node2/node3 ]  
 we can see the app pod we deployed above was scheduled on node1 and node9
#################so the scheduler here is scheduling based on resources


                USE CASES OF DAEMON SET
Explain the kubernetes objects recommended to ensure the scheduler schedule a pod  
in each node or a group of selected node/nodeGroup
- DaemonSets is the ONLY recommended object for pods to be scheduled in each node  
Use cases: 
   - logmgt applications - EFK/ELK    
   - databaseBackup application  

9 worker nodes: = 
    dbnode1, dbnode2, dbnode3, appnode1, appnode2, appnode3
    webnode1, webnode2, webnode13

nodeGroups : in this case, if you have a grp of dbnodes, appnodes, webnodes,, i can schedule pods to be created on only dbnodes grp by passing a node selector, to  say ok,
select only data base  node or a node affirmity for the specific/selected node i want pods to be created only on.
    3 dbnodes  [dbnode1, dbnode2, dbnode3  ]
    3 appnodes [appnode1, appnode2, appnode3]
    3 webnodes [webnode1, webnode2, webnode13]


TAINTING NODE
we can also taint a node when we want to do :
  -- recommissioning / upgrades / updates / patching  
kubectl taint nodes node1 key1=value1:NoSchedule     [taint the node]   .................  to taint a node
kubectl taint nodes node5 key2=value2:NoExecute      [taint the node]
kubectl taint nodes node1 key1=value1:NoSchedule-    [untaint the node] ..................  to untaint a node

Master node is tainted by default
ubuntu@master:~$ kubectl describe node master
under Annotations
taints :we see master node is tainted
Unscheduled: false        .. ********* false means with toration we can scedule pods on the node

                   APPLYING TAINT
ubuntu@master:~$ kubectl taint nodes node1 key1=value1:NoSchedule ...... am  tainting this node wit no schedule which means pod cannot be scheduled on the node  ...1:35:55

       ADD TOLORATION TO TAINTED NODE
for  a pod to be scheduled on the tainted node we nid to add tolerations in the spec:
- operator: Exists                       we v operators like exists, effect 
        effect: "NoSchedule"




KUB 6  . . class34
Deployments  
==========
Deployment is a kubernetes object. 
k8 objects are used to run workloads in k8
Deployment is the recommended kubernetes object for deploying applications, running workloads and managing/contolling pods in our environment.   
Deployment as an object ,deployment strategy: ReCreate strategy , RollingUpdate.  Deployment techniques , 
The default strategy is ROLLING UPDATE & it has no down time bc b4 it brings down a pod eg an old version, it makes sure the one it rolled up has started.

Advantages:
     Deploy/rollout a RS.
     Updates pods (PodTemplateSpec).
     Rollback to older Deployment versions.
     Scale Deployment up or down.
     Pause and resume the Deployment.
     Use the status of the Deployment to determine state of replicas.
     Clean up older RS that you donâ€™t need anymore.


     ****i think this is rolling update
e.g we create a deployment called webapp and say that replica is 2
with deployment as an object :2 rplicas of the appl wil b deployed &2pods for d webapp  &when an update is done ie verson2, the update is deployed wit a new
set of 2replicas and new pods but the previous replicas will be zero (0) bc the pods wil be 0 bt the replicas stil exists and the webapp for each version 
still exists, this is the same process for each version that is rolled out and so we can easily delelete or roll back to a previous set bc their replica sets
were recorded

 1.19.00     DISADVANTAGE OF ROLLING UPDATE
*with roling update we can decide to push out 10 replicas of the new version and keep 10 replicas of the old version to that xtent ders a kind of
trafic mgt and trafic is flowing to both version1 and version2 bt we cant deter who is goin to get version1 and who gets version2 like we ar able to det in
deployment technique anoda problem of rollin updtae is that even after completely deploying version2 if ders a problem, you still v to rollback

1.14.00
the issue with rollingupdate deployment, if you release an important appl like a medicapp for an hospital then you realise ders an error and need to rollback
but before you can rollback lives are gone so we dnt want to deploy using rolling update, we want to deploy using blue green..

*********testing in UAT doesnt mean that ther is no possibilty that der could still be a problem n we wud need to roll back.....1:14:30



USE IDES TO GENERATE FILES

#### i can use IDES to generate a file and just copy, then jst use what i nid v... integrated development env
*****strategy can be rolling update or recreate... 
rolling update is the default startegy  so if u dnt enter a strategy, it wil be uisng rolling update
also if w dnt define replica only 1 replica will be created


Deployment strategies:
====================== ..........................  ******************       25:33
ReCreate strategy  --- 
  It comes with downtime because the current application version is destroy entirely
  before creating the new version  

  eg; 8 pods of version1 running and you want to 
   deploy 8 pods of version2 , recreate will first destroy all the 8pods of the current version
therfore ther will be downtime


                         UPDATE DEPLOYMENT WITH SET IMAGE
Update Deployments  (mayb by rolling out a new version) ,,,,, .......................52:38
Introduction
    We can update deployments using two options
        Set Image   ...OR
        Edit Deployment(manifest file)



  1.08.30    DEPLOYMENT TECHNIQUES
we have blue green techniques & carnary technique

                                                                    2:21:21
DEPLOYMENT TECHNIQUE  ************************************(this is under using deployment not deploymnet as an object) ............1:08:44
Blue Green deployment Technique :    
   version1/blue running in production
   version2/green  Just 2 replicas deployed in testing env    ... 1.15.30  
   we deploy version2 in the test environment and observe its performance, 
   Once the performance is good then we deploy version2/green in production    
the problem with blue green deployment is that it makes use of lot of ressources bc we need to hv 2sets of deployment running.. each set is consuming 
resources

1.14.00
the issue with rollingupdate deployment, if you release an important appl like a medicapp for an hospital then you realise ders an error and need to rollback
but before you can rollback lives are gone so we dnt want to deploy using rolling update, we want to deploy using blue green...
so using blue green deploying we have version1 running in production , then we deploy version2 , so  version2 is green and its deployed in the testing/UAT 
env, which cud be in a diff name space(in k8 we use namespace to isolate clusters) and while its deployed (ders anoda service use To expose it), further test
is being done ,we are observing hw its performing, we can decide to test it on a client with less severe cases or some rabbit & by the time d testing is 
saticfactory, we need it to be running in production and in k8 we expose it using a service.the application already runing in prod is being accessed using 
d service its a nodePortSVC MEDSVC (selector app:medic) wich is routin traffic .the deployment runing in UAT has label=app:med
NOW for the service MEDSVC to  start routing traffic to the appl in UAT and stop routing traffic to the appl in deployment, i will need to change my selector
in the service from medic to med and immediately the appl in UAT is now being exposed in production and traffic is no longer routed to the previous appl  and they can be brought down...
thats how blue green works by switching the service over once we are sactisfied with testing. .... this  is not just a quick switch , this is an ultra fast 
switch.. so oncee we are satisfied, we then bring the old version down.
 1:19:46

                              CANARY  DEPLOYMENT TECHNIQUE
   Canary deployment Technique :  this gives you a longer time to test ur appl ... (class33 video7a....1:12:35)
   - TRAFFIC MGT   
   - 25% traffic VERSION2 goes to Canada  , and if we are sactisfied with how its running we can then switch all the traffic to to version2
   - 75% traffic VERSION1 goes to USA  
   - VERSION1 -- 40% to version1    ...e.g we can decide that we hv version1 runing we can decide that since we ar rolling out vesion2 let 40% of our traffic go to version1
   - VERSION2 -- 60% to version2              and  40% go to version2 and we observe for an extended period and if its sactisfactory we move all the traffic to version2
   - 40years+   75% traffic VERSION1    OR  we could use the cx age group (using canary setting u can set all that up,, der are some appl that ask for your date of birth etc
   - 18-39years 25% traffic VERSION2
   -we can alsouse based on biometrics e.g sex or ages



KUB 7a

we can use a vscode to help when it comes to the manifest file 
but we also hv a github repository that conatins almost all the manifest file tha you may need
prof is there any file you have written that i can go there and copy and use them even at work? ...YES
CLONE THIS REPOSITORY
https://github.com/LandmakTechnologies/Kubernetes-manifests    .... almost everytin that we v done so far u wil find it here, when ur trying to deploy appli
                                                    most of the tins that we v been able to cover in k8, u can see then here, like hw to deploy in mysql db 
                                                       with configmap and secret , all of that, al d manifest files that we wil stil be loking at u wil find 
                                                                      them there when it comes to k8

CREATING POD & MAKING USE OF REQUEST, RESOURCE & LIMIT
one tin i wanted us to look at .... creating a pod and making use of request, RESOURCE,and limit
when YOu want to create a resourec in k8 generally, we need to decide hw much resources is goin to be asssigned
when you assign resources to your pod for that pod to be scheduled on a node, the node must have the requested resources available.

**bt while d container is running, if d container need to optimize d resource d  maximum d conatiner can use is what has been defined as limit

after we deploy the file below 
kubectl describe , if u look at this pod, the pod has a imit as to hw much resources it can consume in a clutser
this are key aspect when it comes to auto scaling our pod , so if we wnat to deploy our apply, we can deploy it making use of REQ and limit as we v seen here


Resource, requests and limits:
--------------------------
Requests and limits are the mechanisms Kubernetes uses to control resources such as CPU and memory. 
Requests are what the container is guaranteed to get. 
If a container requests a resource, 
Kubernetes will only schedule it on a node that can give it that resource.

                                 WHAT IS LIMIT
Limits, on the other hand, make sure a container never goes above
a certain value. The container is only allowed to go up to the limit, and then it is restricted.

Resource Limit:  .. a container wil never go above a certain value
A limit is the maximum amount of resources that 
Kubernetes will allow the container to use.

Resource request:
---------------
A request is the amount of that resources that the system will guarantee for the container, and Kubernetes will use this value 
to decide on which node to place the pod. 


KUB 7b                             TYPES OF SCALING
 scaling in kubernetes: we have Manual scaling and automated Scaling

1) MANUAL SCALING
How do we scale manually in k8 If you deployin your application for you to scale manually, you can run the command:
kubectl scale deployment/rs/rc/sts
ie we can scale rs , deployment, rc, stateful set, ...... these ar the scalable objs in k8 and these scaling can be done maually
we can also scale automatically, in k8 using another object Callled HPA .
 eg manual scaling:
      kubectl scale deployment/rs/rc/sts/ app --replicas 4   

2) AUTO SCALING : Automatically scaling the replicas, it states the minumum& maximum number of replicas


                      TYPES OF AUTOSCALING
TYPES OF AUTOSCALING:   ... WE ILL LOOK AT the rest of the autoscaling as far as k8 is concerned
we can use one of these to automate scaling..
we need the guage bc we hv traffic coming into our cluster, and we hava a HPA, which we want that pod to automate the traffic for us. so we are saying 
1) Horizontal Pod AutoScaling  :                                                               KUB 8&9, i bliv this is the metrix pod
2)Vertical Pod AutoScaling : 
3)Cluster AutoScaling:

1) HORIZONTAL POD AUTOSCALING - HPA
POD AutoScaling -->  **meee** this will hv its own manifest file, * its selector will select the 'podtemplate label which is deploying the applica cluster**
Kuberenets POD AutoScaling Will make sure u have minimum number pod replicas available at any time & based on the observed CPU/Memory 
utilization on pods, it can scale PODS automatically.
HPA Will Scale up/down pod replicas of Deployment/ReplicaSet/ReplicationController 
based on observerd CPU & Memory utilization base the target specified. 
   automated scaling:  
Horizontal Pod AutoScaling  - HPA 
   
***mee** with HPA, we are able to auto scale the number of pod replicas requested by for by the k8 objects eg when we use deployment strategy 
it is based on the resource consumed by each pods
eg; if my replica is 5, hw much resources is each of d replica suppose to consume? for memoery 128mi and cpu:500m, thi sis hw much resources a 
pod can consume, however if each of the pods are all consuming all the 128mi, then its means that more rsource needs to be allocated, thefore bc the 
vol of resources being used by the pod is much so it will add one pod :+1, it will keep ading and the process wil be automated  .. 12.06




                     ***meee*** STEPS TO DEPLOY THE APPLICATION/POD & the HPA  or KUB8&9 HOW CAN SCALING BE AUTOMATED
or (cluster autoscaler ie an automated application cluster where by the replicas are created automatically by the HPA) 
                                      
1) install a metrix server... kub8&9 By installing a guage API [: metric-server ] in our cluster and using 
 KUB 8&9.. so we hv a matrix server service and its also deployed as a pod. the pod is able to check hw much resource is been used here
***ONLINE**
the Kubernetes Metrics Server is not a Horizontal Pod Autoscaler (HPA). Instead, the Metrics Server is a component that
provides the necessary data for the Horizontal Pod Autoscaler to function.
2) deploy the application cluster:  ***mee*** includes 3 manifest files
A) the manifest file using deployment strategy to deploy the 'application' & object that states/makes the request for the resources needed for the cluster.
B) the second manifest file is to deploy the HPA **to ensure the resources requested by the objects for d application are maintained at all time. 
     I bliv each application will be deployed will its own autoscaler (HPA/VPA/CAS)manifest file
C)we are creating a service for our cluster autoscaler  **mee** ie the cluster we deployed in (A)the first manifest file using deployment strategy
       I bliv we can use the word 'cluster autoscaler' bc we are autoscaling the cluster
3) Generate load
***i think at work, we wnt nid to generate workload, bt rather wait for the application/cluster to gradually use up the resources which
                              will then trigger the HPA to auto scale.
   ***I Bliv we had to generate a workload here because we need to confirm that the HPA is working, so thats why we used the 'customized image' bc its 
customizedso taht when we run the workload command, it generate the workload... i think if we use any other image & run the workload command, the HPA will
not function bc the image doesnt hv the content to generate the workload needed for the HPA to work ie autoscale 



KUB 8&9

IN DOCKER 6 OR 7
WE Saw that with endpoints, we can use CMD to add instruction in a docker file to determine what we want to display evarytime it restarts 
                                   

                                    EXAMPLES OF SERVICE DISCOVERY

1)CLUSTER IP  2)  noodeport  service  3)loadbalancer  service  4)External name  
5)headless service

these Are the different services in communities.



                                           DERVICE DISCOVERY & ENDPOINTS
So with these, we can use any of these  to discover our application.
Therefore, we saw again, this diagram, where, with the help of a service, am I able to access, you know,access some  pods that are deployed in  my cluster.
Look at this diagram,  Look at this service. I have a cluster I-P service here okay now Let's.
Assume this my web service for my web application is able to route traffic and load balance traffic to all of this pods, to the group of pods so this 
service acts as a single DNS resolver.(domain name service) .. 8:00
What that means is that, now, if I want to access this pod, I want to call, okay, like, this pod will have, like, maybe Web port one.
Did you see that? Web port one?Now observe something This web pod one.,This pod two.Okay. Now if you want to access all of this pod, web, pod three.,web pod4
Now do I need to be calling Web for one Web, web port2,No, I-I don't have to do that so I can use one name to call all of this pod,
so which Name would i will use, the name of the service.So, because my servicename is, is constant.So DNS domain name is resolver.So  it, it is going to achieve 
domain name resolution.Okay, to all of the set of pods that are linked to this service now.So how do we call this pods in  k8 that are linked to a service?
     #####We call them What END POINTS? Okay, so this service has How many end point? How many end points? How many end points are there now? They are four
and point.
Now, for my scale, if need be can I scale?  if there is need to scale, am I able to scale to have maybe more pod, Can the number of end point increase?The 
answer is yes.The number of end point, okay, can increase.and once it increases
The same service is going to be achieving DNS resolution. The same service because that's what services are all about It's good to be achieving DNS resolution,.

If need be? Can we have more end point?, yes, if need be.
We can scale to how many replicas , right now 6
Okay. So one service, one service is routing traffic, to how many pods of the application six pods at the moment .  thatâ€™s one service.
This is what is going to be happening. So this is the feature of k8, as we have seen, a cluster, IP noteport, load balancer, external name, headless service.
And we are going to look at the ingress  as well as part of the options we can use for service discovery.  11:36


          API VERSION
API version for pod  is V1.
What is the API version for replication? Controller? V1.
What is the API version for replica? Sets.. apps/V1.
What is it API version for daemon set, app/V1,   for deployments, app/V1.


    HOW CAN SCALING BE AUTOMATED?
     WE need a type of guauge in this case ; metric server
By installing a guage API [: metric-server ] in our cluster and using 
*we can use one of these to automate scaling..
we need the guage bc we hv traffic coming into our cluster, and we hava a HPA, which we want that pod to automate the traffic for us. so we are saying 
that :HPA
min2/max50    ....ie replicas....  this is what we ar defing for our HPA , we ar also sayin that our target resource can be a deployment,RS/RC/STS
target resource 
deployment RS/RC/STS      ,, it could be any of these   .... now what is my target ultilization?
target ultilization  ... now when theHPA react? .. the HPA wil react when CPU ultilization or memUltilistaion is greater than 70% ,& thats when it wil scale
cpu Utilisation>70
memUltilisation >70


  HOW HPA WORKS
so our HPA is targeting one of d resource for e.g we use deployment & d deployment name is myapp,the deployment is suppose to control a set of pods &the 
deployment(myapp) is using a selector (match labels, if it is equality based) to know the set of pods that its suppose to control.,  to know the pods we ar
managing.. we wnt to automate it,, e.g we hv node1 and node9  and based on our replica:2  we hv mayapp1 on node1 and myapp2 on node9 so in our cluster we  
hv deployed another object in the cluster,which is called a metrix server whic is runin as a pod ,,so we hv a matrix server service and its also deployed as
a pod. the pod  is able to check hw much resource is been used here.. e.g it can check n tell us we ar using 10% memory, etc and when it gets TO  70%, ie if
ders a spike & we hv to process abt e.g 40m request, my app1 & myapp2 are receiving 20m reQ each due to Load been balanced automatically in k8, bt the 20m 
REQ is making use of up to abt 70% usage, our CPU usage exceeds 70% avg for all our pods,
our HPA will be triggered and one more pod will be automatically created,, this is an automatic addition and once the pod is created, more
traffic will be assigned to the pod.

34.18              

                             CLUSTER  AUTOSCALER 
********if we are now processing 60m REQ, ie our nodes are at their max carryin capacity, but inside our cluster we hv deployed/install CAS 
n in the CAS we ar sayin that athe max number of nodes is min2/max40 .CAS is scaling the number of nodes. now when ar we goin to scale the node or when shud
the node be scaled??? for us to knw the proportion of our CPU that is been used, k8 wil use matrix server to check the vol of cpu for my node
 CAS/min2/max40 
target ultilization  ... 
cpu Utilisation>70
memUltilisation >70
so when our HPA wants the scheduler To scheedule more nodes bt all our nodes has insufficient resources so CAS wil be triggerred and it will add more node
to the cluster,& once the node is added new pods can be scheduled. this process is automatic
now the CAS is runin in our cluster n its runing as a pod and we hv the server which wil be checking hw much resources is our node consuming, 
our pod consuming, thats how it goes
... 37:25


                  HOW WIL VERTICAL pod autoscaler work.//..... 
Let assume we want to deploy another application & we want to use the VPA to manage it
VPA resources:  under resources, we have request, how much resource can a pod requet
request:
cpu=500m  miliquo
mem=128Mi   miliquo
limits:cpu 900m
memory:512Mi
 based on the VPA, when a pod is created, this is the resource (the resources listed above) that is going to be assigned initially/the first time. resources
assigned to the pod wil be based on what is reQuested.assumin that the pod is caryin out some load,load is being assigned to the pod, at 500m it can process
a max of abt 20m requests bt if the number of rEQ is increasing,its goin to scale vertically ie increasing/growing upwards in size. the max cpu can increase
to is 900m , so again, more req can be processed to a max point of when cpu grows to 900m & it processin abt 40m REQ therfore, the same pod that was using 
cpu of 500m & processing 20m REQ has grown vertically to 900m and processing 40m REQ..  ... 43:00



    WE ARE DEPLOYING OUR legacy application
https://github.com/LandmakTechnology/spring-boot-docker         *** this application has a github repository, the application is called spring boot docker
-# Spring App & Mongod DB as POD without volumes         *** we are deploying a springboot application & a database pod
we will be deploying: a Stateless & a stateful application
1)Stateless applications deployment doesn't require to maintain it state   :   ***i bliv this is the main application itself
  Use deployment as the choice kubernetes object  
  ReplicaSets/ReplicationController:  
2)Stateful applications maintained their state. Examples are  :
  databases, Jenkins, where we create jenkins job , we hv to maintain its state
but generally databases are stateful applications
     use StatefulSets as the choice kubernetes object for Stateful application deployments   


                        WE WANT TO DEPLOY THIS MONGO DB  ***mee** with DATABASE & SERVICE
Why is required for this deployment, if you look at the database configuration & the appliaction,
if we decide to deploy this application some objects ar goin to be affected  ..... 49:01 
we wnt to deploy our stateless appli called springboot mongo n we also  deploy our stateful application wich is a mongo db, 
so we ar goin to create 2files:
      *mee** one file for the application & one for the database & we wil deploy each of the files seperately with their own service manifest file
1)Stateless  
--- springapp.yml/            ***meee** this is the main application
2)Stateful 
--- mongo.yml/                   ** this is the database


1. image: mongo         ******this is maintained by the mongo community, in docker hub, we can check for the image inf to get the env variable  1:16:30, just type mongo
2. image: mylandmarktech/spring-boot-mongo               ********this is maintained by us

repository to clone:
1. https://github.com/LandmakTechnology/kubernetes-notes
2. https://github.com/LandmakTechnology/kops-k8s
3. https://github.com/LandmakTechnology/kubernetes-manifests

######this is a springboot application that requires a database, it has has been developed by our developers usin the springboot framewrk
this was not hardcoded, our developers left the hostname, username,passwd as a variable, meaning that if i am deployin in the dev, uat, prod env, i can use
diff values based on my project requirement....   .. 53:15


*********the default deployment strategy in k8 is rolling update
now since am using the default strategy,i wnt put a value for strategy bc the default wil be used ie rolling update



                STEPS TO DEPLOY THE SPRING APP , THE MONGO DB & THEIR SERVICE

so we ar goin to create 2files:
      *mee** one file for the application & one for the database & we wil deploy each of the files seperately with their own service manifest file
1) We create & deploy the springapp manifest file & a nodeport service for the application
    ***we need a nodport service so that: 
 we can hv external users and their traffic can get into the cluster either thru any of d worker nodes or the master node
   bc we v kubeproxy running ,the trafic doesnt leave d node n talk to d pods directly, it goes tru the service n once it recieves d msg it does dns 
         resolution, it routes the traffic n it lb to all d end points .. as seen below:

    *** I didnt put the spring application manifest file here 

                     NODEPORT SERVICE MANIFEST FILE FOR THE APPLICATION
apiVersion: v1
kind: Service
metadata:
  name: springappsvc                               1:22:30
spec:                         for us to be abble to acces athe  springapp pod, 
  type: NodePort      ,, we created  a service , nodeport service:nodeportsvc and the service name is called springappsvc n the service has a label; app:springapp
  selector:             since its a nodeportservice, we can hv external users and their traffic can get into the cluster either thru any of d worker nodes or the master node
    app: springapp       bc we v kubeproxy running ,the trafic doesnt leave d node n talk to d pods directly, it goes tru the service n once it recieves d msg it does dns 
  ports:                          resolution, it routes the traffic n it lb to all d end points.. now dis appl nids to write in the db so we ar deployin a db pod n 4 d 
  - port: 80                          comm to b est they wil communicate tru a service, dis  is internal comm so we nid clusterip service, so we create a mongosvc;app:mongo
    targetPort: 8080               so for springapp to talk to d db it wil be resolved via the service name of the db  ... 1:26:40



                                GET END POINTS
ubuntu@master:~$  kubectl get ep
NAME                    ENDPOINTS                      AGE 
springappsvc             10.36.0.1:8080,10.44.0.3:8080   67s

   ***************if i wnt ot communicate wit the pod, i can use the name of the sevrice: springappsvc or the service ip: 10.100.100.234 
such that if i curl, 
ubuntu@master:~$ curl 10.100.100.234       ..... i successfully get a response 
so the service is achieving dns resolution  ,,....... 1:07:50

we can access the appl externally since we ar using node port , 
ubuntu@master:~$ kubectl get svc 
NAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORTS    AGE
springappsvc   NodePort    10.100.100.234     none        80:32136/TCP 

*****all we nid to use is one of our node even the master node in this case 
ubuntu@master:~$ curl ifconfig.me
3.36.7561ubuntu@master:~$
ubuntu@master:~$
ubuntu@master:~$ curl 3.16.75.61:32136    ... or  in google 3.16.75.61:32136 , w ecan access the application
  ###we can see this appl is a springbootapp thet needs the mondodb  databse and we can automate it using jenkins,docker & k8 , and we are using k8 now, 
we already used docker to containerize
***** right now our database has nt been deployed n so ders no database that the appl is able to comm with.


2) We create & deploy the mongo db manifest file & a clusterip service for the application
   now dis appl nids to write in the db so we ar deployin a db pod & for the communication to be established they wil communicate tru
a service, dis  is internal comm so we nid clusterip service, so we create a mongosvc;app:mongo
      so for springapp to talk to the mongo db it wil be resolved via the service name of the db  ... 1:26:40


*********8*************now we deploy our database,      *** I didnt put the mongo database manifest file here 


   ---                           CLUSTER MANIFSET FILE FOR THE MONGO DB
kind: Service   
apiVersion:  v1  
metadata:  
   name: mongosvc 
spec:                 the default service type in k8 is clusterip so even if i dnt write it, the clusterip type will be created. type:clusterip
  selector:
    app: mongo    
  ports:
  - port: 27017 
    targetPort: 27017  


to deploy the db      .........  1:10:10
ubuntu@master:~$ vi spring/mongo.yml n paste the  db manifest file
ubuntu@master:~$ kubectl apply -f spring/
replicaset.apps/mongors created
service/mongosvc created            ****** we see the service created 
deployment.apps/springapp unchanged
service/springappsvc unchanged
ubuntu@master:~$
ubuntu@master:~$ kubetcl get all 
we can see mongosvc is running  ...... n when we check in google we can see its running, so we are able to deploy our stateful appl n stateless appli
so this process can be automated
if we run kubectl delete pod mongors-966hc                 ,......1:33:40
we can see a new pod is being created automatically.. n this is permitted bc it has a controller manger which is our mongors but if u run: kubectl delete rs mongors, db wnt
although it was automTICALLY CREATED, we cant get back data that was previously created                       be recreated bc D replication controller MANAGer managing the.
for us to be able to capture data we hv to look at volume concept in k8                                      db pod has been deleted  ... 1:42:50
                                                                ***  bt goin further in d video he kefpt deletin the rs each time he had to deploy a manifest 
                                                                                                   file  like 


WE ARE GOING TO BE LOOKING AT VOLUME CONCEPT IN KUBERNETES

IMP REPOSITORIES TO CLONE
repository to clone:
1. https://github.com/LandmakTechnology/kubernetes-notes
2. https://github.com/LandmakTechnology/kops-k8s
3. https://github.com/LandmakTechnology/kubernetes-manifests


              CREATING HOSTPATH VOLUMES .................
 hostPath volume will store data ONLY on the node where the POD is scheduled ..
ie inf entered while d db pod was on d node cant be transfered to anoda node if d db is recreated on anoda node
  This can result data lost and data inconsistency      
EG db was created on node1 but node1 was later tainted & so db needs to be recreated on node3, if this happens, data on node1 cant be tarnsferred to node3
i bliv be the mount point was created on node1.

****IF U ARE TRYING to deploy a stateless (i bliv he meant stateful) appl we hv to mk use of k8 volumes  ...  1:3540
since we are deployin a database we nid to ensure that a piece of storage is created..
we ar goin to see hw to use vol for example:   ..
and the only deployment that will be affected is our mongors , **mee** ie the database ... so we wil deploy it agin this time around With volume

**  under spec, we add volumes
      volumes:        *********vol is a list so we can v more than one vol e.g we can v, name: vol2 ,hostpath:  and path can jst be /data n also multiple mount point
      - name: mongodbhostvol    
        hostPath:
          path: /mongodata        *****if the db pod was created on node9, a mountpoint will be created on node9 and it wil be called /mongodata
      containers:


                   SPRINGMONGO APPLICATION , MONGODB, VOLUMES & NFS
DEPLOYING OUR LEGacy AAPLICATION : Springmongo and mongodb and volumes , NFS ... ............  persistent vol and claim policy
without nfs :*if the db pod(ie the database container) was created on node9, a mountpoint will be created on node9 and it wil be called /mongodata
in the file in dockerhub we see data is stored in /data/db therfore in our db container, data will be stored /data/db ,ie the mountpath in our manifest file
if we introduce any of these, then we are goin to create an nfs server &ther wil be a mount poinT on d node& nfs server then data is captured in both the db
container and the nfs server such that evEN if the container is recreated on another node, once the container syncronizes with the nfs server all the data 
that was captured in the Nfs server wil reflect in the container and this wil result in data consistency


   DEPLOYING OUR VOL Uing NFS
######## ******************    2:01:20  ..   netwoRK FILE SYSTEM OR EFS

NFS    
if we introduce any of these, then we are goin to create an nfs server & ther wil be a mount poinT on d node& nfs server then data is captured in both the db
container and the nfs such that evEN if the container is recreated on another node, once the container syncronizes with the nfs server all the data that was
captured in the Nfs server wil reflect in the container and this wil result in data consistency

  volumes:
      - name: mongodbhostvol    
        hostPath:
          path: /mongodata 
      volumes:
      - name: mongodbhostvol    
        nfs:
          path: /mongodata 

when it comes to VOLUME CONCEPT, ther are several vol options we can look at like:
awsElasticBlockStore, azureDisk, azureFile, configMap, emptyDir g, cePersistentDisk, gitRepo (deprecated) 
hostPath, nfs, persistentVolumeClaim, secret


                         Configuration of NFS Server ( WE CAN USE ANSIBLE TO AUTOMATE THE PROCESS)
  NFS = Network file system 
        distributed file system 
        shared file system 

A) Step1 TO 3
Step 1: Install NFS Kernel Server
Before installing the NFS Kernel server, 
  we need to update our systemâ€™s repository index with that of the Internet 
  through the following apt command as sudo:
B) 

          HOW TO USE NFS SEERVER FOR OUR DEPLOYMENT
-- the only thing that wil change in this manifest file compared to the hostpath file is the nfs server address and remove hostpath cos d vol type we are 
using is nfs, then everytin remains the same

with this nfs deploymnet we dnt have issues with data loss / inconsistency .............2:33

############WE CAN USE EFS instead of NFS, but we used NFS without any issues here




           Persistent volumes             ***we wil use retain claim policy most of the time
===================
this are simply a piece of storage in your cluster
 the ideal appraoch to store data in k8 is by using persiastent vol by it is managd by the kube service
just like docker vol is manged by docker and thats why we use persistent vol in docker as well


*** Based on someones question: the persistence vol could either be host path, NFS and other vol options 
with persistent vol, the vol itself is managed by the kube service
the nfs is kind of storage option that we can use to store data like we saw hostpath wher data wil only be hosted by one particular node in the clusted bt
in the nfs data is distributed to all the nodes and taht leads to data constitency,, however if u create a nfs and it is nt created under a persistent vol, 
bc it is the recommended approach to store data in k8 bc when u create a persistent vol it is manged by the kube service , therefore the vol is independent 
of the livecycle of the pod that is consuming the vol so if the pod goes down we can stil retieve the data


deploy a kubernetes using Kops  


PV --> It's a piece of storage (hostPath, nfs,ebs,azurefile,azuredisk) in k8s cluster. 
PV exists independently from from pod life cycle form which it is consuming.

PersistentVolumeClaim -->
   It's request for storage(Volume).Using PVC we can request(Specify) 
   how much storage u need and with what access mode u need.

Persistent Volumes are provisioned in two ways, Statically or Dynamically.:  ... 2:35:40  ... power point script

1) Static Volumes (Manual Provisionging)
    A k8's Administrator can create a PV manually so that pv's can be available for PODS which requires.
    Create a PVC so that PVC will be attached PV. We can use PVC with PODS to get an access to PV. 

2) Dynamic Volumes (Dynamic Provisioning)
     It's possible to have k8s provision(Create) volumes(PV) as required. 
     Provided we have configured A storageClass [sc].
     So when we create PVC if PV is not available Storage Class will Create PV dynamically.        

PVC: If pod requires access to storage(PV),it will get an access using PVC.
     PVC will be attached to PV.

PersistentVolume â€“ the low level representation of a storage volume.
PersistentVolumeClaim â€“ the binding between a Pod and PersistentVolume.
Pod â€“ a running container that will consume a PersistentVolume.
StorageClass â€“ allows for dynamic provisioning of PersistentVolumes.

PV Will have Access Modes:
============================
ReadWriteOnce=RWO â€“ the volume can be mounted as read-write by a single node = EBS 
ReadOnlyMany=ROM  â€“ the volume can be mounted read-only by many nodes
ReadWriteMany=RWM â€“ the volume can be mounted as read-write by many nodes = NFS 


Claim Policies
================
A Persistent Volume Claim can have several different claim policies associated with it including
RetainÂ â€“Â WhenÂ the claim(PVC) is deleted, the volume(PV) will exists.
RecycleÂ â€“Â When the claim is deleted the volume remains but in a state where the data can be manually recovered.
DeleteÂ â€“Â The persistent volume is deleted when the claim is deleted.

The claim policy (associated at the PV and not the PVC) is responsible for what happens to the data when the claim is deleted.

2:37:13
**********************************CREATING THE DATABASE POD USING PERSISTENT VOL 
inside my cluster, i wil create persistent vol, now my db nids this vol n the vol can only be assigned using persistent vol claim ,,, PVC, with the PVC we 
are able to mount we ar able to create a mount point btw the container n the persistent vol, the PVC permits the container to claim the persistent vol
*************with this system ders no data loss if db container goes down bc persistent vol is anoda k8 object whic is independent of my pod lifecycle
    ***we wil use retain claim policy most of the time


 KUBERNETES VOLUME CONCEPT FOR PVC
In k8, we can create volumes with or without PVC    ***for it to be persistent, its adviced to use PVC
1)   HOSTPATH
 a) Without PVC; the hostpath created is like a mount point just like in 'DOCKER bind mount', we hv to create a mount point & its not persistent
            STEPS TO CREATING HOSTPATH VOL WITHOUT PVC MANIFEST FILE 
 Creating hostpath vol for database..  the manifest file for mongo db with hostpath, will have: under spec
      i)we add volumes & 
     ii)path: /mongodata  *****if the db pod was created on node9, a mountpoint will be created on node9 and it wil be called /mongodata
          
 b)With PVC; the hostpath created is the PVC,  its mg8 by kubernetes so its like 'Docker vol' we created in docker , its persistent bc its mg8 by docker 
               **8 check main file For the 4  manifest file
2)    NFS OR EFS 
     a) Without PVC; the NFS/EFS created is like  external vol 'EBS' we created in Doker, its persistent but i think not mg8 by k8
            STEPS TO CREATING NFS/EFS VOL WITHOUT PVC MANIFEST FILE 
      Under spec,we add NFS 
     the only thing that wil change in this manifest file compared to the hostpath file is the nfs server address and remove hostpath cos d vol type we are 
      using is nfs, then everytin remains the same
       nfs:
          server: 10.0.0.76          ********using the private ip bc they ar in the same vpc ie both my nfs server & k8 nodes
          path: /mnt/share
      b) with PVC; the NFS/EFS created is mg8 by kubernetes so its like 'Docker vol' we created in docker , its persistent bc its mg8 by docker 

                  4 MANIFEST FILES FOR HOSTPATH WITH PVC  **PERSISTENT VOL, this is manual provisioning, we can automate the process with Ansible
   Creating a host path with persistent vol
  a) manifest file to create the persistent vol
  b) manifest file to create the mongo db with PV. .. persistent vol
  c) manifest file to create the persistent claim policy
  d) manifest file to create the mongodb with the  persistent claim policy



