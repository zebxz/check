 

 
***** linux 1............  1:43:00  the remote host is the ip address of our server

please i cant remember that for now, can we come back to that later

I Didnt print
Sand 
Gateway 
Eks


****meee*********
we use maven for build and tomcat for deployment having integrated it with nginx webserver for loadbalancing & routing traffic from endusers to the appservers 
we bring in sonar for security, then nexus aas a repository for backing up our artifacts and jenkins to automate the end to end process (CI/CD)
Most importantantly or in addition to that; because security is inheerent in our environment we utilize AWS for various levels of security like ........
furtheer more we use docker & kubernetes for .....
and finally Ansible and terrfaorm for ..........


jenkin 7&8
someone is aking we hv nt deployed to tomcat ,, we are going to be deploying containerized app, tomcat was an introduction
  1:12:18 all of these is nt happening in one agent , mvn package will tk place in maven build server, sonar,the code quality analysis will talk to a 3rd party server wich is sonarqube server

2:04:00  we can use jenkins for non java applications and any application as expalined in jenkins shared lib video


LINUX , scripting, git , maven, tomat/ngnix tomcat, sonar, nexus, jenkins , aws, docker, k8, terraform, ansible, k8/helm 
In our env, we use LINUX OS for file,process,package,security mgt, GIT FOR VERSIONING, MAVEN FOR BUILD, JENKINS automates the end to end process, 
AWS for cloud computing services/resources, DOCKER for containerization ie deploying light weight containers, DOCKER SWAMP for container orchestration, KUBERNETES for
container orchestration/MGT, Orchestrate means to arrange.




*************************





 **** onLINE*** 
AI Overview
Kubernetes clusters can be categorized based on various characteristics, including their deployment environment, architecture, and networking configuration.
1. Based on Deployment Environment:
On-Premises Clusters: These clusters are deployed and managed within an organization's own data centers, offering full control over infrastructure but
requiring significant operational overhead.
Cloud-Based Clusters: These are hosted and managed by cloud providers (e.g., Google Kubernetes Engine (GKE), Amazon Elastic Kubernetes Service (EKS), Azure 
Kubernetes Service (AKS)). They offer scalability, managed services, and reduced operational burden.
Hybrid Clusters: These combine elements of both on-premises and cloud-based deployments, allowing workloads to run across different environments.
2. Based on Architecture and Availability:
Single-Zone Clusters: All nodes are located within a single geographical zone, making them vulnerable to zonal outages.
Multi-Zonal Clusters: Nodes are distributed across multiple zones within a single region, improving availability and resilience against zonal failures.
Multi-Regional Clusters: Nodes are spread across different geographical regions, providing the highest level of availability and disaster recovery 
capabilities.
Multi-Cluster Architectures: Involve managing and orchestrating multiple independent Kubernetes clusters, often for reasons like isolation, compliance, or
scaling across diverse environments. These can be:
Cluster-centric: Focus on managing individual clusters as distinct units.
Application-centric: Focus on deploying and managing applications across multiple clusters.
3. Based on Networking Configuration:
IPv4-only Clusters: Configured to assign only IPv4 addresses to pods and services.
IPv6-only Clusters: Configured to assign only IPv6 addresses to pods and services.
Dual-Stack (IPv4/IPv6) Clusters: Configured to assign both IPv4 and IPv6 addresses, enabling communication using either protocol.
4. Based on Local Development/Testing:
Local Clusters: Tools like Kind (Kubernetes in Docker), Minikube, or K3s allow developers to run lightweight Kubernetes clusters on their local machines for
development and testing purposes.
These categories are not mutually exclusive, and a Kubernetes cluster can exhibit characteristics from multiple types simultaneously. For example, a 
multi-zonal cluster could be deployed in a cloud environment with a dual-stack networking configuration.


**********************************************************************************************************************************************



KUB 3&4
DISADVANTAGES OF DOCKER    ****bc of the disadvantages of docker, we therefore use tools like docker swamp, kubernetes & open sheet.     

 we can use software like docker to deploy applications ,to deploy n mange containers bt is it recommanded, shud we use 
docker ??? we shud nt and u are going to see that shortly, we shud rather use container managers or ochestration tools like docker swarm kub n open sheet.
when we studied dodcker, we deployed appl using docker bt what are the limitations using docker to deploy applications, we ar going to look at that shortly
7: 43 bt these ar the contarization software  ... check

with docker i cannot v more than 1 replica of webapp for e.g u are deploying ur appl n ders a spike , no i cant v multiple replicas in docker , e.g: so if u decide to run
docker run -name webapp -replicas 5 , ie trying to get the number of replicas u wnat, it wont work.. docker doesnt support it
also, if my engine goes down , my contaiiners will stop running 
also docker doesnt support a scenario wher i can v multiple serveers in a cluster  e.g docker engine1 (with container), docker engine2(with containers) such that if one 
engine is down the 2nd engine can support it ,, docker doesnt v such sys bc the docker ntwrk dose nt support MULTI HOSTING where we can v a cluster of servers, 
docker netwrking sys doesnt support multi hosting and thats a big problem ,, the docker system doesnt v an overlay ntwrk wher u can v a cluster of server,
no overlsy ntwrk support and d worst is that when appl ar deplloyed, to expose them to endusers is very limited and we wil see hw it is diff with kub...
bc in docker we do port forwarding bt we wil see what kub brings to the table  .. i bliv port forwarding is how we expose the application (on your internal 
netwrok) to the internet


    HOW Did WE DEPLOY A WORK LOAD IN DOCKER ??
we had to use docker resources or objects to deploy applications like docker file we use it to build images n we can now deploy our appl wit the help of 
ntwrks, vols, using the docker-compose file.
........docker resources/objects use to deploy applications:
  Dockerfiles/images/networks/volumes/docker-compose.yml/etc. but the docker compose file doesnt have the 'deployment strategy' so we didnt hv the option of 
replicas


DOCKER 1&2
 APPROACH USED TO DEPLOY WORKLOAD IN DOCKER 

WE DEPLOY workloads using Imperative and declarive approaches    

1-#Containerize application Imperative approach =  Command
   LIKE:
docker build -t teslaimage:1 .
docker run -d -p 80:8080 --name webapp teslaimage:1

2-declarive approach= DOC7.. makes use of deployment script *ie(vi to create eg spring.sh & put the image,db,netwrk command) & compose files/less commands 
Docker Compose is a tool for defining/declaring and running multiple containerised micro-services applications.
-# Using compose file     *** but this compose file doesnt have the 'deployment strategy' like docker swamp & kubernetes
  **while d docker compose file for docker includes netwrk, d compose file for docker swamp does nt include netwrk, because an overlay netwrk has been initialized


Docker 10
2.58.56****** WHEN U RUN DOCKER SWAM init, 
By default it creates an overlay ntwrk, the overlay ntwrk is what permits your docker server to communicate in a cluster like this bc in normal docker der 
is no overlay ntwrk for that reason, you cannot v docker servers in a cluster like this bc node1 cannot talk to node2 in normal docker
so once u v the overlay ntwrk, when u create ur containers , when you create ur services, those services can talk to each other irrespective of whether they
ar created in node1 or node10 bc of the overlay ntwrk n the DNS registration is done using the service names

            OVERLAY  NETWORK
whenever we ar deploying in a swarm, our containers wil be created in the overlay netwrk..overlay ntwrk means that u can communicate wit dif nodes, the master
can communicate with multiple nodes using the overlay netwrk, with overlay the container in node1 can talk with the conatinters in all the other nodes.


DOCKER 10
docker swarm has 2modes: REPLICA MODE & GLOBAL MODE
1)Replicas ---> it will deploy based on replicated number    (we v used this mode to deploy above)
***** this is the default mode & with this mode, it wil deploy base on the number of replicas that you want     ... 2:11:24
2)Global--- its like daemon set in kubernetes, its good for deploying logmgt container for monitoring
with the global mode for e.g, if u v a cluster that has 14servers or 14 nodes(3master + 11 workers)
if u v a 14 node cluster n u want to deploy an appl using the global mode what wil happen is that a replica wil be created in each of this node 


KUB 1&2
HOW WE DEPLOY APPLICATION IN K8
########### hw we deploy applictaions in k8 , which is a very key aspect when it comes to my understnading of k8, 
 OBJECTS USED FOR DEPLOYMENT
kubernetes resources/objects used to deploy application include/RUN WORK LOads:
 1)  Pod : ***pod is nt recommemded bc we cant scale bc the pod template doesnt hv the option to create replicas like controller managers, so when a pod dies 
or                                                                                             its nt recreated and that is dangerous.
2)controllerManagers:   ***Deploying with controller managers we are able to scale
      Replication Controller; uses only; matchlabels which is Equality based 
        ReplicaSet; uses both matchlabels which is Equality based &  matchexpression which is set based:, can rol out new version but cant roll back
        DaemonSet,  creates a pod in each of the nodes.. good to use to deploy logmgt for monitoring purpose
       StatefulSets, 
       Deployment,   has strategy like 'Recreate' OR 'rolling update' which is the default strategy, can roll out new version & can roll back
       Volume, Job  

 3)                                     TYPES OF SCALING
Manual scaling (Controller managers) OR Auto scaling the 'replica' using either of these objects:(horizontal/vertical/cluster autoscaler)

    



      

*****************************************************************
KUB 3&4*    POD
in k8, the pod lifecycle is very short so if we ar going to deploy appl, we shud nt use pod
We should not create pods directly to deploy applications.  **kub5**     lacks self-healing capacities
If a node  goes down in which pods are running, Pods will not be rescheduled.
We have to create pods using controllers which manages the POD life cycle.
controllerManagers: ReplicationControllers , ReplicaSets, Deployments, DaemonSets


    **mee**  WHAT A MANIFEST FILE CONTAINS/DETAILS FOR EACH CONTROLLER MANAGER
1)**Mee*** therefore for Rc, in the manifest file, kind will be 'rc' & we will also hv a template for pod inside the same manifest file & in the RC section,
we have selectors & spec,for spec; we will indicate the number of replicas but if we dnt,it wil create 1 replica & if we need to create service, we wil also 
have a template/section for service
2)for RS, in the manifest file, kind will be 'rs' & in the replica set section, we have selectors & spec, for spec, we will indicate the number of replicas,
& we also have a template for pod inside the same manifest file & if we need to create service, we wil also have a template/section for service
3) for daemon set, kind will be Daemon set; in the Daemon section, for spec, we dnt indicate no. of replicas bc because daemon schedules pods on all the nodes
so we dnt need RC/RS, because there is no replica template, i bliv thats why we cant scale with daemon set. & we also have the template for pod, & service.
4) Deployment rolls out/deploys a replica set therefore; in the manifest file, in daemon section, we have spec & selectors, for spec; we 
 we will indicate the strategy type eg rolling update or recreate, but if u dnt indicate the strategy rolling update is the default strategy & if u dnt
indicate the replica, i replica will be deployed & we also hv the pod template & service template

                         AUTOSCALING
****The manifest file states the minumum& maximum number of replicas of the application/pod to maintain at all times.


                             
                   ***meee***        STEPS FOR OCHESTRATING CONTAINERS
1)After installing either self managed (multi node preferred), OR  paid to manage
2)create namespace, add label in pod template, so service can make pod discovered/accessible,
3) create service type either clusterip or nodeport (using service template)
4)chose a controller manager, k8 object for deployment prefered 'DEPLOYMENT' &Its rolling update strategy, although he said smt about blue green techniques, bt didnt apply it.
5) Resources request & limit ie cpu/ memory for the pod/application
6) Autoscale the controller manager


                   CONTROLLER MANAGERS eg RC,RS
1.42,00   Cretaed pods using Replication controller, when we delete them, we see they are RECREATED.
all the pods have been rescheduled/recreated bc my pods are been manged by a 
 Replication controller n the RC has a label app:web n the RC has a selector that has selected the label, once it is
 selected, the pod labels hv defined that the replicas is 3, so at all times, the RC must maintain d desired state of my appli which is 3 

***my own explanation, the pod template has a label & the RC section has a defined the number of replicas & the RC also has a selector that will select the 
label of the pod template & as such, the RC will maintain the defined number of replicas for the app/pod at all time.


advantages of namespace
****clusterRole &clusterRolebinding is for the entire cluster 


 ****in manifest files;
pods have label .. ie key value pair
ReplicationControllers has selectors which is used to be able to connect to the pod label


LABELS & SELECTORS EXPLAINED
****SERVice makes pods accessible using labels & selectors

**************************************************************************************************************



KUB 1/2 & 3/4.. is all about types of k8 cluster to we can install, installing d multi node cluster, creating namespace, how applications are deployed ie usng
pod as a object OR controller managers like RC, RS but we shudnt use pods bc of the short life cycle and inability to scale, also we hv the way we deploy 
workload either using the imperative/command approach or declarative/manifest file approach.
we have then the service discovery ; we have clusterip service & nodeport service.. clusterip is for internal comm within the same ns BUt to est comm from a
diff ns we hee FQDN  while nodeport is for externall comm ie been able to
comm into the cluster from outside , Then to use k8 to pull frm docker/private reg need to create secret login for dockerhub.
KUB 3/4,5,6 & 7, We explored the diff k8 objects used for deployment

 



   SUMMARIZED FOR INTERVIEW

KUB 1&2
XPLAIN THE K8 ARCHITECTURE  
kubernetes Orchestrate and managed containerised Applications
k8 is a grp of servers that are working together to be able to ochestrate containerize applications, its made up of a control plane & worker node, the control
planee comes with components line the API server which is the main adminstrative point and the entry point into the cl8 & it does generally authentication &
authoriztion & it ensures that whatevr Req GETS into the cl8 is been made by an authorized & authenticated individual, apart of the API server, we hv the etcd
that persists data in the cl8, it acts as a database ,whatevr reQ is been made by whoever engr that inf is stored and persisted in the etcd and frm there we 
also hv the scheduler, which is incharge of scheduling  eg if pods are been REQuested to be scheduled the sheduler will schedule that considering the nodes
that hv available resources for such a pods to be hosted and if thers any other constrain all the constraint are been factored in
we also hv the controler managers that is able to ensure that at all times we hv a number of nodes running , pods running , deplyments are runing as expected
we also hv the worker nodes comes with the kublets which is the pry node agent, we equally hv the kube proxy, which is incharge of netwrk 
also the container run time, there are diff types of container run time that you can use, docker used to be one of them , also container-D, container runtime
is any software in your node that is able to create and start containers bc k8 is all abt managing containerized appl n in our env we use containerD as our 
container run time, we cud hv as well choosen other run time but we are using containerD

Also the k8 architecture is such that calls are made into the cl8 and that happens using the k8 client like kubectl or k8 UI form there we can mk certain API 
calls either to create containers, namespaces, security like RBAC and for any of these to happen, authentication must happen via the kube config file
so thats the k8 architecture


KUB 3&4
FOR autHORIZATION;
authorisation via RBAC: Roll base access control, 
this is under kub security , u hv to be authenticated n authorized to perform that task
once u are authentictaed n authorized , the task is being executed bt if u werent succesfully authorized, u wil get permission denied/ authentication failed

After sucessful auth/author, 
in docker we deploy our containers directly in containers, in kub they are deployed in what is called pods
so pod is the smallest unit in kub
so u wnat to deploy an appli, dis is what happens, the API server receives this information either frm UI or kubctl, n immediately it receives it, it will
persist the data in our key value store ie etcd,, (i bliv UI is using imperative/declarative & kubctl is the way we create static pod using the 'etc/' dir)
now u wnat to deploy your appl which are deployed in pods .... in your cluster, we hv the worker nodes that comes with the kublets which is the pry node agent
therfore, the control plane is communicating with the worker node via this pry node agent ,once it commun wit the worker node ,it wil be checking for resouce
availability u wnat to deploy a pod , this is a scheduler, this pods requires abt 800mb bt when it comes to availabilty: my node1 got abt 700mb of memory, 
node9 got abt 8000mb of memory we hv d scheduler, hes going to schedule the pods based on resource availabilty & other factors, all other factors are constant.
At this time the scheduler will do automatic scheduling on node9 which has enough enough resource availability.now when the pod, lemme cal it 
webpod is scheduled on node9, inside the webpod containers nid to be created now the container runtime wil create n start the container, so now i v my 
webcontainer inside the pod.this is the kub architecture, very powerful 
now i v my appl scheduled on the pod, and we nid to deploy multiple replicas , in this case we v 2replicas, the scheduler has scheduled the 2replicas on the 
2diff nodes.therfore 2replicas of the pod have been deployed
inside this cluster we wnat to ensure that we can access this pod, for the pod to be made accessible we ar going to make what is called a service in kuber.
35:34
so u v a service , this service act as a LB
E.G I V A webSVC, what wil happen is that if u wnat to access this appl running in this pod in kub, it is going to be made discoverable using the service
so if u wnat to access the appl, it wil be routed via the service, we call this service discovery, u wnt to discover ur appl, the service is involve
now this entire netwroking wher appls can accessed with relative ease is been suported by a ntwrk agent kube-proxy.

###########if u can xplain dis kub architecture, u ar as good as done 37:26



CLASS 33 vide 5&6.... 52:30 .... i will watch the video later...........
************* for each of the version it goes tru this steps: we create the packages, then run docker build to contianersrize the appl and ship the image with
docker push to the image registry 

this is an extract of the entire CI/CD process  .......7:58 ....(qusetion n answer vidoe 5&6)
b4  we containerize, containerize, b4 we ar creatin images, we had gone tru code quality analysis, and depending on the type of project, der are projects were
we execute sonarqube analysis b4 even creating packages wit mavin,, so if u do that u are good to go, so dis is jst a summary  bc we didnt hv nexus or sonar 
qube connected in this k8 but on our CI/CD pipeline, we hv developers write code, developing new update of the appl, and once they do that they commit to 
github and we can connect github to soanat cloud wer once der is a new commit, sonar cloud wil run code quality analysis, its only when that process passes,
wher our sonarqube or sonar cloud report indicates that it has gone tru all our quality threshold, evrytin is fine , der are no vulnerability in the code, no 
smell, bug n issues , then once we v realised that then we now bring in maven wich then creates packges , once maven is done we upload artifacts into nexus
and frm ther we can now build our image using docker, once image is built we use k8 4 deployment.

INORDER FOR US to update the new versionin on the deployment session on k8, that is done by webhook or pull ACM 
*************SOME ONE SAID ,, with the help of the webhook created in github wich is triggered when thers a new comit, github  then notify jenkins to do a
build ...32:17

*******i will share the jenkins kubernetes integration, it covers evrytin am tryin to xplain to u now, ,, a lot of folks have had jobs by just goin tru that
video, hw all this tools connect to ecah other




**************************************************************************************************************


******if we dnt pass replica, its goin to create one...

*me*Docker is 'compose file' deploy with command 'Docker compose up -d' WHILE kubernetes is 'manifest file' deployed with command 'kubectl apply -f pod.yml'


                           TYPES OF OCHESTRATION TOOLS
Container Orchestration Tools --> :
   Docker Swarm, Kubernetes, OpenShift



START

KUBERNETES
IT is an ochestration engine, an open source platform for managing containerized applications... kubernetes is responsible for container deployment, scaling, descaling of
containers and lB.
actually, kubernetes is nt a replacement for docker bt can be considered as a replacement for docker swamp , kuber is basically more complex than swarm and REQ more work to 
deploy.. born in google written in go and go lang
when we talk abt kub, we classify an all imp tool bc of its imp  of to me as a devops engr, its so important that one of the jobs i can do is a kubernetes administrator 


KUB 1&2 .... 1.43.50
kubernetes uses the kubectl client or the UI to run workloads.




TK NOTE OF THIS TREE
kubernetes:    kub has a cluster , 
   cluster: it is a grp of nodes
   nodes:  in the nodes ther are pods 
   pods:  in the pods ther ar containers ..  
   containers :  

therfore in kub we can say that nodes are the subset of the cluster , pods are subset of the nodes and containers are the subset of the pods
cluster ---> nodes ---> pods  ---> containers  :
********therfore if u are going to deploy any appl in k8, it is going to be deployed in a pod

PODS:
====
POD --> Pod is the smallest building block use to deploy applications in k8s.
Pod represents running processes. Pod can contains one or more containers.
These container will share same network, storage and any other specifications.
Pod will have unique IP Address in k8s cluster. 

Pods
 SingleContainerPods --> Pod will have only one container.       98%
 
 MultiContainerPods(SideCar) --> POD with two or more containers. 2%  
e.g, we can v the  the application container and like a sidecar container
       application Container :
          e.g  webapp    , e.g u v a container running and u wnat to ensure this appl doesnt v any problem, so u attach a logmgt container so that whatsover is 
       SideCar containers:                                                                             happening in the appl, this log is reporting it
            e.g  logMgt  container   ... like a container that is collecting log from ur application,  can als have like a utility container for supplies



 cONTROLLER MANAGER.. >>>>>>>>>
E.G IF U deploy ur appl and you want 3pods to be deployed , lets assume 2pods shud be running, if 1pod is down, the controller manger has been instructed to create 2pods, if
one pod goes down, it will automatically create the pod, those are the function of controller mangers
*** Meee.. it knows what to do at any given time, eg one pod is down, it automatcally brings up another one.



fEATURES:    *** OF K8
1)the first feature it has is that it supports what is called automatic scheduling ie it provides advance scheduler to launch contianers on node
2) it has self hidden capabilities, rescheduling, replacinging and rescheduling containers which are dead and the bauty abt it is that if u deploy a wrong application u can 
go back to what was running before so it has automATED ROLBACKS AND rollouts 
so kuben supports rollouts and rollbacks for the desired state of containerized application therfore, if u had a version running and u v a deploy a new version n ders a 
problem, u can rollback 
3) it has horizomtal scaling and LB , kub can be scaled up and down d appl as per REQuirment
4) service discovery & LB
5)WITH kuber DERS no nid to worry about netwrkn n communicatn bc kub wil automatically assign ip address to containers n a single dns name for a set of containers that can LB
TRAFFIC inside the cluser .... 5:00
containers get their ips so u can put a set of containers behind a single L Balancer ... this is a very key aspect service discovery n LB , hw powerful is that 
6)it also supports storage ochestration, u can mount the storage system of your choice , u can either opt for local storage or choose a public cloud 


          KUBERNETES CLUSTER & ITS COMPONENT

kubernetes architecture:  why are we doing kub and why is kub imp in my devops journey , that in all thy knowings u must knw kub
                                ############  kubernetes comes with a cluster in the cluster we v :
controller noder or controlPlane/MasterNodes: this has some components to tk note of:
  apiServer             : this is the main entry point into the cluster, it performs admin task in the cluster
  etcd                  : the key value store ,,, this is like a database
  scheduler  : 
  controllerManagers   : 
workerNodes           :  this is wher appl are running and here we can v node1, to node9  . the components of the nodes includes:
  kubelet             : this is the main agent that runs on the worker node. its the pry node agent
  pod                 : is the smallest unit in kub, in kub contaners are deployed in what is called pods
  container runtime   : and d container runtime we ar going to be seeing is = [Container-d] 
 kube-proxy           : the netwrk agent
21:30
############### in kub if u wnt to perform any task e.g u want to deploy an appl , for that deployment to happen ther are a few things that kub uses, we call it d client
kub client (how u can access ur cluster)  for e.g we hv docker client , the docker cli
22:11
kubernetes-client      : we also hv kub cli , command line interface ,, command line interface here is kubectl
  kubectl              : if u are authorized, u can run 
     kubectl create/delete/get/describe/apply/run/expose 
  so we v the cli which we can use to make some api calls and we also v other APIs 
  ui- DASHBOARD  
  api  ,, others APIs
Or it has a UI wich is called  a dashboard ,, since we started this programe wher v we see any dashboard , to deploy in jenkins we were able to go to the browser,  der was 
a nice dashboard wher we cud create and run jobs
Now in kub, to perform any task in ur cluster,  for docker we cud do stuff like docker build, docer run n our docker client was docker bc all the commands started wit docker
our kub cli is kubectl ... this is our kub server, u want to run a job in kub , u run commands like kubectl run to deploy an appl, what happens is that kubctl is like a msg
that is sent into ur cluster , dis msg comes to ur cluster, u wnt to deploy an appli, so it cud either b frm ur dashboard, or  ur cli. then regrdless of wher the msg is 
coming from, d API server receives the msg on behalf of our cluster,d API sevrer performs admin task n when u execute dis command, this will get you tru authenticaton,so  u
nid to be authenticated n authorization,, 2things wil happen now u v executed kubctl run, wich means u wnt to deploy an appl, for dis to happen, kub nids to authenticate n
authorize u and the authentication and authorization process wil tk place using a file called .kubeconfig file . this is the file dat authenticates you

kubeconfig [.kube/config ] file will authenticate the admin    ....(ur tryn to mk an APi call, tryn to dd smt to b done) so u must be authenticated) 39:34
                               the caller admin/Developer/Engineer 




 AUTHENTICATION & AUTHORIZATION

FOR autHORIZATION;
authorisation via RBAC: Roll base access control, 
this is under kub security , u hv to be authenticated n authorized to perform that task
once u are authentictaed n authorized , the task is being executed bt if u werent succesfully authorized, u wil get permission denied/ authentication failed

WHO CAN DO WHAT, who can perform what task .. ,, THIS WILL BE SUPPOERTED BY KUB SECURITY  .. 40: 26
kubernetes security - RBAC:
  Developers [ Paul, Joyce, Chidi ] 
  Engineers  [ James, Dominion, Janet ]    .... we can assign them diff functions

authentication via kubeconfig : 
authorisation via RBAC:

  AUTHORIZATION IN AWS
         to be authorized to create an aws EKS cluster in aws, i can create an IAM Role for EKS , this wil permit us to create and manage an EKS cluster
it wil be the same thing for Azure AKS, goodle GKE and Ibm cloud

************* so if i v like 10clusters that i wnt to manage, wich i wnat to be observing hw dey are performing at all times then i need to instal the rancher 
sfware so i can use it to access those clusters



                                           2TYPES OF KUBERNETES CLUSTER YOU CAN INSTALL
1) Single node cluster 
2) Multi Nodes Kubernetes Clusters   .. This is our focus....


                             WAYS TO MANAGE KUBERNETES CLUSTER
you can deploy your cluster using, either a manged or a self managed sfware
ideally, companies will prefer to go for managed bc it will give them more time to focus on the app , bc if u nid to focus on the architecture, it wil mean
that u wil need to spend more time ensuring that ur cluster is running than managing containerized appl,,  bt more time shud be dedicated to appl management.


the first of its kind which we can install is : 
1. Self Managed Kubernetes [k8s] Cluster = IaaS--EC2  :  thess ar infrastructure as a service , we can launch of k8 cluster on a grp of ec2 instances, using kubeadm
    kubeadm --> We can setup multi node k8's cluster using kubeadm.  ... kubeasm is a sfware that is used to launch a k8, both d control plane n worker node
    kubespray --> We can setup multi node k8s cluster using kubespray , it uses ansible playbook
     (Ansible Playbooks Used internally by kubespray).
WITH  Self Managed Kubernetes [k8s] Clusters both the    
     controlPlane: [apiServer, etcd, scheduler, Controller Managers] 
      and 
     workerNodes: [  kubelet, containerRuntime-Container-d, kube-proxy]  
  are managed by the Admin/Kubernetes/DevOps Engineers

*************** so if u have a self managed cluster u hv to ensure that the control plane wit all its components re healthy n functioning and also the components of the 
woker nodes are all running and healthy, it is ur function

we v anoda class of clusters wich are  manged by 3rd party, u will nt bother abt the mgt
2. Managed k8s Cluster  (Cloud Services) = PaaS  : 
   The controlPlane is managed by a cloud provider or third party.  
   The controlPlane and all it components are managed by the Cloud provider 
   ***************  howerever,   The workerNodes are managed by the Admins or engineers

        
         TYPES OF CLOUD manged service:    ***I bliv for ochestrating contaners
   EKS --> Elastic Kubernetes Service(AWS)
   AKS --> Azure Kubernetes Service(Azure)
   GKE --> Google Kubernetes Engine(GCP)
   IKE --> IBM K8s Engine(IBM Cloud)
    Kubernetes Cluster = k8s  

########### so u can deploy your cluster using, either a manged or a self managed sfware
ideally, companies will prefer to go for managed bc it will give them more time to focus on the app , bc if u nid to focus on the architecture, it wil mean
that u wil need to spend more time ensuring that ur cluster is running than managing containerized appl,,  bt more time shud be dedicated to appl management.

*******************we also v another clas of cluster that can be installed ... KOPS

    
KUBERNETES DISTRIBUTION TOOL

****ONLINE
Kubernetes distributions and tools: Several open-source tools and distributions assist in deploying and managing self-managed Kubernetes clusters, offering 
varying degrees of automation and opinionated configurations. Examples include:
Kubeadm: A tool for bootstrapping a minimum viable Kubernetes cluster.
Kops (Kubernetes Operations): A tool for building, operating, and maintaining production-grade Kubernetes clusters in the cloud.
Rancher: A complete software stack for managing multiple Kubernetes clusters.


3. KOPS: is a software use to create production GRADE/ready k8s in AWS and  
         azure for the kops beta version  
         It creates a highly available kubernetes services (he said cluster, mayb a mistake) in Cloud like AWS.
            KOPS will leverage Cloud Sevices like: .. so when u deploy wit kops, it comes along with all dis services n its very esay to craate a kops cluster 
              vpc, 
              AutoScaling Groups, 
              LoadBalancer, 
              Launch Template/configuration
              ec2-instances nodes [workerNodes and masterNodes]



                             A TOOL WE CAN USE TO MANAGE MULTIPLE CLUSTER 
Rancher: - Using Rancher we can deploy both managed and self managed k8s CLUSTER
           Rancher serves as a glass to access and manage multiple k8s  
           from the rancher dashboard [UI]  - rancher dashboard  ,,,, frm rancher i can create a cluster in EKS/AKS/GKE/IKE 
           authentication and authorisation: EKS/AKS/GKE/IKE  ... bt u v to be authorized




DEPLOY/INSTALL MULTI CLUSTER

DEploy master node:
in mobaxterm connect to the server then set hostname to master  and sudo i to switch to root user
in the installation script first, we v to 
we nid to diasable swap n kernel setting,so we wil disable swap memory so that we can enhnace performance in our k8 cluster , then nxt, we ar adding kernel
details n instal container-d, first instal container-d n its dependecies bc our runtime (to b able to start&runour conatiners) is nt docker bt it is container
-d, once it is installed, we will proceed to start container-d  afterwhcih we wil instal kublet, kubeabm and kubectl,, after that we wil start the kublet
service i will run this as a script and i nid to be root user
Initialize Kubernetes control plane by running the below commond as root user.
sudo kubeadm init
****************** but am a root user, so i dnt nid sudo. ... just run kubeadm init 
and for us to be able to initialize this control plane the req port must be opened. 
************************** SUCCESSFUL... now i can process as a regular user

to launch nodes/Wokers:   ***************

8)  Generate the master join token on the master node
so am going to generate the master token on the master node
kubeadm token create --print-join-command                                                                               1:14:44
once the token is created,



K8 BEST PRACTICES FOR LARGE CLUSTER

lets look at k8 BEST PRACTICES FOR LARGE CLUSTERS     *********** so if u have a very large cluster you can click on this link, u wil get more info
https://kubernetes.io/docs/setup/best-practices/cluster-large/
For very large clusters NB: dnt create :
   No more than 5,000 nodes
   No more than 110 pods per node
   No more than 150,000 total pods
   No more than 300,000 total containers


WE WANT TO DEPLOY WORK LOAD IN k8
***ticket002
========
************************************************************    Deploy workloads in kubernetes;
kubernetes resources/objects used to deploy application includes:
kubernetes Orchestrate and managed containerised Applications  
This applications run as containers  
These containers are housed in pods  
Pods are housed in nodes    
nodes are housed in the cluster   
********therfore if u are going to deploy any appl in k8, it is going to be deployed in a pod

now we v created our k8 cluster ,how are we able to deploy appl using this cluster ,, ????????????? dats some of the things we wil be able to look at:


    OBJECTS USED FOR DEPLOYMENT
kubernetes resources/objects used to deploy application include/RUN WORK LOads:
   Pod :
or    
 controllerManagers:
      Replication Controller
      ReplicaSet
      DaemonSet
      StatefulSets
      Deployment
      Volume
      Job       


when u deploy appl in k8, u v to make the appl accessible ie exposing appl/accessing appl
Exposing/accessing applications = Service Discovery:
    Service Types:  it will route service to our appl n their replicas ,
    ClusterIP :  it performs LB  , its used for internal communication inside the cluster.
    NodePort
    LoadBalancer
    ExternalName  
  ingress 
  networkPolicy 




                                 NAMESAPCE     ***i bliv its kinda similar to label & restriction in docker swamp
1:34:30  **********************generally for this deployment to tk place even in k8, we will make use of a namespace
how DO WE ENSURE THE APPL ARE ABLE TO COMMUNICATION  & all of that , ders a concept in k8 called:
Namespace:
  It is a virtual cluster inside your cluster  ,,, we can create a name space for dev stage, uat stage , prod stage , sales ... dependn on d project u are managing
     [ dev / uat / prod ],                            if u create a namespace for e.g, u wnat a situatn wher what dev ar doin does nt affect what is runin in uat &prod
     [sales, accounts, cs, payroll]                so we can use name spaces for isolation, we'l isolate d dev env frm uat, frm prod using namespaces





 APPROACH USED TO DEPLOY WORKLOAD IN K8 ????????????????

WE DEPLOY workloads using Imperative and declarive approaches    

-#Create Name Space Using Imperative approach =  Command
   LIKE:
kubectl create namespace <nameSpaceName>
    kubectl create namespace dev  

-declarive approach =  makes use of files and less commands  
-# Using Declarative Manifest file 

 Use the declarive approach to deploy workloads in kubernetes:
  Manifest files = kams       ........ we use manifest files to deploy in ddeclarative approach
  Manifest files are written in yaml/yml language 

pod.yml  : in this yml language, it deals with stuffs like :
key:value  pairs     
dictionary: number of key:value pairs 
list:

                        KEY VALUE PAIR
hw do we consider keyvalue pair 
**************for e.g when u want to create a manifest file, it has this acronym, comes :   where 'comes' stands for kind, api version, metadata and spec
kind is a key value pair bc for example ; kind is pod , this is a key value pair, it has a key & a value  ,,, so key &value ,, kind:pod
for e.g , key value pair could be  name and the value is simon
key:value  pairs 
name: simon   
 

K8 BEST PRACTICES FOR LARGE CLUSTER

lets look at k8 BEST PRACTICES FOR LARGE CLUSTERS     *********** so if u have a very large cluster you can click on this link, u wil get more info
https://kubernetes.io/docs/setup/best-practices/cluster-large/
For very large clusters NB: dnt create :
   No more than 5,000 nodes
   No more than 110 pods per node
   No more than 150,000 total pods
   No more than 300,000 total containers




 APPLICATIONS IN OUR REGISTRY   ... ..... these are all images we can use to depploy our appli in k8
in our image registry , we have applications like 
Docker images = dockerHub other registries:
-- python-web-app ,  nodeweb-app,  net-webapp 
   mylandmarktech/hello,  nginx,  mysql,  mongo  
   jenkins,  sonarqube, nexus            



KUB 3&4
 SET CONFIGURED NAMESPACE TO CURRENT NAMESPACE
originally, the default namespace is the 'current namspace'
*Mee*by default, kubectl get po' lists the pods in the configured ns, BUT you hv to run the 'kubectl config set-context' command, to set the ns you configured/
created to be the current ns


  LABELS & SELECTORS EXPLAINED
****SERVice makes pods accessible using labels & selectors

********************* for a service to deiscover any pod, its going to be using labels, so what is the label of the pod
so we v labelas n we also v selectors...  so ders a label that says app is mapped to hello, its under the pod, so we v Pod label, so 
under service, we ar goin to v sevice selectors
in my k8 cluster der wil b many sevices,  hw wil the service be able to identify this pod,, it nids to create a selector that matches this pod like that 

***online
No, a manifest file's label does not have to match the container name, but they are often related and must match for certain tools to function correctly. 
For example, in Kubernetes, a manifest's metadata.
labels are used by other resources like a Deployment's selector to identify which pods to manage, not to name the container itself. 
 


SERVICE DISCOVERY
ServiceDicovery:
==============
*********the first service type is:
ClusterIP is the default kubernetes service type that support
communication within the cluster.    *** Used for internal communication
************** to communicate within our cluster, we will use a clusterip service type



What is FQDN?               ..........          45:21   in kubernetes by default we create containers in the same namespace to enable them comm wit each oda, however, 
FQDN = Fully Qualified Domain name.                                                    
If one POD need to access service & which are in different names space we have to use the FQDN of the service.
***We cant use curl' to est 


**we cant est communication btw containers in diff name space using the service name  BUT we can if use the service name if the containers are in the same ns


     CANT USE THE SERVICE TO COMM BTW CONTAINERS IN DIFF NS 
             1) errror... the 'web pod' in the default namespace TRYING TO use the service name to COMM with the 'hello container in the dev namespace  
root@web:/usr/local/tomcat#  curl hellosvc
curl: (6) could not resolve host: hellosvc    ,,,  wheni use just the service name am unable bc they ar in diff namespaces, so u hv to use the FQDN when d 
containers ar running in  diff namespaces.
            

                       USING SERVICE NAME TO COMM WITH CONTAINERS INSIDE THE SAME NS
       2)successful:  the 'web pod' in the dev namespace TRYING TO use the service name to COMM with the 'hello container in the dev namespace  
container
root@web:/usr/local/tomcat#  exit              54:10  ####remember earlier we config our default namespace to be dev n now we didnt indicate default so its 
in dev lik hello
ubuntu@master:~$  kubectl exec -it web bash          **this is the 2nd web container we created in the dev namespace n using just the service name am able to comm
root@web:/usr/local/tomcat# curl hellosvc        ****now am accessing a container that are in the same namespace .. in the same namespace wit just d service name it wil work n 
successfully communicated                                                                 also with the FQDN it wil also work ..


 DISADVANTAGES OF POD   ... 55.50
********We deleted the 'web container' but it wasnt recreated bc when u use pod to deploy k8 objects, pods cannot be recreated and it cannot be scaled
e.g we cannot decide to have more replicas of a container eg 'hello' if we ar going to be using pod
in k8, the pod lifecycle is very short/has a defined life cycle so if we ar going to deploy appl, we shud nt use pod    .....
(we shud use controllers like Replica Sets, Deployment, Deamon sets to keep pod alive)
We should not create pods directly to deploy applications.
it has a defned life cycle becuase for eg; If a node  goes down in which pods are running, Pods will not be rescheduled.
We have to create pods using controllers which manages the POD life cycle.
controllerManagers:
  ReplicationControllers , ReplicaSets, 
  Deployments,  DaemonSets  



STATIC PODS***************

    Static Pods are controlled by the kubelet service  
If we delete a static pod, So long as the manifest file still exists, it will continue to recreate the pod, the kublete service wil restart the pod
To permanently delete a static pod you must have to delete the manifest file used to create the static pod



                                              WORKLOADS
A workload is an application running on Kubernetes consisting of a single 
component or several components that work together inside a set of pods. 
In Kubernetes, a Pod represents a set of running containers on your cluster.


                                   k8 POD LIFE CYCLE
Kubernetes pods have a defined lifecycle. 
For example, once a pod is running in your 
cluster and the node hosting the pod fails then pods running on the node
will fail. Kubernetes treats that level of failure as final. 
You would need to create a new Pod to recover,even if the node later becomes healthy.
****Therfore we nid to use controller mangers such that if this  node goes down a controler manger will ensure that this pod is rescheduled on another node 


USING MANIFEST FILE TO CREATE A REPLICATION controller, that wil be recreating a particular pod
                        includes manifest/template for ReplicaControllers, template for Pod, & template for service (Nodeport)



                **mee**              NODEPORT/ENDPOINTS/KUBEPROXY  (this is service discovery to take note of)
Nodeport service can process external traffic,
WE connect to d nodeport service from outside d cluster by using the nodeIP & the node port, SO frm outside the cluster we ar able to acces d app using nodeport
This is how it works: our nodes have ip address (nodeports) so when endusers type any of the ip address ie trying to access the appl, the nodeport service will
be routing service to either of the pods we have.
the kubeproxy we have in our cluster will immediately identify that this traffic is meant for a particular service n so it wil be routed 
immediately to the service concerned & frm the service, which  now has endpoints ie our pods/containers, so traffic gets to the endpoints 
***i bliv the endpoints are possible bc of the pod/container label which the service selector matches with.
1:39:44 ###################### # ##########READ UP
Kubernetes Objects
NodePort - Exposes the service on each Node's IP at a static port. A ClusterIP service, to which the NodePort service will route, is automatically created. 
You'll connect to the NodePort service, from outside the cluster, by using "<NodelP>:<NodePort›".

2.09.19                                  NODE PORT COMES WITH CLUSTERIP 
***********nternally, i created a nodeport service for python but that nodeport  service also has a cluster IP assigned to it, so if i wnat to access my 
python appl, i can curl, using the clusterip ,to  access the appl internally



 HOW WE DEPLOY APPLICATION IN K8
########### hw we deploy applictaions in k8 , which is a very key aspect when it comes to my understnading of k8, 
 OBJECTS USED FOR DEPLOYMENT
kubernetes resources/objects used to deploy application include/RUN WORK LOads:
   Pod :
or    
 controllerManagers:
      Replication Controller, ReplicaSet, DaemonSet, StatefulSets, Deployment, Volume, Job       


      WHERE APPLICATIONS ARE DEPLOYED IN K8
where are applications deployed in k8?/??......     we shud be able to est the fact that applictaions/containers are running in pods   
so pods can be deployed/managed by using:
                                     
              HOW POD IS MANGED
pods can be deployed/managed by using:
 1. pods as a kubernetes objects  
 2. controllerManagers kubernetes objects , thers a list of k8 objects when it comes to controller managers, we hv seen the first controller manager k8 objects
   *** now we want to talk about replica sets.. 



    REPLICA SET
ReplicaSet = RS :   ...  ****************HOW TO USE REPLICA SET TO deploy our application    ......1:53:50
==========
What is difference b/w replicaset and replication controller?
RS is the next generation of replication controllers, **kub5** with replica set, we can roll out new version of the application but we cant rollback
The only difference as now is the selector support.

          ***types of selectors:
1)matchlabels which is Equality based:
key == value(Equal Condition)     ****me**use of labels
2) matchexpression which is set based:
  key in [ value1, value2, value3 ]   


      DIFF BTW RC & RS
RC
 Supports ONLY equality based selectors.   *mee** we saw the use of labels..
selector:
   matchLabels:   -# Equality Based    ***use of labels
    key: value
    app: javawebapp
    tier: fe    
    client: tesla
WHILE
RS -->  
Supports eqaulity based selectors and/or set based selectors. (it can be either one) ,(set based is that we can hv keys in multiple volumes)
 matchExpressions: -# Set Based  ,   under selectors: it can either be  matchlabels which is Equality conditions OR matchexpression which is set based where we can have
   - key: app              ********* we can hv key, operator, values , so 'app' alone can match to javawebapp,myapp and fe  ... .....1:56:50
     operator: in
     values:
     - javawebpp
     - myapp  
     - fe  


    CREATE SECRET
Create a secret that will authenticate kubernetes to pull images from dockerHub/nexus/jfrog  



KUB5 .. Class 34


with deployment we can run kubectl undo deployment but we cant undo replicaset
to undo replica set, it will have to be destroyed before it can be recreated and we will see hw deployment resolves theta issue and that is why we use 
deployment as an object over replica set

Deploy an app which must hv a pod running in each node = we use: daemonSet
e.g of diff app that shud hv a pod in each node    e.g logmgt/ logshipper
deploy an appl with scaling option = we  use : rc/rs/deployment/statefulsets
***********we cant use pod for  deployment with scaling  capabilities.... although the appl runs inside the pod, we shud nt use pod directly to deploy appl.
we shud nt use pod as an object for deployment



DAEMON SET
daemonset is a controller manger that permit a pod to be scheduled on each node
we deployed/provisioned a cluster:
   nodes [ node1/node2/node3 ]  
 we can see the app pod we deployed above was scheduled on node1 and node9
#################so the scheduler here is scheduling based on resources


                USE CASES OF DAEMON SET
Explain the kubernetes objects recommended to ensure the scheduler schedule a pod  
in each node or a group of selected node/nodeGroup
- DaemonSets is the ONLY recommended object for pods to be scheduled in each node  
Use cases: 
   - logmgt applications - EFK/ELK    
   - databaseBackup application  

9 worker nodes: = 
    dbnode1, dbnode2, dbnode3, appnode1, appnode2, appnode3
    webnode1, webnode2, webnode13

nodeGroups : in this case, if you have a grp of dbnodes, appnodes, webnodes,, i can schedule pods to be created on only dbnodes grp by passing a node selector, to  say ok,
select only data base  node or a node affirmity for the specific/selected node i want pods to be created only on.
    3 dbnodes  [dbnode1, dbnode2, dbnode3  ]
    3 appnodes [appnode1, appnode2, appnode3]
    3 webnodes [webnode1, webnode2, webnode13]


TAINTING NODE
we can also taint a node when we want to do :
  -- recommissioning / upgrades / updates / patching  
kubectl taint nodes node1 key1=value1:NoSchedule     [taint the node]   .................  to taint a node
kubectl taint nodes node5 key2=value2:NoExecute      [taint the node]
kubectl taint nodes node1 key1=value1:NoSchedule-    [untaint the node] ..................  to untaint a node

Master node is tainted by default
ubuntu@master:~$ kubectl describe node master
under Annotations
taints :we see master node is tainted
Unscheduled: false        .. ********* false means with toration we can scedule pods on the node

                   APPLYING TAINT
ubuntu@master:~$ kubectl taint nodes node1 key1=value1:NoSchedule ...... am  tainting this node wit no schedule which means pod cannot be scheduled on the node  ...1:35:55

       ADD TOLORATION TO TAINTED NODE
for  a pod to be scheduled on the tainted node we nid to add tolerations in the spec:
- operator: Exists                       we v operators like exists, effect 
        effect: "NoSchedule"




KUB 6  . . class34
Deployments  
==========
Deployment is a kubernetes object. 
k8 objects are used to run workloads in k8
Deployment is the recommended kubernetes object for deploying applications, running workloads and managing/contolling pods in our environment.   
Deployment as an object ,deployment strategy: ReCreate strategy , RollingUpdate.  Deployment techniques , 
The default strategy is ROLLING UPDATE & it has no down time bc b4 it brings down a pod eg an old version, it makes sure the one it rolled up has started.

Advantages:
     Deploy/rollout a RS.
     Updates pods (PodTemplateSpec).
     Rollback to older Deployment versions.
     Scale Deployment up or down.
     Pause and resume the Deployment.
     Use the status of the Deployment to determine state of replicas.
     Clean up older RS that you don’t need anymore.


     ****i think this is rolling update
e.g we create a deployment called webapp and say that replica is 2
with deployment as an object :2 rplicas of the appl wil b deployed &2pods for d webapp  &when an update is done ie verson2, the update is deployed wit a new
set of 2replicas and new pods but the previous replicas will be zero (0) bc the pods wil be 0 bt the replicas stil exists and the webapp for each version 
still exists, this is the same process for each version that is rolled out and so we can easily delelete or roll back to a previous set bc their replica sets
were recorded

 1.19.00     DISADVANTAGE OF ROLLING UPDATE
*with roling update we can decide to push out 10 replicas of the new version and keep 10 replicas of the old version to that xtent ders a kind of
trafic mgt and trafic is flowing to both version1 and version2 bt we cant deter who is goin to get version1 and who gets version2 like we ar able to det in
deployment technique anoda problem of rollin updtae is that even after completely deploying version2 if ders a problem, you still v to rollback

1.14.00
the issue with rollingupdate deployment, if you release an important appl like a medicapp for an hospital then you realise ders an error and need to rollback
but before you can rollback lives are gone so we dnt want to deploy using rolling update, we want to deploy using blue green..

*********testing in UAT doesnt mean that ther is no possibilty that der could still be a problem n we wud need to roll back.....1:14:30



USE IDES TO GENERATE FILES

#### i can use IDES to generate a file and just copy, then jst use what i nid v... integrated development env
*****strategy can be rolling update or recreate... 
rolling update is the default startegy  so if u dnt enter a strategy, it wil be uisng rolling update
also if w dnt define replica only 1 replica will be created


Deployment strategies:
====================== ..........................  ******************       25:33
ReCreate strategy  --- 
  It comes with downtime because the current application version is destroy entirely
  before creating the new version  

  eg; 8 pods of version1 running and you want to 
   deploy 8 pods of version2 , recreate will first destroy all the 8pods of the current version
therfore ther will be downtime


                         UPDATE DEPLOYMENT WITH SET IMAGE
Update Deployments  (mayb by rolling out a new version) ,,,,, .......................52:38
Introduction
    We can update deployments using two options
        Set Image   ...OR
        Edit Deployment(manifest file)



  1.08.30    DEPLOYMENT TECHNIQUES
we have blue green techniques & carnary technique

                                                                    2:21:21
DEPLOYMENT TECHNIQUE  ************************************(this is under using deployment not deploymnet as an object) ............1:08:44
Blue Green deployment Technique :    
   version1/blue running in production
   version2/green  Just 2 replicas deployed in testing env    ... 1.15.30  
   we deploy version2 in the test environment and observe its performance, 
   Once the performance is good then we deploy version2/green in production    
the problem with blue green deployment is that it makes use of lot of ressources bc we need to hv 2sets of deployment running.. each set is consuming 
resources

1.14.00
the issue with rollingupdate deployment, if you release an important appl like a medicapp for an hospital then you realise ders an error and need to rollback
but before you can rollback lives are gone so we dnt want to deploy using rolling update, we want to deploy using blue green...
so using blue green deploying we have version1 running in production , then we deploy version2 , so  version2 is green and its deployed in the testing/UAT 
env, which cud be in a diff name space(in k8 we use namespace to isolate clusters) and while its deployed (ders anoda service use To expose it), further test
is being done ,we are observing hw its performing, we can decide to test it on a client with less severe cases or some rabbit & by the time d testing is 
saticfactory, we need it to be running in production and in k8 we expose it using a service.the application already runing in prod is being accessed using 
d service its a nodePortSVC MEDSVC (selector app:medic) wich is routin traffic .the deployment runing in UAT has label=app:med
NOW for the service MEDSVC to  start routing traffic to the appl in UAT and stop routing traffic to the appl in deployment, i will need to change my selector
in the service from medic to med and immediately the appl in UAT is now being exposed in production and traffic is no longer routed to the previous appl  and they can be brought down...
thats how blue green works by switching the service over once we are sactisfied with testing. .... this  is not just a quick switch , this is an ultra fast 
switch.. so oncee we are satisfied, we then bring the old version down.
 1:19:46

                              CANARY  DEPLOYMENT TECHNIQUE
   Canary deployment Technique :  this gives you a longer time to test ur appl ... (class33 video7a....1:12:35)
   - TRAFFIC MGT   
   - 25% traffic VERSION2 goes to Canada  , and if we are sactisfied with how its running we can then switch all the traffic to to version2
   - 75% traffic VERSION1 goes to USA  
   - VERSION1 -- 40% to version1    ...e.g we can decide that we hv version1 runing we can decide that since we ar rolling out vesion2 let 40% of our traffic go to version1
   - VERSION2 -- 60% to version2              and  40% go to version2 and we observe for an extended period and if its sactisfactory we move all the traffic to version2
   - 40years+   75% traffic VERSION1    OR  we could use the cx age group (using canary setting u can set all that up,, der are some appl that ask for your date of birth etc
   - 18-39years 25% traffic VERSION2
   -we can alsouse based on biometrics e.g sex or ages



KUB 7a

we can use a vscode to help when it comes to the manifest file 
but we also hv a github repository that conatins almost all the manifest file tha you may need
prof is there any file you have written that i can go there and copy and use them even at work? ...YES
CLONE THIS REPOSITORY
https://github.com/LandmakTechnologies/Kubernetes-manifests    .... almost everytin that we v done so far u wil find it here, when ur trying to deploy appli
                                                    most of the tins that we v been able to cover in k8, u can see then here, like hw to deploy in mysql db 
                                                       with configmap and secret , all of that, al d manifest files that we wil stil be loking at u wil find 
                                                                      them there when it comes to k8

CREATING POD & MAKING USE OF REQUEST, RESOURCE & LIMIT
one tin i wanted us to look at .... creating a pod and making use of request, RESOURCE,and limit
when YOu want to create a resourec in k8 generally, we need to decide hw much resources is goin to be asssigned
when you assign resources to your pod for that pod to be scheduled on a node, the node must have the requested resources available.

**bt while d container is running, if d container need to optimize d resource d  maximum d conatiner can use is what has been defined as limit

after we deploy the file below 
kubectl describe , if u look at this pod, the pod has a imit as to hw much resources it can consume in a clutser
this are key aspect when it comes to auto scaling our pod , so if we wnat to deploy our apply, we can deploy it making use of REQ and limit as we v seen here


Resource, requests and limits:
--------------------------
Requests and limits are the mechanisms Kubernetes uses to control resources such as CPU and memory. 
Requests are what the container is guaranteed to get. 
If a container requests a resource, 
Kubernetes will only schedule it on a node that can give it that resource.

                                 WHAT IS LIMIT
Limits, on the other hand, make sure a container never goes above
a certain value. The container is only allowed to go up to the limit, and then it is restricted.

Resource Limit:  .. a container wil never go above a certain value
A limit is the maximum amount of resources that 
Kubernetes will allow the container to use.

Resource request:
---------------
A request is the amount of that resources that the system will guarantee for the container, and Kubernetes will use this value 
to decide on which node to place the pod. 


KUB 7b                             TYPES OF SCALING
 scaling in kubernetes: we have Manual scaling and automated Scaling

1) MANUAL SCALING
How do we scale manually in k8 If you deployin your application for you to scale manually, you can run the command:
kubectl scale deployment/rs/rc/sts
ie we can scale rs , deployment, rc, stateful set, ...... these ar the scalable objs in k8 and these scaling can be done maually
we can also scale automatically, in k8 using another object Callled HPA .
 eg manual scaling:
      kubectl scale deployment/rs/rc/sts/ app --replicas 4   

2) AUTO SCALING : Automatically scaling the replicas, it states the minumum& maximum number of replicas

                      TYPES OF AUTOSCALING
TYPES OF AUTOSCALING:   ... WE ILL LOOK AT the rest of the autoscaling as far as k8 is concerned
1) Horizontal Pod AutoScaling  :
2)Vertical Pod AutoScaling : 
3)Cluster AutoScaling:

1) HORIZONTAL POD AUTOSCALING - HPA
POD AutoScaling -->  **meee** this will hv its own manifest file, * its selector will select the 'podtemplate label which is deploying the applica cluster**
Kuberenets POD AutoScaling Will make sure u have minimum number pod replicas available at any time & based on the observed CPU/Memory 
utilization on pods, it can scale PODS automatically.
HPA Will Scale up/down pod replicas of Deployment/ReplicaSet/ReplicationController 
based on observerd CPU & Memory utilization base the target specified. 
   automated scaling:  
Horizontal Pod AutoScaling  - HPA 
   
***mee** with HPA, we are able to auto scale the number of pod replicas requested by for by the k8 objects eg when we use deployment strategy 
it is based on the resource consumed by each pods
eg; if my replica is 5, hw much resources is each of d replica suppose to consume? for memoery 128mi and cpu:500m, thi sis hw much resources a 
pod can consume, however if each of the pods are all consuming all the 128mi, then its means that more rsource needs to be allocated, thefore bc the 
vol of resources being used by the pod is much so it will add one pod :+1, it will keep ading and the process wil be automated  .. 12.06




                     ***meee*** STEPS TO DEPLOY THE APPLICATION/POD & the HPA 
or (cluster autoscaler ie an automated application cluster where by the replicas are created automatically by the HPA) 
                                      
1) install a metrix server
***ONLINE**
the Kubernetes Metrics Server is not a Horizontal Pod Autoscaler (HPA). Instead, the Metrics Server is a component that
provides the necessary data for the Horizontal Pod Autoscaler to function.
2) deploy the application cluster:  ***mee*** includes 3 manifest files
A) the manifest file using deployment strategy to deploy the 'application' & object that states/makes the request for the resources needed for the cluster.
B) the second manifest file is to deploy the HPA **to ensure the resources requested by the objects for d application are maintained at all time. 
C)we are creating a service for our cluster autoscaler  **mee** ie the cluster we deployed in (A)the first manifest file using deployment strategy
       I bliv we can use the word 'cluster autoscaler' bc we are autoscaling the cluster
3) Generate load

                       


                        


 
