 

 
***** linux 1............  1:43:00  the remote host is the ip address of our server

please i cant remember that for now, can we come back to that later

I Didnt print
Sand 
Gateway 
Eks


****meee*********
we use maven for build and tomcat for deployment having integrated it with nginx webserver for loadbalancing & routing traffic from endusers to the appservers 
we bring in sonar for security, then nexus aas a repository for backing up our artifacts and jenkins to automate the end to end process (CI/CD)
Most importantantly or in addition to that; because security is inheerent in our environment we utilize AWS for various levels of security like ........
furtheer more we use docker & kubernetes for .....
and finally Ansible and terrfaorm for ..........


jenkin 7&8
someone is aking we hv nt deployed to tomcat ,, we are going to be deploying containerized app, tomcat was an introduction
  1:12:18 all of these is nt happening in one agent , mvn package will tk place in maven build server, sonar,the code quality analysis will talk to a 3rd party server wich is sonarqube server

2:04:00  we can use jenkins for non java applications and any application as expalined in jenkins shared lib video


LINUX , scripting, git , maven, tomat/ngnix tomcat, sonar, nexus, jenkins , aws, docker, k8, terraform, ansible, k8/helm 
In our env, we use LINUX OS for file,process,package,security mgt, GIT FOR VERSIONING, MAVEN FOR BUILD, JENKINS automates the end to end process, 
AWS for cloud computing services/resources, DOCKER for containerization ie deploying light weight containers, DOCKER SWAMP for container orchestration, KUBERNETES for
container orchestration/MGT, Orchestrate means to arrange.




*************************





 **** onLINE*** 
AI Overview
Kubernetes clusters can be categorized based on various characteristics, including their deployment environment, architecture, and networking configuration.
1. Based on Deployment Environment:
On-Premises Clusters: These clusters are deployed and managed within an organization's own data centers, offering full control over infrastructure but
requiring significant operational overhead.
Cloud-Based Clusters: These are hosted and managed by cloud providers (e.g., Google Kubernetes Engine (GKE), Amazon Elastic Kubernetes Service (EKS), Azure 
Kubernetes Service (AKS)). They offer scalability, managed services, and reduced operational burden.
Hybrid Clusters: These combine elements of both on-premises and cloud-based deployments, allowing workloads to run across different environments.
2. Based on Architecture and Availability:
Single-Zone Clusters: All nodes are located within a single geographical zone, making them vulnerable to zonal outages.
Multi-Zonal Clusters: Nodes are distributed across multiple zones within a single region, improving availability and resilience against zonal failures.
Multi-Regional Clusters: Nodes are spread across different geographical regions, providing the highest level of availability and disaster recovery 
capabilities.
Multi-Cluster Architectures: Involve managing and orchestrating multiple independent Kubernetes clusters, often for reasons like isolation, compliance, or
scaling across diverse environments. These can be:
Cluster-centric: Focus on managing individual clusters as distinct units.
Application-centric: Focus on deploying and managing applications across multiple clusters.
3. Based on Networking Configuration:
IPv4-only Clusters: Configured to assign only IPv4 addresses to pods and services.
IPv6-only Clusters: Configured to assign only IPv6 addresses to pods and services.
Dual-Stack (IPv4/IPv6) Clusters: Configured to assign both IPv4 and IPv6 addresses, enabling communication using either protocol.
4. Based on Local Development/Testing:
Local Clusters: Tools like Kind (Kubernetes in Docker), Minikube, or K3s allow developers to run lightweight Kubernetes clusters on their local machines for
development and testing purposes.
These categories are not mutually exclusive, and a Kubernetes cluster can exhibit characteristics from multiple types simultaneously. For example, a 
multi-zonal cluster could be deployed in a cloud environment with a dual-stack networking configuration.
**********************************************************************************************************

  SO FAR;
I understand that in deploying an application that requires endusers to enter some information ,then the deployment requires 2 application & this is will
involve 2steps
STEP1) DEPLOYING THE 2 APPLICATIONS
a)the main application which is stateless bc no info is been entered into it therefor it doesnt require a piece of storage to help retain/remember such info
 b) A database where the information entered by endusers will be captured, this requires to create a piece of storage to help retain/remember such info

STEP2) CREATING A VOL/PIECE OF STORAGE FOR THE DATABASE
The types of vol includes:
awsElasticBlockStore, azureDisk, azureFile, configMap, emptyDir gcePersistentDisk
gitRepo (deprecated), hostPath, nfs, persistentVolumeClaim, secret
Docker 8&9
Docker Volumes
Volumes stored data in a part of the host filesystem managed by docker

    THe type of vol been created can either be persistent or non persistent
NON PERSISTENT VOL: eg bindmount
non persistent means it can be accidentally removed/deleted sudo rm -rf because its not managed by the application process eg docker process
 30:23 Non-Docker processes on the Docker host can modify, Bind Mounts volumes at any time unnoticed so we nid to introduce persistence vol 
PERSISTENT VOL:
 most of the types of vol are persistent including docker vol
However in Kubernetes, its recommended to create all volumes with a PVC  ** eg for NFS with PVC, online i saw its called NFS based PVC..
so far, i understand that when deploying an application that includes a database, its best to use the volume concept provided by the application eg docker,
use docker vol, for kubernetes; use kubernetes volume
KUB 10&11
we always need to maintain its state bc it captures data and we nid to ensure the data is stored and retrievable and vol concept in k8 is what
we can use to achieve that process


**online**
A persistent volume is a storage resource in a cluster that has a lifecycle independent of any single pod. It provides persistent storage for stateful 
applications, ensuring that data is not lost even if the pods using it are deleted or rescheduled. 
Administrators provision persistent volumes, which can be backed by various types of physical storage like local drives or cloud-based systems. 


  SCALING
based on how we seperately deployed the 2 application; ie the main application with a nodeport service & the mongo db application with a clusterip service
i bliv we scale only the data base application, bc its the application concerned with inputing information to be stored, & thats why a piece of storage is
attached which is associated with how much storage space it has & hw much is been consumed.

DOCKER vs KUBERNETES
1)In docker we created networks, & ensured the appl are created in the same ntwrk to ensure communication 
BUT
In kubernetes, its uses DNS resolution ie service discovery eg clusterip , nodeport, loadbalancer service.
2)Docker uses just docker vol concept WHILE  kubernetes uses kubenretes vol concept with PVC
3)In kubernetes we can scale using controler mangers & we can also auto scale using hPA/CAS/VPA BUT in docker swamp we can scale, for now nt sure we can autoscale



**********************************************************************************************************************************************



KUB 3&4
DISADVANTAGES OF DOCKER    ****bc of the disadvantages of docker, we therefore use tools like docker swamp, kubernetes & open sheet.     

 we can use software like docker to deploy applications ,to deploy n mange containers bt is it recommanded, shud we use 
docker ??? we shud nt and u are going to see that shortly, we shud rather use container managers or ochestration tools like docker swarm kub n open sheet.
when we studied dodcker, we deployed appl using docker bt what are the limitations using docker to deploy applications, we ar going to look at that shortly
7: 43 bt these ar the contarization software  ... check

with docker i cannot v more than 1 replica of webapp for e.g u are deploying ur appl n ders a spike , no i cant v multiple replicas in docker , e.g: so if u decide to run
docker run -name webapp -replicas 5 , ie trying to get the number of replicas u wnat, it wont work.. docker doesnt support it
also, if my engine goes down , my contaiiners will stop running 
also docker doesnt support a scenario wher i can v multiple serveers in a cluster  e.g docker engine1 (with container), docker engine2(with containers) such that if one 
engine is down the 2nd engine can support it ,, docker doesnt v such sys bc the docker ntwrk dose nt support MULTI HOSTING where we can v a cluster of servers, 
docker netwrking sys doesnt support multi hosting and thats a big problem ,, the docker system doesnt v an overlay ntwrk wher u can v a cluster of server,
no overlsy ntwrk support and d worst is that when appl ar deplloyed, to expose them to endusers is very limited and we wil see hw it is diff with kub...
bc in docker we do port forwarding bt we wil see what kub brings to the table  .. i bliv port forwarding is how we expose the application (on your internal 
netwrok) to the internet


    HOW Did WE DEPLOY A WORK LOAD IN DOCKER ??
we had to use docker resources or objects to deploy applications like docker file we use it to build images n we can now deploy our appl wit the help of 
ntwrks, vols, using the docker-compose file.
........docker resources/objects use to deploy applications:
  Dockerfiles/images/networks/volumes/docker-compose.yml/etc. but the docker compose file doesnt have the 'deployment strategy' so we didnt hv the option of 
replicas


DOCKER 1&2
 APPROACH USED TO DEPLOY WORKLOAD IN DOCKER 

WE DEPLOY workloads using Imperative and declarive approaches    

1-#Containerize application Imperative approach =  Command
   LIKE:
docker build -t teslaimage:1 .
docker run -d -p 80:8080 --name webapp teslaimage:1

2-declarive approach= DOC7.. makes use of deployment script *ie(vi to create eg spring.sh & put the image,db,netwrk command) & compose files/less commands 
Docker Compose is a tool for defining/declaring and running multiple containerised micro-services applications.
-# Using compose file     *** but this compose file doesnt have the 'deployment strategy' like docker swamp & kubernetes
  **while d docker compose file for docker includes netwrk, d compose file for docker swamp does nt include netwrk, because an overlay netwrk has been initialized


Docker 10
2.58.56****** WHEN U RUN DOCKER SWAM init, 
By default it creates an overlay ntwrk, the overlay ntwrk is what permits your docker server to communicate in a cluster like this bc in normal docker der 
is no overlay ntwrk for that reason, you cannot v docker servers in a cluster like this bc node1 cannot talk to node2 in normal docker
so once u v the overlay ntwrk, when u create ur containers , when you create ur services, those services can talk to each other irrespective of whether they
ar created in node1 or node10 bc of the overlay ntwrk n the DNS registration is done using the service names

            OVERLAY  NETWORK
whenever we ar deploying in a swarm, our containers wil be created in the overlay netwrk..overlay ntwrk means that u can communicate wit dif nodes, the master
can communicate with multiple nodes using the overlay netwrk, with overlay the container in node1 can talk with the conatinters in all the other nodes.


DOCKER 10
docker swarm has 2modes: REPLICA MODE & GLOBAL MODE
1)Replicas ---> it will deploy based on replicated number    (we v used this mode to deploy above)
***** this is the default mode & with this mode, it wil deploy base on the number of replicas that you want     ... 2:11:24
2)Global--- its like daemon set in kubernetes, its good for deploying logmgt container for monitoring
with the global mode for e.g, if u v a cluster that has 14servers or 14 nodes(3master + 11 workers)
if u v a 14 node cluster n u want to deploy an appl using the global mode what wil happen is that a replica wil be created in each of this node 


KUB 1&2
HOW WE DEPLOY APPLICATION IN K8
########### hw we deploy applictaions in k8 , which is a very key aspect when it comes to my understnading of k8, 
 OBJECTS USED FOR DEPLOYMENT
kubernetes resources/objects used to deploy application include/RUN WORK LOads:
 1)  Pod : ***pod is nt recommemded bc we cant scale bc the pod template doesnt hv the option to create replicas like controller managers, so when a pod dies 
or                                                                                             its nt recreated and that is dangerous.
2)controllerManagers:   ***Deploying with controller managers we are able to scale
      Replication Controller; uses only; matchlabels which is Equality based 
        ReplicaSet; uses both matchlabels which is Equality based &  matchexpression which is set based:, can rol out new version but cant roll back
        DaemonSet,  creates a pod in each of the nodes.. good to use to deploy logmgt for monitoring purpose
       StatefulSets, 
       Deployment,   has strategy like 'Recreate' OR 'rolling update' which is the default strategy, can roll out new version & can roll back
       Volume, Job  

 3)                                     TYPES OF SCALING
Manual scaling (Controller managers) OR Auto scaling the 'replica' using either of these objects:(horizontal/vertical/cluster autoscaler)


4) Kubernetes Service discovery objects:
   ClusterIP , NodePort, LoadBalancer, i bliv this comes with ELB, ExternalName, ingress, networkPolicy


      

*****************************************************************
KUB 3&4*    POD
in k8, the pod lifecycle is very short so if we ar going to deploy appl, we shud nt use pod
We should not create pods directly to deploy applications.  **kub5**     lacks self-healing capacities
If a node  goes down in which pods are running, Pods will not be rescheduled.
We have to create pods using controllers which manages the POD life cycle.
controllerManagers: ReplicationControllers , ReplicaSets, Deployments, DaemonSets


    **mee**  WHAT A MANIFEST FILE CONTAINS/DETAILS FOR EACH CONTROLLER MANAGER & OTHER K8 OBJECTS/CONCEPT
1)**Mee*** therefore for Rc, in the manifest file, kind will be 'rc' & we will also hv a template for pod inside the same manifest file & in the RC section,
we have selectors & spec,for spec; we will indicate the number of replicas but if we dnt,it wil create 1 replica & if we need to create service, we wil also 
have a template/section for service
2)for RS, in the manifest file, kind will be 'rs' & in the replica set section, we have selectors & spec, for spec, we will indicate the number of replicas,
& we also have a template for pod inside the same manifest file & if we need to create service, we wil also have a template/section for service
3) for daemon set, kind will be Daemon set; in the Daemon section, for spec, we dnt indicate no. of replicas bc because daemon schedules pods on all the nodes
so we dnt need RC/RS, because there is no replica template, i bliv thats why we cant scale with daemon set. & we also have the template for pod, & service.
4) Deployment rolls out/deploys a replica set therefore; in the manifest file, in daemon section, we have spec & selectors, for spec; we 
 we will indicate the strategy type eg rolling update or recreate, but if u dnt indicate the strategy rolling update is the default strategy & if u dnt
indicate the replica, i replica will be deployed & we also hv the pod template & service template
      ***controller managers end****
 5)                        AUTOSCALING
****The manifest file states the minumum& maximum number of replicas of the application/pod to maintain at all times.

6)          KUBERNETES VOLUME CONCEPT
In k8, we can create volumes with or without PVC    ***for it to be persistent, its adviced to use PVC
1)   HOSTPATH
 a) Without PVC; the hostpath created is like a mount point just like in 'DOCKER bind mount', we hv to create a mount point & its not persistent
 b)With PVC; the hostpath created is the PVC,  its mg8 by kubernetes so its like 'Docker vol' we created in docker , its persistent bc its mg8 by docker 
   it has 4 manifest files  .. but we can use ansible
2)    NFS OR EFS 
        for NFS,with or without PVC, yon have to create NFS server in AWS
     a) Without PVC; the NFS/EFS created is like  external vol 'EBS' we created in Doker, its persistent but i think not mg8 by k8
      b) with PVC; the NFS/EFS created is mg8 by kubernetes so its like 'Docker vol' we created in docker , its persistent bc its mg8 by docker 
            it has 3manifest files... but we can automate with ansible




                   ***meee***        STEPS FOR OCHESTRATING CONTAINERS
i bliv the following steps starts after we have used Docker to containerize the application, then for management & deployment of the application/container
we continue with these steps;

***meee, kubeadm & kops are self managed, they are not paid service, while EKS is paid service the contrl plane is manged by aws (the cloud provider)
by we mg8 the master/worker nodes ourselves

1a)After installing either self managed (multi node preferred), OR  paid to manage 
  b)we install KObs because of the disadvantage of using kubeadm eg it doesnt create aws resource for us
   therfore if we are using kops skip  step (2.3.5,6,7 & 9) bc kops creates them for you including ELB, bt for no.9 to cut down cost, we may need to create nginx
   c) EKS   ....  provides a fully manged contrl plane7 ... then i will need to create frm no.2 below
  WHEN U CREATE an eks cl8 u dnt v access to d master node, the master node is exclusively managed by eks which means that u can only mange ur eks cl8 by using
a remote node n call it our eks control, wich is jst used to control the eks cl8, so it cud be ubuntu node, whatever node it is, can be used.. dats what we did
you cud use anoda node that is nt ur k8 , that is nt found in ur first worker node n master node for kubabdm n kops, so u can jst create a new node for that purpose n use  
that node to manage ur eks cl8 
   D) GKE   .. didnt illustrate this
2)create namespace, add label in pod template, so service can make pod discovered/accessible,
3) create service type either clusterip or nodeport (using service template)
4)for manifest file for deployment; chose a controller manager, k8 object for deployment prefered 'DEPLOYMENT' &Its rolling update strategy, although he said smt about 
carnary, blue green techniques, bt didnt apply it.
5) Resources request & limit ie cpu/ memory for the pod/application
6) Autoscale the controller manager
7) Kubernetes Volume concept (vols can be created with or without PVC  but for it to be consistent, using PVC is recommended)
stateful; we always need to maintain its state bc it captures data and we nid to ensure the data is stored and retrievable and vol concept in k8 is what we
can use to achieve that process
 for NFS with or without PVC
the NFS server has to be launched in the same vpc like the other servers ie k8 servers, like the master node . *** mee** like all the servers in the cluster..
can create a security grp for nfs with the REQ ports open like  ssh  port ffrm anywher, nfs 2049 
Configuration of NFS Server ( WE CAN USE ANSIBLE TO AUTOMATE THE PROCESS)
8) configmap & secret
9)ngnix/ ingress rule  .. this is required for self manged cluster
10) Helm ... its a package manger for k8 (makes use of charts/creates manifest for deployment so we dnt hv to write manifest or add repositories to use for deployment)
   helm can be deployed in any cl8 be it kops, eks etc once its installed u can then use it to deploy Ur appl
11)probes for pod health check 
14)promethus & grafana ..Prometheus and Grafana are a popular combination used for application performance monitoring (APM) Prometheus: Gathers metrics from your applications, 
     Grafana Connects to Prometheus as a data source to create customizable dashboards. These dashboards allow you to visualize the metrics collected by Prometheus
15) EFK/ELK Stack.....  to centralize our log mgt
   Rancher   ... deployed using docker/Docker is reQ to install it....
16) Security  ... RBAC

35.00****kub 10&11  ....  so one deployment requires all of this 
********************************************************************


                   CONTROLLER MANAGERS eg RC,RS
1.42,00   Cretaed pods using Replication controller, when we delete them, we see they are RECREATED.
all the pods have been rescheduled/recreated bc my pods are been manged by a 
 Replication controller n the RC has a label app:web n the RC has a selector that has selected the label, once it is
 selected, the pod labels hv defined that the replicas is 3, so at all times, the RC must maintain d desired state of my appli which is 3 

***my own explanation, the pod template has a label & the RC section has a defined the number of replicas & the RC also has a selector that will select the 
label of the pod template & as such, the RC will maintain the defined number of replicas for the app/pod at all time.


advantages of namespace
****clusterRole &clusterRolebinding is for the entire cluster 


 ****in manifest files;
pods have label .. ie key value pair
ReplicationControllers has selectors which is used to be able to connect to the pod label


LABELS & SELECTORS EXPLAINED
****SERVice makes pods accessible using labels & selectors

**************************************************************************************************************




 



   SUMMARIZED FOR INTERVIEW

KUB 1&2
XPLAIN THE K8 ARCHITECTURE 
kubernetes  is a open source, or ochectration  tool  used to Orchestrate and managed containerised Applications
k8 is a grp of servers that are working together to be able to ochestrate containerize applications, its made up of a control plane & worker node, where the
applications are running & our applications are deployed in pods & the control planee comes with components line the API server which is the main
adminstrative point and the entry point into the cl8 & it does generally authentication & authoriztion & it ensures that whatevr Req GETS into the cl8 is 
been made by an authorized & authenticated individual, apart of the API server, we hv the etcd that persists data in the cl8, it acts as a database ,whatevr
reQ is been made by whoever engr that inf is stored and persisted in the etcd and frm there we also hv the scheduler, which is incharge of scheduling  eg if
pods are been REQuested to be scheduled the sheduler will schedule that considering the nodes that hv available resources for such a pods to be hosted & if
thers any other constrain all the constraint are been factored in we also hv the controler managers that is able to ensure that at all times we hv a number
of nodes running ,pods running ,deplyments are runing as expected.
we also hv the worker nodes comes with the kublets which is the pry node agent, we equally hv the kube proxy, which is incharge of netwrk 
also the container run time, there are diff types of container run time that you can use, docker used to be one of them , also container-D, container runtime
is any software in your node that is able to create and start containers bc k8 is all abt managing containerized appl n in our env we use containerD as our 
container run time, we cud hv as well choosen other run time but we are using containerD

Also the k8 architecture is such that calls are made into the cl8 and that happens using the k8 client like kubectl or k8 UI form there we can mk certain API 
calls either to create containers, namespaces, security like RBAC and for any of these to happen, authentication must happen via the kube config file
so thats the k8 architecture


KUB 3&4
FOR autHORIZATION;
authorisation via RBAC: Roll base access control, 
this is under kub security , u hv to be authenticated n authorized to perform that task
once u are authentictaed n authorized , the task is being executed bt if u werent succesfully authorized, u wil get permission denied/ authentication failed

After sucessful auth/author, 
in docker we deploy our containers directly in containers, in kub they are deployed in what is called pods
so pod is the smallest unit in kub
so u wnat to deploy an appli, dis is what happens, the API server receives this information either frm UI or kubctl, n immediately it receives it, it will
persist the data in our key value store ie etcd,, (i bliv UI is using imperative/declarative & kubctl is the way we create static pod using the 'etc/' dir)
now u wnat to deploy your appl which are deployed in pods .... in your cluster, we hv the worker nodes that comes with the kublets which is the pry node agent
therfore, the control plane is communicating with the worker node via this pry node agent ,once it commun wit the worker node ,it wil be checking for resouce
availability u wnat to deploy a pod , this is a scheduler, this pods requires abt 800mb bt when it comes to availabilty: my node1 got abt 700mb of memory, 
node9 got abt 8000mb of memory we hv d scheduler, hes going to schedule the pods based on resource availabilty & other factors, all other factors are constant.
At this time the scheduler will do automatic scheduling on node9 which has enough enough resource availability.now when the pod, lemme cal it 
webpod is scheduled on node9, inside the webpod containers nid to be created now the container runtime wil create n start the container, so now i v my 
webcontainer inside the pod.this is the kub architecture, very powerful 
now i v my appl scheduled on the pod, and we nid to deploy multiple replicas , in this case we v 2replicas, the scheduler has scheduled the 2replicas on the 
2diff nodes.therfore 2replicas of the pod have been deployed
inside this cluster we wnat to ensure that we can access this pod, for the pod to be made accessible we ar going to make what is called a service in kuber.
35:34
so u v a service , this service act as a LB
E.G I V A webSVC, what wil happen is that if u wnat to access this appl running in this pod in kub, it is going to be made discoverable using the service
so if u wnat to access the appl, it wil be routed via the service, we call this service discovery, u wnt to discover ur appl, the service is involve
now this entire netwroking wher appls can accessed with relative ease is been suported by a ntwrk agent kube-proxy.

###########if u can xplain dis kub architecture, u ar as good as done 37:26



CLASS 33 vide 5&6.... 52:30 .... i will watch the video later...........
************* for each of the version it goes tru this steps: we create the packages, then run docker build to contianersrize the appl and ship the image with
docker push to the image registry 

this is an extract of the entire CI/CD process  .......7:58 ....(qusetion n answer vidoe 5&6)
b4  we containerize, containerize, b4 we ar creatin images, we had gone tru code quality analysis, and depending on the type of project, der are projects were
we execute sonarqube analysis b4 even creating packages wit mavin,, so if u do that u are good to go, so dis is jst a summary  bc we didnt hv nexus or sonar 
qube connected in this k8 but on our CI/CD pipeline, we hv developers write code, developing new update of the appl, and once they do that they commit to 
github and we can connect github to soanat cloud wer once der is a new commit, sonar cloud wil run code quality analysis, its only when that process passes,
wher our sonarqube or sonar cloud report indicates that it has gone tru all our quality threshold, evrytin is fine , der are no vulnerability in the code, no 
smell, bug n issues , then once we v realised that then we now bring in maven wich then creates packges , once maven is done we upload artifacts into nexus
and frm ther we can now build our image using docker, once image is built we use k8 4 deployment.

INORDER FOR US to update the new versionin on the deployment session on k8, that is done by webhook or pull ACM 
*************SOME ONE SAID ,, with the help of the webhook created in github wich is triggered when thers a new comit, github  then notify jenkins to do a
build ...32:17

*******i will share the jenkins kubernetes integration, it covers evrytin am tryin to xplain to u now, ,, a lot of folks have had jobs by just goin tru that
video, hw all this tools connect to ecah other




**************************************************************************************************************


******if we dnt pass replica, its goin to create one...

*me*Docker is 'compose file' deploy with command 'Docker compose up -d' WHILE kubernetes is 'manifest file' deployed with command 'kubectl apply -f pod.yml'


                           TYPES OF OCHESTRATION TOOLS
Container Orchestration Tools --> :
   Docker Swarm, Kubernetes, OpenShift



START

KUBERNETES
IT is an ochestration engine, an open source platform for managing containerized applications... kubernetes is responsible for container deployment, scaling, descaling of
containers and lB.
actually, kubernetes is nt a replacement for docker bt can be considered as a replacement for docker swamp , kuber is basically more complex than swarm and REQ more work to 
deploy.. born in google written in go and go lang
when we talk abt kub, we classify an all imp tool bc of its imp  of to me as a devops engr, its so important that one of the jobs i can do is a kubernetes administrator 


KUB 1&2 .... 1.43.50
kubernetes uses the kubectl client or the UI to run workloads.




TK NOTE OF THIS TREE
kubernetes:    kub has a cluster , 
   cluster: it is a grp of nodes
   nodes:  in the nodes ther are pods 
   pods:  in the pods ther ar containers ..  
   containers :  

therfore in kub we can say that nodes are the subset of the cluster , pods are subset of the nodes and containers are the subset of the pods
cluster ---> nodes ---> pods  ---> containers  :
********therfore if u are going to deploy any appl in k8, it is going to be deployed in a pod

PODS:
====
POD --> Pod is the smallest building block use to deploy applications in k8s.
Pod represents running processes. Pod can contains one or more containers.
These container will share same network, storage and any other specifications.
Pod will have unique IP Address in k8s cluster. 

Pods
 SingleContainerPods --> Pod will have only one container.       98%
 
 MultiContainerPods(SideCar) --> POD with two or more containers. 2%  
e.g, we can v the  the application container and like a sidecar container
       application Container :
          e.g  webapp    , e.g u v a container running and u wnat to ensure this appl doesnt v any problem, so u attach a logmgt container so that whatsover is 
       SideCar containers:                                                                             happening in the appl, this log is reporting it
            e.g  logMgt  container   ... like a container that is collecting log from ur application,  can als have like a utility container for supplies



 cONTROLLER MANAGER.. >>>>>>>>>
E.G IF U deploy ur appl and you want 3pods to be deployed , lets assume 2pods shud be running, if 1pod is down, the controller manger has been instructed to create 2pods, if
one pod goes down, it will automatically create the pod, those are the function of controller mangers
*** Meee.. it knows what to do at any given time, eg one pod is down, it automatcally brings up another one.



fEATURES:    *** OF K8
1)the first feature it has is that it supports what is called automatic scheduling ie it provides advance scheduler to launch contianers on node
2) it has self hidden capabilities, rescheduling, replacinging and rescheduling containers which are dead and the bauty abt it is that if u deploy a wrong application u can 
go back to what was running before so it has automATED ROLBACKS AND rollouts 
so kuben supports rollouts and rollbacks for the desired state of containerized application therfore, if u had a version running and u v a deploy a new version n ders a 
problem, u can rollback 
3) it has horizomtal scaling and LB , kub can be scaled up and down d appl as per REQuirment
4) service discovery & LB
5)WITH kuber DERS no nid to worry about netwrkn n communicatn bc kub wil automatically assign ip address to containers n a single dns name for a set of containers that can LB
TRAFFIC inside the cluser .... 5:00
containers get their ips so u can put a set of containers behind a single L Balancer ... this is a very key aspect service discovery n LB , hw powerful is that 
6)it also supports storage ochestration, u can mount the storage system of your choice , u can either opt for local storage or choose a public cloud 





          KUBERNETES CLUSTER & ITS COMPONENT

kubernetes architecture:  why are we doing kub and why is kub imp in my devops journey , that in all thy knowings u must knw kub
                                ############  kubernetes comes with a cluster in the cluster we v :
controller noder or controlPlane/MasterNodes: this has some components to tk note of:
  apiServer             : this is the main entry point into the cluster, it performs admin task in the cluster
  etcd                  : the key value store ,,, this is like a database
  scheduler  : 
  controllerManagers   : 
workerNodes           :  this is wher appl are running and here we can v node1, to node9  . the components of the nodes includes:
  kubelet             : this is the main agent that runs on the worker node. its the pry node agent
  pod                 : is the smallest unit in kub, in kub contaners are deployed in what is called pods
  container runtime   : and d container runtime we ar going to be seeing is = [Container-d] 
 kube-proxy           : the netwrk agent
21:30
############### in kub if u wnt to perform any task e.g u want to deploy an appl , for that deployment to happen ther are a few things that kub uses, we call it d client
kub client (how u can access ur cluster)  for e.g we hv docker client , the docker cli
22:11
kubernetes-client      : we also hv kub cli , command line interface ,, command line interface here is kubectl
  kubectl              : if u are authorized, u can run 
     kubectl create/delete/get/describe/apply/run/expose 
  so we v the cli which we can use to make some api calls and we also v other APIs 
  ui- DASHBOARD  
  api  ,, others APIs
Or it has a UI wich is called  a dashboard ,, since we started this programe wher v we see any dashboard , to deploy in jenkins we were able to go to the browser,  der was 
a nice dashboard wher we cud create and run jobs
Now in kub, to perform any task in ur cluster,  for docker we cud do stuff like docker build, docer run n our docker client was docker bc all the commands started wit docker
our kub cli is kubectl ... this is our kub server, u want to run a job in kub , u run commands like kubectl run to deploy an appl, what happens is that kubctl is like a msg
that is sent into ur cluster , dis msg comes to ur cluster, u wnt to deploy an appli, so it cud either b frm ur dashboard, or  ur cli. then regrdless of wher the msg is 
coming from, d API server receives the msg on behalf of our cluster,d API sevrer performs admin task n when u execute dis command, this will get you tru authenticaton,so  u
nid to be authenticated n authorization,, 2things wil happen now u v executed kubctl run, wich means u wnt to deploy an appl, for dis to happen, kub nids to authenticate n
authorize u and the authentication and authorization process wil tk place using a file called .kubeconfig file . this is the file dat authenticates you

kubeconfig [.kube/config ] file will authenticate the admin    ....(ur tryn to mk an APi call, tryn to dd smt to b done) so u must be authenticated) 39:34
                               the caller admin/Developer/Engineer 




 AUTHENTICATION & AUTHORIZATION

FOR autHORIZATION;
authorisation via RBAC: Roll base access control, 
this is under kub security , u hv to be authenticated n authorized to perform that task
once u are authentictaed n authorized , the task is being executed bt if u werent succesfully authorized, u wil get permission denied/ authentication failed

WHO CAN DO WHAT, who can perform what task .. ,, THIS WILL BE SUPPOERTED BY KUB SECURITY  .. 40: 26
kubernetes security - RBAC:
  Developers [ Paul, Joyce, Chidi ] 
  Engineers  [ James, Dominion, Janet ]    .... we can assign them diff functions

authentication via kubeconfig : 
authorisation via RBAC:

  AUTHORIZATION IN AWS
         to be authorized to create an aws EKS cluster in aws, i can create an IAM Role for EKS , this wil permit us to create and manage an EKS cluster
it wil be the same thing for Azure AKS, goodle GKE and Ibm cloud

************* so if i v like 10clusters that i wnt to manage, wich i wnat to be observing hw dey are performing at all times then i need to instal the rancher 
sfware so i can use it to access those clusters



                                           2TYPES OF KUBERNETES CLUSTER YOU CAN INSTALL
1) Single node cluster 
2) Multi Nodes Kubernetes Clusters   .. This is our focus....



                             WAYS TO MANAGE KUBERNETES CLUSTER
***meee, kubeadm & kops are self managed, they are not paid service, while EKS is paid service the contrl plane is manged by aws (the cloud provider)
by we mg8 the master/worker nodes ourselves


you can deploy your cluster using, either a manged or a self managed sfware
ideally, companies will prefer to go for managed bc it will give them more time to focus on the app , bc if u nid to focus on the architecture, it wil mean
that u wil need to spend more time ensuring that ur cluster is running than managing containerized appl,,  bt more time shud be dedicated to appl management.


the first of its kind which we can install is : 
1. Self Managed Kubernetes [k8s] Cluster = IaaS--EC2  :  thess ar infrastructure as a service , we can launch of k8 cluster on a grp of ec2 instances, using kubeadm
    kubeadm --> We can setup multi node k8's cluster using kubeadm.  ... kubeasm is a sfware that is used to launch a k8, both d control plane n worker node
    kubespray --> We can setup multi node k8s cluster using kubespray , it uses ansible playbook
     (Ansible Playbooks Used internally by kubespray).
WITH  Self Managed Kubernetes [k8s] Clusters both the    
     controlPlane: [apiServer, etcd, scheduler, Controller Managers] 
      and 
     workerNodes: [  kubelet, containerRuntime-Container-d, kube-proxy]  
  are managed by the Admin/Kubernetes/DevOps Engineers

*************** so if u have a self managed cluster u hv to ensure that the control plane wit all its components re healthy n functioning and also the components of the 
woker nodes are all running and healthy, it is ur function

we v anoda class of clusters wich are  manged by 3rd party, u will nt bother abt the mgt
2. Managed k8s Cluster  (Cloud Services) = PaaS  : 
   The controlPlane is managed by a cloud provider or third party.  
   The controlPlane and all it components are managed by the Cloud provider 
   ***************  howerever,   The workerNodes are managed by the Admins or engineers

        
         TYPES OF CLOUD manged service:    ***I bliv for ochestrating contaners
   EKS --> Elastic Kubernetes Service(AWS)
   AKS --> Azure Kubernetes Service(Azure)
   GKE --> Google Kubernetes Engine(GCP)
   IKE --> IBM K8s Engine(IBM Cloud)
    Kubernetes Cluster = k8s  

########### so u can deploy your cluster using, either a manged or a self managed sfware
ideally, companies will prefer to go for managed bc it will give them more time to focus on the app , bc if u nid to focus on the architecture, it wil mean
that u wil need to spend more time ensuring that ur cluster is running than managing containerized appl,,  bt more time shud be dedicated to appl management.

*******************we also v another clas of cluster that can be installed ... KOPS

    
KUBERNETES DISTRIBUTION TOOL

****ONLINE
Kubernetes distributions and tools: Several open-source tools and distributions assist in deploying and managing self-managed Kubernetes clusters, offering 
varying degrees of automation and opinionated configurations. Examples include:
Kubeadm: A tool for bootstrapping a minimum viable Kubernetes cluster.
Kops (Kubernetes Operations): A tool for building, operating, and maintaining production-grade Kubernetes clusters in the cloud.
Rancher: A complete software stack for managing multiple Kubernetes clusters.


3. KOPS: is a software use to create production GRADE/ready k8s in AWS and  
         azure for the kops beta version  
         It creates a highly available kubernetes services (he said cluster, mayb a mistake) in Cloud like AWS.
            KOPS will leverage Cloud Sevices like: .. so when u deploy wit kops, it comes along with all dis services n its very esay to craate a kops cluster 
              vpc, 
              AutoScaling Groups, 
              LoadBalancer, 
              Launch Template/configuration
              ec2-instances nodes [workerNodes and masterNodes]

kops is an open-source tool for automating the creation, management, and maintenance of production-ready Kubernetes clusters in cloud environments. It handles the provisioning
of cloud infrastructure and automates tasks like cluster upgrades, scaling, and deletion, allowing users to manage their clusters with a configuration-as-code approach.
While it officially supports AWS, it also offers support for other providers like GCP, DigitalOcean, and OpenStack. 

 **online
Kops (Kubernetes Operations) is an open-source and self-managed tool for deploying and managing Kubernetes clusters. It is not a paid service itself. 
Here are the key points regarding its management and cost:
Self-Managed: When you use kops, you are responsible for managing both the Kubernetes control plane (master nodes) and the worker nodes. This means your team handles tasks 
like patching, upgrades, monitoring, and incident response.
Open-Source and Free: The kops software itself is completely free to use.



                             A TOOL WE CAN USE TO MANAGE MULTIPLE CLUSTER 
Rancher: - Using Rancher we can deploy both managed and self managed k8s CLUSTER
           Rancher serves as a glass to access and manage multiple k8s  
           from the rancher dashboard [UI]  - rancher dashboard  ,,,, frm rancher i can create a cluster in EKS/AKS/GKE/IKE 
           authentication and authorisation: EKS/AKS/GKE/IKE  ... bt u v to be authorized


*****online
Rancher is an open-source platform that provides a unified interface for managing container clusters, especially Kubernetes. It simplifies the deployment, management, and 
scaling of containerized applications across various environments, including multi-cloud and on-premises. Rancher adds value by centralizing authentication, providing tools 
for monitoring and logging, and integrating with other DevOps tools to manage the entire application lifecycle. 
Key functions
Cluster management: Allows users to manage multiple Kubernetes clusters from a single dashboard, whether they are in the cloud or on-premise.
Centralized control: Provides a single point for managing user access, authentication, and security policies across all clusters.
Application deployment: Simplifies the deployment and management of applications on Kubernetes, including integrations with tools like Helm.
Monitoring and logging: Includes features for detailed monitoring, alerting, and shipping logs to external providers.
Hybrid and multi-cloud support: Enables consistent governance and policy enforcement across different cloud providers and on-premises infrastructure. 
Other meanings
Rancher (person): In a general or dictionary sense, a rancher is a person who owns or works on a ranch, typically raising livestock like cattle or sheep




DEPLOY/INSTALL MULTI CLUSTER

DEploy master node:
in mobaxterm connect to the server then set hostname to master  and sudo i to switch to root user
in the installation script first, we v to 
we nid to diasable swap n kernel setting,so we wil disable swap memory so that we can enhnace performance in our k8 cluster , then nxt, we ar adding kernel
details n instal container-d, first instal container-d n its dependecies bc our runtime (to b able to start&runour conatiners) is nt docker bt it is container
-d, once it is installed, we will proceed to start container-d  afterwhcih we wil instal kublet, kubeabm and kubectl,, after that we wil start the kublet
service i will run this as a script and i nid to be root user
Initialize Kubernetes control plane by running the below commond as root user.
sudo kubeadm init
****************** but am a root user, so i dnt nid sudo. ... just run kubeadm init 
and for us to be able to initialize this control plane the req port must be opened. 
************************** SUCCESSFUL... now i can process as a regular user

to launch nodes/Wokers:   ***************

8)  Generate the master join token on the master node
so am going to generate the master token on the master node
kubeadm token create --print-join-command                                                                               1:14:44
once the token is created,



K8 BEST PRACTICES FOR LARGE CLUSTER

lets look at k8 BEST PRACTICES FOR LARGE CLUSTERS     *********** so if u have a very large cluster you can click on this link, u wil get more info
https://kubernetes.io/docs/setup/best-practices/cluster-large/
For very large clusters NB: dnt create :
   No more than 5,000 nodes
   No more than 110 pods per node
   No more than 150,000 total pods
   No more than 300,000 total containers


WE WANT TO DEPLOY WORK LOAD IN k8
***ticket002
========
************************************************************    Deploy workloads in kubernetes;
kubernetes resources/objects used to deploy application includes:
kubernetes Orchestrate and managed containerised Applications  
This applications run as containers  
These containers are housed in pods  
Pods are housed in nodes    
nodes are housed in the cluster   
********therfore if u are going to deploy any appl in k8, it is going to be deployed in a pod

now we v created our k8 cluster ,how are we able to deploy appl using this cluster ,, ????????????? dats some of the things we wil be able to look at:


    OBJECTS USED FOR DEPLOYMENT
kubernetes resources/objects used to deploy application include/RUN WORK LOads:
   Pod :
or    
 controllerManagers:
      Replication Controller
      ReplicaSet
      DaemonSet
      StatefulSets
      Deployment
      Volume
      Job       


when u deploy appl in k8, u v to make the appl accessible ie exposing appl/accessing appl
Exposing/accessing applications = Service Discovery:
    Service Types:  it will route service to our appl n their replicas ,
    ClusterIP :  it performs LB  , its used for internal communication inside the cluster.
    NodePort
    LoadBalancer
    ExternalName  
  ingress 
  networkPolicy 




                                 NAMESAPCE     ***i bliv its kinda similar to label & restriction in docker swamp
1:34:30  **********************generally for this deployment to tk place even in k8, we will make use of a namespace
how DO WE ENSURE THE APPL ARE ABLE TO COMMUNICATION  & all of that , ders a concept in k8 called:
Namespace:
  It is a virtual cluster inside your cluster  ,,, we can create a name space for dev stage, uat stage , prod stage , sales ... dependn on d project u are managing
     [ dev / uat / prod ],                            if u create a namespace for e.g, u wnat a situatn wher what dev ar doin does nt affect what is runin in uat &prod
     [sales, accounts, cs, payroll]                so we can use name spaces for isolation, we'l isolate d dev env frm uat, frm prod using namespaces





 APPROACH USED TO DEPLOY WORKLOAD IN K8 ????????????????

WE DEPLOY workloads using Imperative and declarive approaches    

-#Create Name Space Using Imperative approach =  Command
   LIKE:
kubectl create namespace <nameSpaceName>
    kubectl create namespace dev  

-declarive approach =  makes use of files and less commands  
-# Using Declarative Manifest file 

 Use the declarive approach to deploy workloads in kubernetes:
  Manifest files = kams       ........ we use manifest files to deploy in ddeclarative approach
  Manifest files are written in yaml/yml language 

pod.yml  : in this yml language, it deals with stuffs like :
key:value  pairs     
dictionary: number of key:value pairs 
list:

                        KEY VALUE PAIR
hw do we consider keyvalue pair 
**************for e.g when u want to create a manifest file, it has this acronym, comes :   where 'comes' stands for kind, api version, metadata and spec
kind is a key value pair bc for example ; kind is pod , this is a key value pair, it has a key & a value  ,,, so key &value ,, kind:pod
for e.g , key value pair could be  name and the value is simon
key:value  pairs 
name: simon   
 

K8 BEST PRACTICES FOR LARGE CLUSTER

lets look at k8 BEST PRACTICES FOR LARGE CLUSTERS     *********** so if u have a very large cluster you can click on this link, u wil get more info
https://kubernetes.io/docs/setup/best-practices/cluster-large/
For very large clusters NB: dnt create :
   No more than 5,000 nodes
   No more than 110 pods per node
   No more than 150,000 total pods
   No more than 300,000 total containers




 APPLICATIONS IN OUR REGISTRY   ... ..... these are all images we can use to depploy our appli in k8
in our image registry , we have applications like 
Docker images = dockerHub other registries:
-- python-web-app ,  nodeweb-app,  net-webapp 
   mylandmarktech/hello,  nginx,  mysql,  mongo  
   jenkins,  sonarqube, nexus            



KUB 3&4
 SET CONFIGURED NAMESPACE TO CURRENT NAMESPACE
originally, the default namespace is the 'current namspace'
*Mee*by default, kubectl get po' lists the pods in the configured ns, BUT you hv to run the 'kubectl config set-context' command, to set the ns you configured/
created to be the current ns


  LABELS & SELECTORS EXPLAINED
****SERVice makes pods accessible using labels & selectors

********************* for a service to deiscover any pod, its going to be using labels, so what is the label of the pod
so we v labelas n we also v selectors...  so ders a label that says app is mapped to hello, its under the pod, so we v Pod label, so 
under service, we ar goin to v sevice selectors
in my k8 cluster der wil b many sevices,  hw wil the service be able to identify this pod,, it nids to create a selector that matches this pod like that 

***online
No, a manifest file's label does not have to match the container name, but they are often related and must match for certain tools to function correctly. 
For example, in Kubernetes, a manifest's metadata.
labels are used by other resources like a Deployment's selector to identify which pods to manage, not to name the container itself. 
 


SERVICE DISCOVERY
ServiceDicovery:
==============
*********the first service type is:
ClusterIP is the default kubernetes service type that support
communication within the cluster.    *** Used for internal communication
************** to communicate within our cluster, we will use a clusterip service type



What is FQDN?               ..........          45:21   in kubernetes by default we create containers in the same namespace to enable them comm wit each oda, however, 
FQDN = Fully Qualified Domain name.                                                    
If one POD need to access service & which are in different names space we have to use the FQDN of the service.
***We cant use curl' to est 


**we cant est communication btw containers in diff name space using the service name  BUT we can if use the service name if the containers are in the same ns


     CANT USE THE SERVICE TO COMM BTW CONTAINERS IN DIFF NS 
             1) errror... the 'web pod' in the default namespace TRYING TO use the service name to COMM with the 'hello container in the dev namespace  
root@web:/usr/local/tomcat#  curl hellosvc
curl: (6) could not resolve host: hellosvc    ,,,  wheni use just the service name am unable bc they ar in diff namespaces, so u hv to use the FQDN when d 
containers ar running in  diff namespaces.
            

                       USING SERVICE NAME TO COMM WITH CONTAINERS INSIDE THE SAME NS
       2)successful:  the 'web pod' in the dev namespace TRYING TO use the service name to COMM with the 'hello container in the dev namespace  
container
root@web:/usr/local/tomcat#  exit              54:10  ####remember earlier we config our default namespace to be dev n now we didnt indicate default so its 
in dev lik hello
ubuntu@master:~$  kubectl exec -it web bash          **this is the 2nd web container we created in the dev namespace n using just the service name am able to comm
root@web:/usr/local/tomcat# curl hellosvc        ****now am accessing a container that are in the same namespace .. in the same namespace wit just d service name it wil work n 
successfully communicated                                                                 also with the FQDN it wil also work ..


 DISADVANTAGES OF POD   ... 55.50
********We deleted the 'web container' but it wasnt recreated bc when u use pod to deploy k8 objects, pods cannot be recreated and it cannot be scaled
e.g we cannot decide to have more replicas of a container eg 'hello' if we ar going to be using pod
in k8, the pod lifecycle is very short/has a defined life cycle so if we ar going to deploy appl, we shud nt use pod    .....
(we shud use controllers like Replica Sets, Deployment, Deamon sets to keep pod alive)
We should not create pods directly to deploy applications.
it has a defned life cycle becuase for eg; If a node  goes down in which pods are running, Pods will not be rescheduled.
We have to create pods using controllers which manages the POD life cycle.
controllerManagers:
  ReplicationControllers , ReplicaSets, 
  Deployments,  DaemonSets  



STATIC PODS***************

    Static Pods are controlled by the kubelet service  
If we delete a static pod, So long as the manifest file still exists, it will continue to recreate the pod, the kublete service wil restart the pod
To permanently delete a static pod you must have to delete the manifest file used to create the static pod



                                              WORKLOADS
A workload is an application running on Kubernetes consisting of a single 
component or several components that work together inside a set of pods. 
In Kubernetes, a Pod represents a set of running containers on your cluster.


                                   k8 POD LIFE CYCLE
Kubernetes pods have a defined lifecycle. 
For example, once a pod is running in your 
cluster and the node hosting the pod fails then pods running on the node
will fail. Kubernetes treats that level of failure as final. 
You would need to create a new Pod to recover,even if the node later becomes healthy.
****Therfore we nid to use controller mangers such that if this  node goes down a controler manger will ensure that this pod is rescheduled on another node 


USING MANIFEST FILE TO CREATE A REPLICATION controller, that wil be recreating a particular pod
                        includes manifest/template for ReplicaControllers, template for Pod, & template for service (Nodeport)



                **mee**              NODEPORT/ENDPOINTS/KUBEPROXY  (this is service discovery to take note of)
Nodeport service can process external traffic,
WE connect to d nodeport service from outside d cluster by using the nodeIP & the node port, SO frm outside the cluster we ar able to acces d app using nodeport
This is how it works: our nodes have ip address (nodeports) so when endusers type any of the ip address ie trying to access the appl, the nodeport service will
be routing service to either of the pods we have.
the kubeproxy we have in our cluster will immediately identify that this traffic is meant for a particular service n so it wil be routed 
immediately to the service concerned & frm the service, which  now has endpoints ie our pods/containers, so traffic gets to the endpoints 
***i bliv the endpoints are possible bc of the pod/container label which the service selector matches with.
1:39:44 ###################### # ##########READ UP
Kubernetes Objects
NodePort - Exposes the service on each Node's IP at a static port. A ClusterIP service, to which the NodePort service will route, is automatically created. 
You'll connect to the NodePort service, from outside the cluster, by using "<NodelP>:<NodePort".

2.09.19                                  NODE PORT COMES WITH CLUSTERIP 
***********nternally, i created a nodeport service for python but that nodeport  service also has a cluster IP assigned to it, so if i wnat to access my 
python appl, i can curl, using the clusterip ,to  access the appl internally



 HOW WE DEPLOY APPLICATION IN K8
########### hw we deploy applictaions in k8 , which is a very key aspect when it comes to my understnading of k8, 
 OBJECTS USED FOR DEPLOYMENT
kubernetes resources/objects used to deploy application include/RUN WORK LOads:
   Pod :
or    
 controllerManagers:
      Replication Controller, ReplicaSet, DaemonSet, StatefulSets, Deployment, Volume, Job       


      WHERE APPLICATIONS ARE DEPLOYED IN K8
where are applications deployed in k8?/??......     we shud be able to est the fact that applictaions/containers are running in pods   
so pods can be deployed/managed by using:
                                     
              HOW POD IS MANGED
pods can be deployed/managed by using:
 1. pods as a kubernetes objects  
 2. controllerManagers kubernetes objects , thers a list of k8 objects when it comes to controller managers, we hv seen the first controller manager k8 objects
   *** now we want to talk about replica sets.. 



    REPLICA SET
ReplicaSet = RS :   ...  ****************HOW TO USE REPLICA SET TO deploy our application    ......1:53:50
==========
What is difference b/w replicaset and replication controller?
RS is the next generation of replication controllers, **kub5** with replica set, we can roll out new version of the application but we cant rollback
The only difference as now is the selector support.

          ***types of selectors:
1)matchlabels which is Equality based:
key == value(Equal Condition)     ****me**use of labels
2) matchexpression which is set based:
  key in [ value1, value2, value3 ]   


      DIFF BTW RC & RS
RC
 Supports ONLY equality based selectors.   *mee** we saw the use of labels..
selector:
   matchLabels:   -# Equality Based    ***use of labels
    key: value
    app: javawebapp
    tier: fe    
    client: tesla
WHILE
RS -->  
Supports eqaulity based selectors and/or set based selectors. (it can be either one) ,(set based is that we can hv keys in multiple volumes)
 matchExpressions: -# Set Based  ,   under selectors: it can either be  matchlabels which is Equality conditions OR matchexpression which is set based where we can have
   - key: app              ********* we can hv key, operator, values , so 'app' alone can match to javawebapp,myapp and fe  ... .....1:56:50
     operator: in
     values:
     - javawebpp
     - myapp  
     - fe  


KUB 8&9                       WE have 2 classes of selectors
   ***Equality base & Selector based selectors
Replica set, daemon set and  deployment. has what we call both equality base.& Set base selectors .. 16.50
BUT Replica controller has only Equality based selector


    CREATE SECRET
Create a secret that will authenticate kubernetes to pull images from dockerHub/nexus/jfrog  



KUB5 .. Class 34


with deployment we can run kubectl undo deployment but we cant undo replicaset
to undo replica set, it will have to be destroyed before it can be recreated and we will see hw deployment resolves theta issue and that is why we use 
deployment as an object over replica set

Deploy an app which must hv a pod running in each node = we use: daemonSet
e.g of diff app that shud hv a pod in each node    e.g logmgt/ logshipper
deploy an appl with scaling option = we  use : rc/rs/deployment/statefulsets
***********we cant use pod for  deployment with scaling  capabilities.... although the appl runs inside the pod, we shud nt use pod directly to deploy appl.
we shud nt use pod as an object for deployment



DAEMON SET
daemonset is a controller manger that permit a pod to be scheduled on each node
we deployed/provisioned a cluster:
   nodes [ node1/node2/node3 ]  
 we can see the app pod we deployed above was scheduled on node1 and node9
#################so the scheduler here is scheduling based on resources


                USE CASES OF DAEMON SET
Explain the kubernetes objects recommended to ensure the scheduler schedule a pod  
in each node or a group of selected node/nodeGroup
- DaemonSets is the ONLY recommended object for pods to be scheduled in each node  
Use cases: 
   - logmgt applications - EFK/ELK    
   - databaseBackup application  

9 worker nodes: = 
    dbnode1, dbnode2, dbnode3, appnode1, appnode2, appnode3
    webnode1, webnode2, webnode13

nodeGroups : in this case, if you have a grp of dbnodes, appnodes, webnodes,, i can schedule pods to be created on only dbnodes grp by passing a node selector, to  say ok,
select only data base  node or a node affirmity for the specific/selected node i want pods to be created only on.
    3 dbnodes  [dbnode1, dbnode2, dbnode3  ]
    3 appnodes [appnode1, appnode2, appnode3]
    3 webnodes [webnode1, webnode2, webnode13]


TAINTING NODE
we can also taint a node when we want to do :
  -- recommissioning / upgrades / updates / patching  
kubectl taint nodes node1 key1=value1:NoSchedule     [taint the node]   .................  to taint a node
kubectl taint nodes node5 key2=value2:NoExecute      [taint the node]
kubectl taint nodes node1 key1=value1:NoSchedule-    [untaint the node] ..................  to untaint a node

Master node is tainted by default
ubuntu@master:~$ kubectl describe node master
under Annotations
taints :we see master node is tainted
Unscheduled: false        .. ********* false means with toration we can scedule pods on the node

                   APPLYING TAINT
ubuntu@master:~$ kubectl taint nodes node1 key1=value1:NoSchedule ...... am  tainting this node wit no schedule which means pod cannot be scheduled on the node  ...1:35:55

       ADD TOLORATION TO TAINTED NODE
for  a pod to be scheduled on the tainted node we nid to add tolerations in the spec:
- operator: Exists                       we v operators like exists, effect 
        effect: "NoSchedule"




KUB 6  . . class34
Deployments  
==========
Deployment is a kubernetes object. 
k8 objects are used to run workloads in k8
Deployment is the recommended kubernetes object for deploying applications, running workloads and managing/contolling pods in our environment.   
Deployment as an object ,deployment strategy: ReCreate strategy , RollingUpdate.  Deployment techniques , 
The default strategy is ROLLING UPDATE & it has no down time bc b4 it brings down a pod eg an old version, it makes sure the one it rolled up has started.

Advantages:
     Deploy/rollout a RS.
     Updates pods (PodTemplateSpec).
     Rollback to older Deployment versions.
     Scale Deployment up or down.
     Pause and resume the Deployment.
     Use the status of the Deployment to determine state of replicas.
     Clean up older RS that you dont need anymore.


     ****i think this is rolling update
e.g we create a deployment called webapp and say that replica is 2
with deployment as an object :2 rplicas of the appl wil b deployed &2pods for d webapp  &when an update is done ie verson2, the update is deployed wit a new
set of 2replicas and new pods but the previous replicas will be zero (0) bc the pods wil be 0 bt the replicas stil exists and the webapp for each version 
still exists, this is the same process for each version that is rolled out and so we can easily delelete or roll back to a previous set bc their replica sets
were recorded

 1.19.00     DISADVANTAGE OF ROLLING UPDATE
*with roling update we can decide to push out 10 replicas of the new version and keep 10 replicas of the old version to that xtent ders a kind of
trafic mgt and trafic is flowing to both version1 and version2 bt we cant deter who is goin to get version1 and who gets version2 like we ar able to det in
deployment technique anoda problem of rollin updtae is that even after completely deploying version2 if ders a problem, you still v to rollback

1.14.00
the issue with rollingupdate deployment, if you release an important appl like a medicapp for an hospital then you realise ders an error and need to rollback
but before you can rollback lives are gone so we dnt want to deploy using rolling update, we want to deploy using blue green..

*********testing in UAT doesnt mean that ther is no possibilty that der could still be a problem n we wud need to roll back.....1:14:30



USE IDES TO GENERATE FILES

#### i can use IDES to generate a file and just copy, then jst use what i nid v... integrated development env
*****strategy can be rolling update or recreate... 
rolling update is the default startegy  so if u dnt enter a strategy, it wil be uisng rolling update
also if w dnt define replica only 1 replica will be created


Deployment strategies:
====================== ..........................  ******************       25:33
ReCreate strategy  --- 
  It comes with downtime because the current application version is destroy entirely
  before creating the new version  

  eg; 8 pods of version1 running and you want to 
   deploy 8 pods of version2 , recreate will first destroy all the 8pods of the current version
therfore ther will be downtime


                         UPDATE DEPLOYMENT WITH SET IMAGE
Update Deployments  (mayb by rolling out a new version) ,,,,, .......................52:38
Introduction
    We can update deployments using two options
        Set Image   ...OR
        Edit Deployment(manifest file)



  1.08.30    DEPLOYMENT TECHNIQUES
we have blue green techniques & carnary technique

                                                                    2:21:21
DEPLOYMENT TECHNIQUE  ************************************(this is under using deployment not deploymnet as an object) ............1:08:44
Blue Green deployment Technique :    
   version1/blue running in production
   version2/green  Just 2 replicas deployed in testing env    ... 1.15.30  
   we deploy version2 in the test environment and observe its performance, 
   Once the performance is good then we deploy version2/green in production    
the problem with blue green deployment is that it makes use of lot of ressources bc we need to hv 2sets of deployment running.. each set is consuming 
resources

1.14.00
the issue with rollingupdate deployment, if you release an important appl like a medicapp for an hospital then you realise ders an error and need to rollback
but before you can rollback lives are gone so we dnt want to deploy using rolling update, we want to deploy using blue green...
so using blue green deploying we have version1 running in production , then we deploy version2 , so  version2 is green and its deployed in the testing/UAT 
env, which cud be in a diff name space(in k8 we use namespace to isolate clusters) and while its deployed (ders anoda service use To expose it), further test
is being done ,we are observing hw its performing, we can decide to test it on a client with less severe cases or some rabbit & by the time d testing is 
saticfactory, we need it to be running in production and in k8 we expose it using a service.the application already runing in prod is being accessed using 
d service its a nodePortSVC MEDSVC (selector app:medic) wich is routin traffic .the deployment runing in UAT has label=app:med
NOW for the service MEDSVC to  start routing traffic to the appl in UAT and stop routing traffic to the appl in deployment, i will need to change my selector
in the service from medic to med and immediately the appl in UAT is now being exposed in production and traffic is no longer routed to the previous appl  and they can be brought down...
thats how blue green works by switching the service over once we are sactisfied with testing. .... this  is not just a quick switch , this is an ultra fast 
switch.. so oncee we are satisfied, we then bring the old version down.
 1:19:46

                              CANARY  DEPLOYMENT TECHNIQUE
   Canary deployment Technique :  this gives you a longer time to test ur appl ... (class33 video7a....1:12:35)
   - TRAFFIC MGT   
   - 25% traffic VERSION2 goes to Canada  , and if we are sactisfied with how its running we can then switch all the traffic to to version2
   - 75% traffic VERSION1 goes to USA  
   - VERSION1 -- 40% to version1    ...e.g we can decide that we hv version1 runing we can decide that since we ar rolling out vesion2 let 40% of our traffic go to version1
   - VERSION2 -- 60% to version2              and  40% go to version2 and we observe for an extended period and if its sactisfactory we move all the traffic to version2
   - 40years+   75% traffic VERSION1    OR  we could use the cx age group (using canary setting u can set all that up,, der are some appl that ask for your date of birth etc
   - 18-39years 25% traffic VERSION2
   -we can alsouse based on biometrics e.g sex or ages



KUB 7a

we can use a vscode to help when it comes to the manifest file 
but we also hv a github repository that conatins almost all the manifest file tha you may need
prof is there any file you have written that i can go there and copy and use them even at work? ...YES
CLONE THIS REPOSITORY
https://github.com/LandmakTechnologies/Kubernetes-manifests    .... almost everytin that we v done so far u wil find it here, when ur trying to deploy appli
                                                    most of the tins that we v been able to cover in k8, u can see then here, like hw to deploy in mysql db 
                                                       with configmap and secret , all of that, al d manifest files that we wil stil be loking at u wil find 
                                                                      them there when it comes to k8

CREATING POD & MAKING USE OF REQUEST, RESOURCE & LIMIT
one tin i wanted us to look at .... creating a pod and making use of request, RESOURCE,and limit
when YOu want to create a resourec in k8 generally, we need to decide hw much resources is goin to be asssigned
when you assign resources to your pod for that pod to be scheduled on a node, the node must have the requested resources available.

**bt while d container is running, if d container need to optimize d resource d  maximum d conatiner can use is what has been defined as limit

after we deploy the file below 
kubectl describe , if u look at this pod, the pod has a imit as to hw much resources it can consume in a clutser
this are key aspect when it comes to auto scaling our pod , so if we wnat to deploy our apply, we can deploy it making use of REQ and limit as we v seen here


Resource, requests and limits:
--------------------------
Requests and limits are the mechanisms Kubernetes uses to control resources such as CPU and memory. 
Requests are what the container is guaranteed to get. 
If a container requests a resource, 
Kubernetes will only schedule it on a node that can give it that resource.

                                 WHAT IS LIMIT
Limits, on the other hand, make sure a container never goes above
a certain value. The container is only allowed to go up to the limit, and then it is restricted.

Resource Limit:  .. a container wil never go above a certain value
A limit is the maximum amount of resources that 
Kubernetes will allow the container to use.

Resource request:
---------------
A request is the amount of that resources that the system will guarantee for the container, and Kubernetes will use this value 
to decide on which node to place the pod. 


KUB 7b                             TYPES OF SCALING
 scaling in kubernetes: we have Manual scaling and automated Scaling

1) MANUAL SCALING
How do we scale manually in k8 If you deployin your application for you to scale manually, you can run the command:
kubectl scale deployment/rs/rc/sts
ie we can scale rs , deployment, rc, stateful set, ...... these ar the scalable objs in k8 and these scaling can be done maually
we can also scale automatically, in k8 using another object Callled HPA .
 eg manual scaling:
      kubectl scale deployment/rs/rc/sts/ app --replicas 4   

2) AUTO SCALING : Automatically scaling the replicas, it states the minumum& maximum number of replicas


                      TYPES OF AUTOSCALING
TYPES OF AUTOSCALING:   ... WE ILL LOOK AT the rest of the autoscaling as far as k8 is concerned
we can use one of these to automate scaling..
we need the guage bc we hv traffic coming into our cluster, and we hava a HPA, which we want that pod to automate the traffic for us. so we are saying 
1) Horizontal Pod AutoScaling  :                                                               KUB 8&9, i bliv this is the metrix pod
2)Vertical Pod AutoScaling : 
3)Cluster AutoScaling:

1) HORIZONTAL POD AUTOSCALING - HPA
POD AutoScaling -->  **meee** this will hv its own manifest file, * its selector will select the 'podtemplate label which is deploying the applica cluster**
Kuberenets POD AutoScaling Will make sure u have minimum number pod replicas available at any time & based on the observed CPU/Memory 
utilization on pods, it can scale PODS automatically.
HPA Will Scale up/down pod replicas of Deployment/ReplicaSet/ReplicationController 
based on observerd CPU & Memory utilization base the target specified. 
   automated scaling:  
Horizontal Pod AutoScaling  - HPA 
   
***mee** with HPA, we are able to auto scale the number of pod replicas requested by for by the k8 objects eg when we use deployment strategy 
it is based on the resource consumed by each pods
eg; if my replica is 5, hw much resources is each of d replica suppose to consume? for memoery 128mi and cpu:500m, thi sis hw much resources a 
pod can consume, however if each of the pods are all consuming all the 128mi, then its means that more rsource needs to be allocated, thefore bc the 
vol of resources being used by the pod is much so it will add one pod :+1, it will keep ading and the process wil be automated  .. 12.06




                     ***meee*** STEPS TO DEPLOY THE APPLICATION/POD & the HPA  or KUB8&9 HOW CAN SCALING BE AUTOMATED
or (cluster autoscaler ie an automated application cluster where by the replicas are created automatically by the HPA) 
                                      
1) install a metrix server... kub8&9 By installing a guage API [: metric-server ] in our cluster and using 
 KUB 8&9.. so we hv a matrix server service and its also deployed as a pod. the pod is able to check hw much resource is been used here
***ONLINE**
the Kubernetes Metrics Server is not a Horizontal Pod Autoscaler (HPA). Instead, the Metrics Server is a component that
provides the necessary data for the Horizontal Pod Autoscaler to function.
2) deploy the application cluster:  ***mee*** includes 3 manifest files
A) the manifest file using deployment strategy to deploy the 'application' & object that states/makes the request for the resources needed for the cluster.
B) the second manifest file is to deploy the HPA **to ensure the resources requested by the objects for d application are maintained at all time. 
     I bliv each application will be deployed will its own autoscaler (HPA/VPA/CAS)manifest file
C)we are creating a service for our cluster autoscaler  **mee** ie the cluster we deployed in (A)the first manifest file using deployment strategy
       I bliv we can use the word 'cluster autoscaler' bc we are autoscaling the cluster
3) Generate load
***i think at work, we wnt nid to generate workload, bt rather wait for the application/cluster to gradually use up the resources which
                              will then trigger the HPA to auto scale.
   ***I Bliv we had to generate a workload here because we need to confirm that the HPA is working, so thats why we used the 'customized image' bc its 
customizedso taht when we run the workload command, it generate the workload... i think if we use any other image & run the workload command, the HPA will
not function bc the image doesnt hv the content to generate the workload needed for the HPA to work ie autoscale 



KUB 8&9

IN DOCKER 6 OR 7
WE Saw that with endpoints, we can use CMD to add instruction in a docker file to determine what we want to display evarytime it restarts 
                                   

                                    EXAMPLES OF SERVICE DISCOVERY

1)CLUSTER IP  2)  noodeport  service  3)loadbalancer  service  4)External name  
5)headless service

these Are the different services in communities.



                                           DERVICE DISCOVERY & ENDPOINTS
So with these, we can use any of these  to discover our application.
Therefore, we saw again, this diagram, where, with the help of a service, am I able to access, you know,access some  pods that are deployed in  my cluster.
Look at this diagram,  Look at this service. I have a cluster I-P service here okay now Let's.
Assume this my web service for my web application is able to route traffic and load balance traffic to all of this pods, to the group of pods so this 
service acts as a single DNS resolver.(domain name service) .. 8:00
What that means is that, now, if I want to access this pod, I want to call, okay, like, this pod will have, like, maybe Web port one.
Did you see that? Web port one?Now observe something This web pod one.,This pod two.Okay. Now if you want to access all of this pod, web, pod three.,web pod4
Now do I need to be calling Web for one Web, web port2,No, I-I don't have to do that so I can use one name to call all of this pod,
so which Name would i will use, the name of the service.So, because my servicename is, is constant.So DNS domain name is resolver.So  it, it is going to achieve 
domain name resolution.Okay, to all of the set of pods that are linked to this service now.So how do we call this pods in  k8 that are linked to a service?
     #####We call them What END POINTS? Okay, so this service has How many end point? How many end points? How many end points are there now? They are four
and point.
Now, for my scale, if need be can I scale?  if there is need to scale, am I able to scale to have maybe more pod, Can the number of end point increase?The 
answer is yes.The number of end point, okay, can increase.and once it increases
The same service is going to be achieving DNS resolution. The same service because that's what services are all about It's good to be achieving DNS resolution,.

If need be? Can we have more end point?, yes, if need be.
We can scale to how many replicas , right now 6
Okay. So one service, one service is routing traffic, to how many pods of the application six pods at the moment .  thats one service.
This is what is going to be happening. So this is the feature of k8, as we have seen, a cluster, IP noteport, load balancer, external name, headless service.
And we are going to look at the ingress  as well as part of the options we can use for service discovery.  11:36


          API VERSION
API version for pod  is V1.
What is the API version for replication? Controller? V1.
What is the API version for replica? Sets.. apps/V1.
What is it API version for daemon set, app/V1,   for deployments, app/V1.


    HOW CAN SCALING BE AUTOMATED?
     WE need a type of guauge in this case ; metric server
By installing a guage API [: metric-server ] in our cluster and using 
*we can use one of these to automate scaling..
we need the guage bc we hv traffic coming into our cluster, and we hava a HPA, which we want that pod to automate the traffic for us. so we are saying 
that :HPA
min2/max50    ....ie replicas....  this is what we ar defing for our HPA , we ar also sayin that our target resource can be a deployment,RS/RC/STS
target resource 
deployment RS/RC/STS      ,, it could be any of these   .... now what is my target ultilization?
target ultilization  ... now when theHPA react? .. the HPA wil react when CPU ultilization or memUltilistaion is greater than 70% ,& thats when it wil scale
cpu Utilisation>70
memUltilisation >70


  HOW HPA WORKS
so our HPA is targeting one of d resource for e.g we use deployment & d deployment name is myapp,the deployment is suppose to control a set of pods &the 
deployment(myapp) is using a selector (match labels, if it is equality based) to know the set of pods that its suppose to control.,  to know the pods we ar
managing.. we wnt to automate it,, e.g we hv node1 and node9  and based on our replica:2  we hv mayapp1 on node1 and myapp2 on node9 so in our cluster we  
hv deployed another object in the cluster,which is called a metrix server whic is runin as a pod ,,so we hv a matrix server service and its also deployed as
a pod. the pod  is able to check hw much resource is been used here.. e.g it can check n tell us we ar using 10% memory, etc and when it gets TO  70%, ie if
ders a spike & we hv to process abt e.g 40m request, my app1 & myapp2 are receiving 20m reQ each due to Load been balanced automatically in k8, bt the 20m 
REQ is making use of up to abt 70% usage, our CPU usage exceeds 70% avg for all our pods,
our HPA will be triggered and one more pod will be automatically created,, this is an automatic addition and once the pod is created, more
traffic will be assigned to the pod.

34.18              

                             CLUSTER  AUTOSCALER 
********if we are now processing 60m REQ, ie our nodes are at their max carryin capacity, but inside our cluster we hv deployed/install CAS 
n in the CAS we ar sayin that athe max number of nodes is min2/max40 .CAS is scaling the number of nodes. now when ar we goin to scale the node or when shud
the node be scaled??? for us to knw the proportion of our CPU that is been used, k8 wil use matrix server to check the vol of cpu for my node
 CAS/min2/max40 
target ultilization  ... 
cpu Utilisation>70
memUltilisation >70
so when our HPA wants the scheduler To scheedule more nodes bt all our nodes has insufficient resources so CAS wil be triggerred and it will add more node
to the cluster,& once the node is added new pods can be scheduled. this process is automatic
now the CAS is runin in our cluster n its runing as a pod and we hv the server which wil be checking hw much resources is our node consuming, 
our pod consuming, thats how it goes
... 37:25


                  HOW WIL VERTICAL pod autoscaler work.//..... 
Let assume we want to deploy another application & we want to use the VPA to manage it
VPA resources:  under resources, we have request, how much resource can a pod requet
request:
cpu=500m  miliquo
mem=128Mi   miliquo
limits:cpu 900m
memory:512Mi
 based on the VPA, when a pod is created, this is the resource (the resources listed above) that is going to be assigned initially/the first time. resources
assigned to the pod wil be based on what is reQuested.assumin that the pod is caryin out some load,load is being assigned to the pod, at 500m it can process
a max of abt 20m requests bt if the number of rEQ is increasing,its goin to scale vertically ie increasing/growing upwards in size. the max cpu can increase
to is 900m , so again, more req can be processed to a max point of when cpu grows to 900m & it processin abt 40m REQ therfore, the same pod that was using 
cpu of 500m & processing 20m REQ has grown vertically to 900m and processing 40m REQ..  ... 43:00



    WE ARE DEPLOYING OUR legacy application
https://github.com/LandmakTechnology/spring-boot-docker         *** this application has a github repository, the application is called spring boot docker
-# Spring App & Mongod DB as POD without volumes         *** we are deploying a springboot application & a database pod
we will be deploying: a Stateless & a stateful application
1)Stateless applications deployment doesn't require to maintain it state   :   ***i bliv this is the main application itself
  Use deployment as the choice kubernetes object  
  ReplicaSets/ReplicationController:  
2)Stateful applications maintained their state. Examples are  :
  databases, Jenkins, where we create jenkins job , we hv to maintain its state
but generally databases are stateful applications
     use StatefulSets as the choice kubernetes object for Stateful application deployments   


                        WE WANT TO DEPLOY THIS MONGO DB  ***mee** with DATABASE & SERVICE
Why is required for this deployment, if you look at the database configuration & the appliaction,
if we decide to deploy this application some objects ar goin to be affected  ..... 49:01 
we wnt to deploy our stateless appli called springboot mongo n we also  deploy our stateful application wich is a mongo db, 
so we ar goin to create 2files:
      *mee** one file for the application & one for the database & we wil deploy each of the files seperately with their own service manifest file
1)Stateless  
--- springapp.yml/            ***meee** this is the main application
2)Stateful 
--- mongo.yml/                   ** this is the database


1. image: mongo         ******this is maintained by the mongo community, in docker hub, we can check for the image inf to get the env variable  1:16:30, just type mongo
2. image: mylandmarktech/spring-boot-mongo               ********this is maintained by us

repository to clone:
1. https://github.com/LandmakTechnology/kubernetes-notes
2. https://github.com/LandmakTechnology/kops-k8s
3. https://github.com/LandmakTechnology/kubernetes-manifests

######this is a springboot application that requires a database, it has has been developed by our developers usin the springboot framewrk
this was not hardcoded, our developers left the hostname, username,passwd as a variable, meaning that if i am deployin in the dev, uat, prod env, i can use
diff values based on my project requirement....   .. 53:15


*********the default deployment strategy in k8 is rolling update
now since am using the default strategy,i wnt put a value for strategy bc the default wil be used ie rolling update



                STEPS TO DEPLOY THE SPRING APP , THE MONGO DB & THEIR SERVICE

so we ar goin to create 2files:
      *mee** one file for the application & one for the database & we wil deploy each of the files seperately with their own service manifest file
1) We create & deploy the springapp manifest file & a nodeport service for the application
    ***we need a nodport service so that: 
 we can hv external users and their traffic can get into the cluster either thru any of d worker nodes or the master node
   bc we v kubeproxy running ,the trafic doesnt leave d node n talk to d pods directly, it goes tru the service n once it recieves d msg it does dns 
         resolution, it routes the traffic n it lb to all d end points .. as seen below:

    *** I didnt put the spring application manifest file here 

                     NODEPORT SERVICE MANIFEST FILE FOR THE APPLICATION
apiVersion: v1
kind: Service
metadata:
  name: springappsvc                               1:22:30
spec:                         for us to be abble to acces athe  springapp pod, 
  type: NodePort      ,, we created  a service , nodeport service:nodeportsvc and the service name is called springappsvc n the service has a label; app:springapp
  selector:             since its a nodeportservice, we can hv external users and their traffic can get into the cluster either thru any of d worker nodes or the master node
    app: springapp       bc we v kubeproxy running ,the trafic doesnt leave d node n talk to d pods directly, it goes tru the service n once it recieves d msg it does dns 
  ports:                          resolution, it routes the traffic n it lb to all d end points.. now dis appl nids to write in the db so we ar deployin a db pod n 4 d 
  - port: 80                          comm to b est they wil communicate tru a service, dis  is internal comm so we nid clusterip service, so we create a mongosvc;app:mongo
    targetPort: 8080               so for springapp to talk to d db it wil be resolved via the service name of the db  ... 1:26:40



                                GET END POINTS
ubuntu@master:~$  kubectl get ep
NAME                    ENDPOINTS                      AGE 
springappsvc             10.36.0.1:8080,10.44.0.3:8080   67s

   ***************if i wnt ot communicate wit the pod, i can use the name of the sevrice: springappsvc or the service ip: 10.100.100.234 
such that if i curl, 
ubuntu@master:~$ curl 10.100.100.234       ..... i successfully get a response 
so the service is achieving dns resolution  ,,....... 1:07:50

we can access the appl externally since we ar using node port , 
ubuntu@master:~$ kubectl get svc 
NAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORTS    AGE
springappsvc   NodePort    10.100.100.234     none        80:32136/TCP 

*****all we nid to use is one of our node even the master node in this case 
ubuntu@master:~$ curl ifconfig.me
3.36.7561ubuntu@master:~$
ubuntu@master:~$
ubuntu@master:~$ curl 3.16.75.61:32136    ... or  in google 3.16.75.61:32136 , w ecan access the application
  ###we can see this appl is a springbootapp thet needs the mondodb  databse and we can automate it using jenkins,docker & k8 , and we are using k8 now, 
we already used docker to containerize
***** right now our database has nt been deployed n so ders no database that the appl is able to comm with.


2) We create & deploy the mongo db manifest file & a clusterip service for the application
   now dis appl nids to write in the db so we ar deployin a db pod & for the communication to be established they wil communicate tru
a service, dis  is internal comm so we nid clusterip service, so we create a mongosvc;app:mongo
      so for springapp to talk to the mongo db it wil be resolved via the service name of the db  ... 1:26:40


*********8*************now we deploy our database,      *** I didnt put the mongo database manifest file here 


   ---                           CLUSTER MANIFSET FILE FOR THE MONGO DB
kind: Service   
apiVersion:  v1  
metadata:  
   name: mongosvc 
spec:                 the default service type in k8 is clusterip so even if i dnt write it, the clusterip type will be created. type:clusterip
  selector:
    app: mongo    
  ports:
  - port: 27017 
    targetPort: 27017  


to deploy the db      .........  1:10:10
ubuntu@master:~$ vi spring/mongo.yml n paste the  db manifest file
ubuntu@master:~$ kubectl apply -f spring/
replicaset.apps/mongors created
service/mongosvc created            ****** we see the service created 
deployment.apps/springapp unchanged
service/springappsvc unchanged
ubuntu@master:~$
ubuntu@master:~$ kubetcl get all 
we can see mongosvc is running  ...... n when we check in google we can see its running, so we are able to deploy our stateful appl n stateless appli
so this process can be automated
if we run kubectl delete pod mongors-966hc                 ,......1:33:40
we can see a new pod is being created automatically.. n this is permitted bc it has a controller manger which is our mongors but if u run: kubectl delete rs mongors, db wnt
although it was automTICALLY CREATED, we cant get back data that was previously created                       be recreated bc D replication controller MANAGer managing the.
for us to be able to capture data we hv to look at volume concept in k8                                      db pod has been deleted  ... 1:42:50
                                                                ***  bt goin further in d video he kefpt deletin the rs each time he had to deploy a manifest 
                                                                                                   file  like 


WE ARE GOING TO BE LOOKING AT VOLUME CONCEPT IN KUBERNETES

IMP REPOSITORIES TO CLONE
repository to clone:
1. https://github.com/LandmakTechnology/kubernetes-notes
2. https://github.com/LandmakTechnology/kops-k8s
3. https://github.com/LandmakTechnology/kubernetes-manifests


              CREATING HOSTPATH VOLUMES .................
 hostPath volume will store data ONLY on the node where the POD is scheduled ..
ie inf entered while d db pod was on d node cant be transfered to anoda node if d db is recreated on anoda node
  This can result data lost and data inconsistency      
EG db was created on node1 but node1 was later tainted & so db needs to be recreated on node3, if this happens, data on node1 cant be tarnsferred to node3
i bliv be the mount point was created on node1.

****IF U ARE TRYING to deploy a stateless (i bliv he meant stateful) appl we hv to mk use of k8 volumes  ...  1:3540
since we are deployin a database we nid to ensure that a piece of storage is created..
we ar goin to see hw to use vol for example:   ..
and the only deployment that will be affected is our mongors , **mee** ie the database ... so we wil deploy it agin this time around With volume

**  under spec, we add volumes
      volumes:        *********vol is a list so we can v more than one vol e.g we can v, name: vol2 ,hostpath:  and path can jst be /data n also multiple mount point
      - name: mongodbhostvol    
        hostPath:
          path: /mongodata        *****if the db pod was created on node9, a mountpoint will be created on node9 and it wil be called /mongodata
      containers:


                   SPRINGMONGO APPLICATION , MONGODB, VOLUMES & NFS
DEPLOYING OUR LEGacy AAPLICATION : Springmongo and mongodb and volumes , NFS ... ............  persistent vol and claim policy
without nfs :*if the db pod(ie the database container) was created on node9, a mountpoint will be created on node9 and it wil be called /mongodata
in the file in dockerhub we see data is stored in /data/db therfore in our db container, data will be stored /data/db ,ie the mountpath in our manifest file
if we introduce any of these, then we are goin to create an nfs server &ther wil be a mount poinT on d node& nfs server then data is captured in both the db
container and the nfs server such that evEN if the container is recreated on another node, once the container syncronizes with the nfs server all the data 
that was captured in the Nfs server wil reflect in the container and this wil result in data consistency


   DEPLOYING OUR VOL Uing NFS
######## ******************    2:01:20  ..   netwoRK FILE SYSTEM OR EFS

NFS    
if we introduce any of these, then we are goin to create an nfs server & ther wil be a mount poinT on d node& nfs server then data is captured in both the db
container and the nfs such that evEN if the container is recreated on another node, once the container syncronizes with the nfs server all the data that was
captured in the Nfs server wil reflect in the container and this wil result in data consistency

  volumes:
      - name: mongodbhostvol    
        hostPath:
          path: /mongodata 
      volumes:
      - name: mongodbhostvol    
        nfs:
          path: /mongodata 

when it comes to VOLUME CONCEPT, ther are several vol options we can look at like:
awsElasticBlockStore, azureDisk, azureFile, configMap, emptyDir g, cePersistentDisk, gitRepo (deprecated) 
hostPath, nfs, persistentVolumeClaim, secret


                         Configuration of NFS Server ( WE CAN USE ANSIBLE TO AUTOMATE THE PROCESS)
  NFS = Network file system 
        distributed file system 
        shared file system 

A) Step1 TO 3
Step 1: Install NFS Kernel Server
Before installing the NFS Kernel server, 
  we need to update our systems repository index with that of the Internet 
  through the following apt command as sudo:
B) 

          HOW TO USE NFS SEERVER FOR OUR DEPLOYMENT
-- the only thing that wil change in this manifest file compared to the hostpath file is the nfs server address and remove hostpath cos d vol type we are 
using is nfs, then everytin remains the same

with this nfs deploymnet we dnt have issues with data loss / inconsistency .............2:33

############WE CAN USE EFS instead of NFS, but we used NFS without any issues here




           Persistent volumes             ***we wil use retain claim policy most of the time
===================
this are simply a piece of storage in your cluster
 the ideal appraoch to store data in k8 is by using persiastent vol by it is managd by the kube service
just like docker vol is manged by docker and thats why we use persistent vol in docker as well


*** Based on someones question: the persistence vol could either be host path, NFS and other vol options 
with persistent vol, the vol itself is managed by the kube service
the nfs is kind of storage option that we can use to store data like we saw hostpath wher data wil only be hosted by one particular node in the clusted bt
in the nfs data is distributed to all the nodes and taht leads to data constitency,, however if u create a nfs and it is nt created under a persistent vol, 
bc it is the recommended approach to store data in k8 bc when u create a persistent vol it is manged by the kube service , therefore the vol is independent 
of the livecycle of the pod that is consuming the vol so if the pod goes down we can stil retieve the data


deploy a kubernetes using Kops  


PV --> It's a piece of storage (hostPath, nfs,ebs,azurefile,azuredisk) in k8s cluster. 
PV exists independently from from pod life cycle form which it is consuming.

PersistentVolumeClaim -->
   It's request for storage(Volume).Using PVC we can request(Specify) 
   how much storage u need and with what access mode u need.

Persistent Volumes are provisioned in two ways, Statically or Dynamically.:  ... 2:35:40  ... power point script

1) Static Volumes (Manual Provisionging)
    A k8's Administrator can create a PV manually so that pv's can be available for PODS which requires.
    Create a PVC so that PVC will be attached PV. We can use PVC with PODS to get an access to PV. 

2) Dynamic Volumes (Dynamic Provisioning)
     It's possible to have k8s provision(Create) volumes(PV) as required. 
     Provided we have configured A storageClass [sc].
     So when we create PVC if PV is not available Storage Class will Create PV dynamically.        

PVC: If pod requires access to storage(PV),it will get an access using PVC.
     PVC will be attached to PV.

PersistentVolume  the low level representation of a storage volume.
PersistentVolumeClaim  the binding between a Pod and PersistentVolume.
Pod  a running container that will consume a PersistentVolume.
StorageClass  allows for dynamic provisioning of PersistentVolumes.

PV Will have Access Modes:
============================
ReadWriteOnce=RWO  the volume can be mounted as read-write by a single node = EBS 
ReadOnlyMany=ROM   the volume can be mounted read-only by many nodes
ReadWriteMany=RWM  the volume can be mounted as read-write by many nodes = NFS 


Claim Policies
================
A Persistent Volume Claim can have several different claim policies associated with it including
RetainWhenthe claim(PVC) is deleted, the volume(PV) will exists.
RecycleWhen the claim is deleted the volume remains but in a state where the data can be manually recovered.
DeleteThe persistent volume is deleted when the claim is deleted.

The claim policy (associated at the PV and not the PVC) is responsible for what happens to the data when the claim is deleted.

2:37:13
**********************************CREATING THE DATABASE POD USING PERSISTENT VOL 
inside my cluster, i wil create persistent vol, now my db nids this vol n the vol can only be assigned using persistent vol claim ,,, PVC, with the PVC we 
are able to mount we ar able to create a mount point btw the container n the persistent vol, the PVC permits the container to claim the persistent vol
*************with this system ders no data loss if db container goes down bc persistent vol is anoda k8 object whic is independent of my pod lifecycle
    ***we wil use retain claim policy most of the time


 KUBERNETES VOLUME CONCEPT FOR PVC
In k8, we can create volumes with or without PVC    ***for it to be persistent, its adviced to use PVC
1)   HOSTPATH
 a) Without PVC; the hostpath created is like a mount point just like in 'DOCKER bind mount', we hv to create a mount point & its not persistent
            STEPS TO CREATING HOSTPATH VOL WITHOUT PVC MANIFEST FILE 
 Creating hostpath vol for database..  the manifest file for mongo db with hostpath, will have: under spec
      i)we add volumes & 
     ii)path: /mongodata  *****if the db pod was created on node9, a mountpoint will be created on node9 and it wil be called /mongodata
          
 b)With PVC; the hostpath created is the PVC,  its mg8 by kubernetes so its like 'Docker vol' we created in docker , its persistent bc its mg8 by docker 
               **8 check main file For the 4  manifest file
2)    NFS OR EFS  
     a) Without PVC; the NFS/EFS created is like  external vol 'EBS' we created in Doker, its persistent but i think not mg8 by k8
            STEPS TO CREATING NFS/EFS VOL WITHOUT PVC MANIFEST FILE 
      Under spec,we add NFS 
     the only thing that wil change in this manifest file compared to the hostpath file is the nfs server address and remove hostpath cos d vol type we are 
      using is nfs, then everytin remains the same
       nfs:
          server: 10.0.0.76          ********using the private ip bc they ar in the same vpc ie both my nfs server & k8 nodes
          path: /mnt/share
      b) with PVC; the NFS/EFS created is mg8 by kubernetes so its like 'Docker vol' we created in docker , its persistent bc its mg8 by docker 

                  4 MANIFEST FILES FOR HOSTPATH WITH PVC  **PERSISTENT VOL, this is manual provisioning, we can automate the process with Ansible
   Creating a host path with persistent vol
  a) manifest file to create the persistent vol
  b) manifest file to create the mongo db with PV. .. persistent vol
  c) manifest file to create the persistent claim policy
  d) manifest file to create the mongodb with the  persistent claim policy



KUB 10&11

we also realize that why we v dif option when it cm to vol der are some other imp aspect to consider like:
diff vol option:
secret
nfs
hostpath
azuredisk
configmaps

env variables are  like secret variable ,, because the inf is stored in our source code mgt and so we shouldnt pass the env variable like this 
 env:
           - name: MONGO_DB_HOSTNAME
             value: mongosvc
           - name: MONGO_DB_USERNAME
             value: devdb
           - name: MONGO_DB_PASSWORD
             value: devdb@123


WHERE WE STORE CONFIG MAP & SECRET
 where do we store the configmap and screts
ans; 
we store them in our local env nt in the scm


CONFIG MAP
######  HOW CAN WE DEPLOY THE ENV variable/file in a more secured way

kubernetes volumes :

======== by using:
Config Maps & Secrets
======================
We can create ConfigMap & Secretes in the Cluster using command or also using yml.
********************ConfigMaps:
  are used to passed non confidential information in key:value  pair that weren't 
  hardcoded/included in the Dockerfiles/code by Developers
   E.G we can use config map to pass stuff like 
    HOSTNAME
    USERNAME 
8.25 ...also if developers are required nt to hardcode, we are then required to use config maps to pass/verify to the variables

e.g we are tryin to deloy an appl and we are using a kind of a tomcat image  n in that tomcat image hw do we access n manage tomcat
we hv a file called tomcat users.xml, so  we could create a config map for our tomat users


   OPTIONS FOR CREATING  CONFIGMAP:
1)Create ConfigMap Using Command:
  kubectl create configmap springappconfig --from-literal=db-username=devdb 
  kubectl create configmap springappconfig --from-literal=db-hostname=mongosvc 

2)
Create ConfigMap Using files/declarative approach

11.43 
eg we are trying to deploy this application, as part of this deployment, we hv env variables
i can decide to create a config map for stuff like hostnamename, username, bc config map is used to pass non confidential values in key value pair
& we could create it in this manner, creating config map using commands... 
17.53 .. but its not recommended to use config map to create PW, rather we use secret
should we use imperative or declarative approach to create config maps? we shud use files/declarative approach



    SECRETS
****************************Secrets:
  are used to passed confidential information in base64 format e.g 
    PASSWORD
    SSH_PRIVATE_KEYS  
    dockerHub LOGIN password  
    tls certificate  


  OPTIONS FOR CREATING SECRET
***me
   1)Secret Using Command: 
kubectl create secret generic springappsecret --from-literal=mongodbpassword=devdb@123 

2) Create Secret Using files/declarative approach



 APPLICATION DEPLOYMENT
if we are going to deploy an application what is involved????
LETS ASSUME the we are deploying the database as a stateful set , database stateful set or as we saw as a replica set
what else are we deploying as part of the database, we nid a storageclass, wich we wil be sseeing verysoon , thats part of vol, also persistent vol, pvc , 
this is one appl we are deploying, we cud nid configmaps and secret ,  so one deployment requires all of this.. we'l also nid to deploy a service to mk this
database discoverable

Stateful applications mongodb :
  - statefulsets/RS  
  - storageClass 
  - PersistentVolume 
  - PersistentVolumeClaim
  - configMaps 
  - Secrets   
  - service = ClusterIP   
stateless applications:  
  - deployment       .....recommended obj is deployment 
  - configMaps 
  - Secrets   
  - service - most of the time this could be NodePort / LoadBalancer     .......35:40

#######pls take note this is  very imp this is a key aspect  to underline and thats one thing that we are covering to mk sure u have a gud understanding when 
it cm to configmap n secrets





  ***mee***            DIFF BTW KOPS & EKS
1) Both of them dont have a master node
  a)we did nt nid to sh into the master node bc we created a kops sever bt tk note that the kops server is nt our k8 master node, so frm the kops server
we cud jst remotely mange our kops cl8
 b)eks automatically creates & deploys the control pane & we can use a worker node as the control pane to mg8 the cluster
ANY SERVER BC WHEN U CREATE an eks cl8 u dnt v access to d master node, the master node is exclusively managed by eks which means that u can only mange ur eks cl8 by using
a remote node n call it our eks control, wich is jst used to control the eks cl8, so it cud be ubuntu node, whatever node it is, can be used.. dats what we did
so u can jst create a new node for that purpose n use that node to manage ur eks cl8 




  QUESTIONS

                 KOPS SERVER DIFF FROM MASTER NODE
1)FROM THE CL8 WE created using kobs i unsderstood it but, i knw we shh tru d ec2 insatnce, the server wher wecarried out the whole operation is der a way we 
can ssh into the master node using mobaxterm or vscode remote host
****ANS: we did nt nid to sh into the master node bc we created a kops sever bt tk note that the kops server is nt our k8 master node, so frm the kops server
we cud jst remotely mange our kops cl8, we did nt nid to ssh into the kops bt if we had to do that then we can stil do that

 SSH KEY & SECRET
student : cos i knw we did nt create a key pair dasts why am asking, hw do we then do it ?
ANS: WE did nt nid to create a key pair bt we craeted sssh keys while deploying our kops command , when we ran the ssh gen key command and we had exported that
key into the kops as a secret n we can use the key.



  ***************** INSTALL & DEPLOY A K8 CLUSTER USING KOBS    ...... 40:50  ,, read  the  file in github account

                  **I noticed that kops doesnt have any commands perculiar to kops...


       KOBS IS A SOFTWARE used to deploy a production ready k8 cluster
here is our github account wich we'l be using to create a kobs cluster
kobs is iaac eg terraform


.kops is a software use to create production ready k8s cluster in a cloud provider like AWS.

kOPS SUPPORTS MULTIPLE CLOUD PROVIDERS

Kops compete with managed kubernestes services like EKS, AKS and GKE

Kops is cheaper than the others.

Kops create production ready K8S.

KOPS create resources like: LoadBalancers, ASG, Launch Configuration, woker node Master node (CONTROL PLANE.

KOPS is IaaC


kops:
Ticket07: Install a kops cluster using company documentation
          https://github.com/LandmakTechnology/kops-k8s


installation step                           
7) ssh-keygen      ............  57:27


59.30                             EVERYTHING THAT KOPS CREATED
It created all of these objects listed here
the rule for kobs is that whatever u are craeting must end with .k8s.local
we can scroll down to see everything/objects kobs will create , including a vpc , autoscaling grp fro master n worker node, lb for the api server,even some
aws resources, vpc, subnet route gatway, evrytin wil be created

**********is this similar to what we saw in terrafrom as terraform plan???  in terraform when you do terrafrom plan it wil tell u evrtin tht WIL BE Created
####copy the sshkey so that we can ssh to our master node  ....1:01:39


1:09:21 
1:09:21 
when we use kops, ITS a software
we are running ;kops create cluster when we execute what does it do?
kops create cluster : since our target is aws cloud, it gets into the aws cloud and in the aws cloud this command execute/achieves , it creates a series of 
resources e.g vpc n subnets, re.g subnet1& subnet2 , LB, autoscaling grp managing some worker noder, masternode, we didnt need to create server manually.
look at what kobs is doing
is this what were able to do with terraform? were we able to use terraform to create resourcesin  AWS? with terraform we cud create resources in a cloud provide


EXPORT KUBECONFIG FILE
10b)   1:14:55                                   **lets export the kubeconfig file, we want to mange the file as the admin user
   kops export kubecfg $NAME --admin            once we run this command, it exports the kubectl context to the s3 bucket we created earlier, 
kubectl get node  ,,  we get a response                      now we see some nodes hv been deployed by the kobs software



**** so we hv deployed our kops cluster 

how many clusters do we hv now?
now we hv kubadbm **(i bliv the previous one when we first install multi node cluster & ran sudo kubeadm init) and kops cluster, we v deployed a self manged
cluster using kops
1:18:05 *********lets go to our github and deploy a particular application



*** after we deployed the legacy application  ... he ran several commands including the one below..

kops@master:~$ kubectl get svc
we can see it created a lb in aws 
when we used kubeabdm, when we did a similar task using kubeabdm it wasnt able to create a lb in aws, it is stil show pending, waitin for external ip......... 1:26:30
*********  he put the appl ip in google n we are able to write data

the lb that has been created is manged nt self managed bc lb created in aws is manged by aws

managed or self managed  LB 
if i create a self mangaed lb, wit the self managed LB, i may create a server n in the server, i wil install a software like ngnix afterwich i wil deter hw 
service is routed
self managed =
   NGINX   




 CLUSTER COMPONENT
scheduler, etcd, controllerManagers, kube-proxy, kubectl, kubelet,
  container-D, Kubernetes-cni[weave, flannel], kubectl-csi, apiServer   


SCHEDULER / etcd

scheduler for scheduling pods on nodes  , this is the function of the scheduler n if for some reasn the sscheduler is nt running we wont be able to deploy any application
we can also check the sheduler pod :
ubuntu@master:~$ kubectl get pod -n kube-system    :  .... the kubectl has very imp pod that is used to run the cluster
NAME            READY       STATUS          RESTARTS             AGE
etcd-master      1/1      running            ....... we can see the etcd pod is running, if its nt running ther wil be a problem, it means we wont be able to have record bc it
                                       is our key value store, whatevr is happenin in d cl8 is stored here,like if u create a pod here, the record of the pod is found in etcd
 kube-apiserver-master     running                the apisever is the main administrative component of the cl8, the entrypoint to d cl8, if its nt running we cant mk api calls
                                               bc we cant run any of the kubectl commands, these are calls we ar making, so the apiserver needs to be healthy.


        
                   KUBE PROXY/ KUBECTL/ KUBLET/ CONTAINERD /CNI/ CSI
kube proxy .. if  its nt running all the networking will fail if the proxy is not working
kuectl : if it is nt installed, we can make api calls
kublet : if its nt running appsl cannot be deployed, its the main agent runing in our worker node that comm with the master
cointainerD:  k8is managing containers, if our container runtime like containerD is nt installed, we cant create and start containers
**FROM QUESTIONS**
containerD in k8 is simply doing 2thing: it create and start the container that is all bc thats why containerD is running in the workernode that create n 
start the container so in the past we used to use docker to achieve that purpose but k8 has deprecated dockcer tahts why we no londer use docker, we sue 
conatinerD
Kubernetes-cni: container netwrk interface, e.g weave ntwrk, flannel. if these netwrk interfaces are nt running, our nodes wil nt be ready
kubectl-csi: container storage interface: if its nt running we cant deploy our stateful appl
********* all of these we hv to check if they ar working or nt and have been able to check all of these  2:14:00



 PARAMETERS TO CONSIDER IN SCHEDULING
parameteres to consider when it comes to scheduling 
1)the default is resource 
2)node selector bt thers a problem with selector so we have
3) node afffirmity  : this is like selecting a node with some flexibility .. :with this we hv prefferred n required
  this is the new generation of node selector
4) pod affirnity: 

                                  DEFAULT IS RESOURCE/ PENDING STATE
2:22:28 
when an api call is made fOR pods to be made,d scheduler wil wil schedule the pod on the node with sufficient resources but when all the nodes dont have 
sufficient resources (this is the default expectation) the pod wil be in pending state..
the default is resource that are available but ther are other options:
node selector : for e.g i wnat to create a pod wich is my fronend app,in this deployment, if i want to ensure thatbthe pod is only scheduled on the node that 
is optimized for frontend appl, i can select the node by using node selector, so i will select it based on the label, so the scheduler wil schedule the pod on
the right node i want but if the pod i want doesnt have sufficient resource and thers anoda pod that has sufficient resource the scheduler wont schedule it 
bc the selection doesnt match that label therfor my pod wil be left in pending state.


                     DEFAULT IS RESOURCE/ PENDING STATE
3)node afffirmity  : this is like selecting a node with some flexibility .. :with this we hv prefferred n required
e.g i can say i prefer u host my frontend appl on node2 but with node affirnity, if node2 doesnt hv sufficient resource the node wil stil be scheduled bc it
now depend on the constraint, bc if the constraint says it is preferred to schedule this pod on node2 bt if node2 lack sufficient resources we can schedule 
the pod on any other node with sufficient resources... this is affirnity
one other thing is possible here, we have preferred during scheduling and;
required during execution   .................. bc with affirnity its like selecting a node with some flexibility


                                    POD AFFINITY
4(pod affirnity: we can decide that is one pod is in us1east, the other one shud go to useast2 etc
based on our topology, it says that let the pod be distributed accross diff availabilty zones , therfore if we ar deploying replicas, all of them wil nt be in
one AZ, IT WIL be distributed.


####        ######## he wil stil send the video



                         PARAMETERS TO CONSIDER IN SCHEDULING, HELM/CHARTS
we still have helm
if you run:
helm create springapp ], it will create charts
lets assune u wnat to deploy an application, charts is like manifest files all i will do is just cahnge some information
 charts-manifest files [  
it wil create a deployment for springapp: 
        deployment.yml,
if it reqs service , it wil create
service.yml,
if it reqs ingress, it wil create 
ingress.yml,
horizontal auto scale hpa.yml,
if my deployment reQS security, it wil create 
rbac.yml 

so evrytin that is needed  so once u run helm create, it wil create all my chart and nd all i may need to chnage cud be the pod,the tag, the imgage name bt 
everytin chart wil be created for me 
helm is a package manager for k8, we wil look at it in nxt class





KUB 12                            NGINX

frm an infrastructure point of view u realise that we can access our appl using multiple services ,,, if u were going to access ur appl uisng a lbalancer service that wil
incur additional cost bc a lbalancer wil create a new lbalancer in aws  
**mee** the loadbalancer service type is created when we are using ELB, eg in KUB 10&11 when we installed kops, it created and ELB, then when we deployed an
application and run 'kubectl svc, we could see lb as the service type.

5:37
we can discover our appl using  a service and all service act as lbalancer, for internal comm we can use cl8 ip service, trafic wil be routed accordingly the service perfoms
dns resolution for my appl ... for external traffic our service type wil be nodeport, it can attract external traffic, and as part of nodeport service like for a mg8 cl8
what else can we create? we can create loadbalancer(this a lbalancer service)... bt generally cl8ip is for cmmu within the cl8, whereas nodeport n lbalncer is gud for 
external comm

 WE CAN USE NGNIX TO REDUCE LB COST
frm an infrastructure point of view we can place our lb in the public subnet n our node in private subnet, for (one appl to com to the other, it nids to go tru the service of 
the appl) evrytin is runin in aws, lb is nt free , they are xpensive (for us to be able to keep our cost at the barest minimun we can deploy nginx ingress,we can create a new
service in the cl8 called ngnix ingress

**I bliv when he says we can reduce cost by using ngnix, it means that rather than creating a lb service for all the application which then means that  each of
the appl will incurr a seperate cost, rather we can create a lb service for nginx then we use the ngnix service for all the applications, that way its only the
nginx that will incurr lb cost.***

                                                            **mee** ie we are changing the nodeport or lb service type we have being using for to deploy the services/app
when we create ngnix service for our appl we can now hv service of type cl8 ip service, that way all the service/*me*app in the cl8 wil be a cl8 ip type for the appl to recive 
external traffic we wil deploy an nginx ingress controller inside the cl8, on a node we can do a dployment called ngnix ingress so the pod is scheduled wil mg8 hw traffic is 
routed to our backend appl, so as part of the deployment we are also creating a service (ingressSVC )for ngnix ingress controler and the service will act as our frontend n 
based on this service  we aer goin to create some ingress rules wich will state hw traffic can be routed



           DEPLOYING NGINX INGRESS CONTROLLER
This was deployed 
then we changed the service type of a previous application we deployed in kub10&11 
& we changed the service type from lb to ngnix


 CHANGING SERVICE TYPE
 if our service type was clusterip, we can change it to loadbalancer or nodeport but if it was loadbalancer we cant change it to clusterip
we can also change from nodeport to loadbalancer


 CREATING NGINX SERVICES FOR THE APPLICATION/ KUBERNETES PLUGGIN
                             
bc we hv nginx controller which is a k8 operator or what we call a k8 plugin or an add on that enhances the function and the security of k8, 
so with it we are sure that our cl8 wil be more secured so we can take advantege of such a secured parameter 
the type of service we will create is  clusterip and as such i can decide to write it or not and ther shud be no port ip and no NopePort number as wud hv been d csae if 
it was NodePort service


STEPS FOR CREATING INGRESS RULE
1)IN Aws, route53; create record set to establish the response
2) Create the ingress rule (manifest file)for the route type ie either bath based OR host based .
3) deploy the rule


                                            CREATING RECORD SET FOR HOST BASED ROUTING

55:36
##...  continuing with our ingress installation script
7) we can use hostbased routing or path based routing

in aws  we see a LBalancer was deployed in uS West1, where our eks cluster is running     .....  .......  56:10
since i hv this lb, i wil go to the dns service, aws route53, 
creating record set , A record
its an alias to an appl lb
if u type this, its routed to .....> this service
springapp.mylandmarktech.net ... >  springapp
web.mylandmarktech.net   .........> webappsvc
hello.mylnadmark .........> hellosvc

.........now we will create an ingress rule to achieve this
vi ingress-resource.yml


  HOW TRAFFIC IS ROUTED TO BACKENDPOINTS
lets describe the ingress , its called app ingress  rule
kubectl describe ingress apps-ingress-rule
we can see the host and the backend app its routing traffic to 
host                                     path backends                the backendpoints
springapp.mylandmarktech.net              /   springapp:80 )10.10.0.122:8080,10.10.1.148:8080,10.10.1.235:8080)
                                                    this springapp service has 3 endpoints
hello.mylandmarktech.net
web.mylandmarktech.net


  **mee***      DIFF BTW HOSTBASED ROUTING & PATH BASED ROUTING
1) Host based manifest file can have more than one 'host' object ...lol@ object
 1.200... its is used for when we have diff applications **i bliv like monolethic application
while
2) Path based will have only one host' object
   it is used for microservices ie decouple monolethic application


        WHEN WE CAN USE HOST BASED ROUTING
in a situation like this the recommendation to use is path based routing 
if i have diff appl in my env am managing then ill use host based e.g
app.com
webapp.com
aws.amazon.com
amazon.com



KUB 13         EKS

   *** was all about setting up the EKS cluster.






KUB 14                         HELM


                                HELM ARCHITECTURE

Helm:                   ...... CAN GO TRU THE POWERPOINT SLIDE
  Helm is a package manager for Kubernetes applications.
Helm will ease the process of managing hw appl are deployed in our cl8
with the use of helm we can deploy diff appl in k8, remember if u wnt to deploy an appl in k8 most of the time like our spring app deployment, what was needed
to deploy our spring app????????? now we hv a stateless appl , 
We deploy workloads in kubernetes using kubernetes objects declared in manifests files:
 e.g we deloyed a springapp Stateless web application: and how many objects were involved?
we had to write a manifest file for :
     deployment
     service 
     ingress 
     hpa       atimes we also need this 
     secrets  
     configmaps 
all these was for one appl deployment
and in addition to this we are also deploying a stateful appl

   mongodb Stateful database application: and for us to deploy this app, we nid smt like this either we ar using
     deployment/ and or what else can we use with deployment
     service 
     ingress 
     hpa 
     secrets  
     configmaps 
     persistentvolumes  
     persistentvolumeclaims 
     StatefulSets

so all of this obj need to be written, we nid to write manifest file for all of this
and this is too much job but with the help of helm, it eases n simplifies the deployment of workload in k8



                       WITH THE HELP OF MANIFEST FILE , WE have deployed some:
Custom applications: these are appl developed by our developers, our team of developers hv developed this appl
    springapp  
    java-web-app  
    maven-web-app 
    pythonapp  
    nodejsapp  
  Third party applications:
    mongo 
    mysql 
    jenkins
    metrics-server 
    csi drivers - EBSCSI  
    CNI Plugins/Add-ons:
      vpc-cni 
      weave 
      calico 
      Flannel  
    nginx-ingress 
    RBAC: services in k8
      clusterrole and clusterrolebinding   
      Role  and RoleBinding  
      serviceaccount

################################################# ALL OF THESE involves manifest files


               
                          HELM SIMPLIFIES THE DEPLOYMENT OF WORKLOADS


 HOW CAN WE SIMPLIFY THE PROCESS OF DEPLOYMENT IN K8
We can simplify the process of deployment in k8 using  helm as a package manger

DIFF PACKAGE MANAGERS:
we use package managers to install packages:
  apt /  FOR  = package manager for ubuntu/debian
  yum /   FOR = package manager for redhatOS/centOS  
  dnf /  FOR  = package manager for redhatOS/centOS 
  rpm /  FOR = package manager for redhat 
  deb /      = 
  choco /  FOR = package manager for windows OS
  pip-python-pip = package manager FOR PYTHON  
  brew = package manager for macOS  
WE also have :
Helm is a package manager for kubernetes.


                                  HELM CONCEPTS
Helm concepts:
  Helm cli: = It's a command line interface used to work with packages to 
             install upgrade or uninstall 

         e.g you can run               
           helm install springapp app/springapp /     (he said that this is helm chart :app/springapp 
             helm upgrade springapp app/springapp  /     ..... to upgrade to version of ur appl
             helm uninstall springapp
             helm add repo    ********* to add a helm repository in nexus or other locations
             kubectl apply app.yml       


                                 HELM CHARTS
Helm charts: are packaged kubernetes manifest files organised in a directory 
  format that can be reused
  helm charts are stored in helm repository and frm der we can pull the helm chart n deploy our appl in mutiple env
so we can create a helm repository n these helm charts are like artifacts n we can store them in repository like nexus as well as github pages
it has a cli wich we can equally use to run helm commands :helm add repo 
  Helm repository: -- 
      jar/ear/war, npm, python



***online.......      KUBERNETES ARCHITECTURE

A Kubernetes client-server architecture refers to the interaction between clients that manage and interact with a Kubernetes cluster,
and the server-side components of that cluster.
In essence, the Kubernetes API Server acts as the central hub, receiving instructions from various clients and orchestrating the cluster's state accordingly.

THE SERVER (Kubernetes Control Plane):
The "server" in this context is the Kubernetes API Server, a core component of the Kubernetes control plane. It exposes a RESTful API that serves as the central point of 
interaction for all clients.
FUNCTION: The API Server processes requests, validates them, and updates the state of Kubernetes objects (like Pods, Deployments, Services) in the cluster's distributed 
key-value store (etcd). It also handles authentication and authorization.
COMPONENTS: It is part of the control plane, which also includes the scheduler, controller manager, and etcd.



          HELM AUTHENTICATION
Lets cal it a k8 client server , frm this server we can mk api calls like kubectl -apply, onCe we mk the call we are authenticated n inside my cl8 we v master &worker node
so once we mk the call, we need to be uathenticated, on the master node the api authenticates u, it ensures u are authoirzed to mk such call, n the authentication allows
you to have that kube config file, so the file is authenticating you that yes u are permitted to mk the api call .. 
similarly we can execute commands like : 
helm install , once this is done the same kube config file wil authenticate u but it will depend on the helm architecture

***meee*** So it means we can install helm in a k8 cluster & we are able to run helm commands using the cli as well as k8 commands



QUESTION

WHAT IS THE BESTS TOOL WE CAN USE TO MANAGE OUR CL8

ANS:      it all depends 
maybe u want to ask hw we can best deploy our cl8
............u can deploy ur EKS cl8 using terraform scipt, u can use commands to deploy your EKS ie rather than going to the console 
u can jst run on the CLI , aws  EKS create cl8 ( the name of the cl8) that wil create it or eks ctl create cl8 bt u nid to ensure 
that your EKS command line utility has already been installed 
so u can use :
commaNd
console 
terraform
once u do that u can now proceed to deploy appl bc ur main task is appl deployment
most time at wrk u are goinmg to join a coy that already has running cl8 , it cud be EKS cl8, kubeabm cl8, kops cl8
once the cl8 is already existing all u nid is to use the cl8 to deploy appl, so main task now is appl deployment bc u wrk in an env 
with existing cl8 already , so u are goin to be worried abt how  appl are ruuing that are installed and exposed , u hv vol installed and all of that.


  CAN U CONSIDER KOPS IN THE SAME LINE AS HELM??? .... is it the same as helm

ANS:
kops is a software used to create a k8 cl8
what do u need to deploy ur appl in k8 , u nid running cl8 , a cl8 that is functionig 
a cl8 is a grp of nodes  , that cl8 may v been created using kops or u cud hv created it using aws k8 service or using kubadm
it doesnt amtter what u hv used,all u nid is a cl8,  once u hv  a cl8 u can deploy appl 

***************************************************************************


NEXT....
   HELM INSTALLATION


 THE MOST IMPORTANT FILE
            Its values.yaml


***mee**               2 WAYS TO DEPLOY THE APPLICATION USING HELM
1)By using just the image then creating the manifest/helm charts, review the application template & deploy
2) By cloning the application from then repo add the repo, review the applicaton template & deploy






KUB 15 .... PROBES

1)Liveness Probe:
Suppose that a Pod is running our application inside a container, 
but due to some reason lets say memory leak, cpu usage, application deadlock etc the
application is not responding to our requests, and stuck in error state.
Liveness probe checks the container health as defined (we tell it do), 
and if for some reason the liveness probe fails, it restarts the container.
internally , when i deploy my appl i can check if the appl is runnin by runnin the curl command
  curl localhost:8080       ..even for normal tomcat when I DEPLOY A tomcat appl, i can check if the appl is running or nt, i can curl
  curl localhost:8080/java-web-app     ...... also, i can the path to the appl
  curl localhost:8080/maven-web-app 
  curl localhost:8080/tesla       this is an apl that we v deployed, internally this appl can mk sure that its able to start this container, it must ensure that this
                                         path is healthy, if its nt healthy our liveness probe is goin to fail n the container wil be restarted via d kubelet service..19.00
$? = 0  exit code !=0                  if it runs this command curl n the exit code is equal to $=0, then it means that it wil be routing trafic to that particular pod bt 
  curl localhost:80/tesla                 if nt equals to 0, it cant route trafic to the pod, thats hw liveness probe is expected

if '$' is equal to '0', it will route traffic 


2)Readiness Probe:
This type of probe is used to detect if a container is ready to accept traffic.
You can use this probe to manage which pods are used as
backends for load balancing services. If a pod is not ready, 
it can then be removed from the list of load balancers.

#############EXPLANATION: 15:00
COS services act as lb, n if a pod is nt ready , it wil nt add the pod to the pool, so if a pod is ready dats the only time the job is goin to be performed, if its nt ready
then it means our health check has failed, so thats what readiness probe is going to achieve

(routing traffic to the pods)
      service                                 
      webappSVC                      Routing traffic to webapp003, & webapp002 & webapp007

  ****** so if for e.g pod007 is nt ready, it will remove it frm our lb agorithm, it wnt be der anymore, the service wil nt be routing traffic to an unhealthy pod, 
until when the pod is active thats when it will route traffic


3)startup Probe:
This is use for slow starting containers  
60 secs  
for containers that are taking a long time to start, we deploy the container using using startup probe as the health check

18:00


                          WE SOME INF ONLINE THAT U CHECK ABT PROBES ASwell, hw it works, it has a gud documentaction we can look at

https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ 
https://kubernetes.io/

https://github.com/LandmakTechnology/kubernetes-manifests/blob/main/liveness_readiness_probes_example.yml
https://github.com/LandmakTechnology/kubernetes-manifests/





20.25                                          DEPLOYING APPL INCORPORATING LIVENESS PROBES...........
               LETs deploy a replica set
in the ubuntu server he switched to sudo su - kops
but got error when he ran
kubectl get all     ... 24:24
he was unable to access his kops cl8 so 


                           TROUBLESHOOTING KOPS INSTALLATION ERROR
when that happens we nid to export the kube config file again(i guess thats 10b in the inatallation file we used in kops installation): run kops export kubecfg $NAME --admin
now run
kubectl get all and we hv our cl8 running

***********we wnt to deploy an appl that has liveness probes






KUB 16                    PROMETHEUS AND GRAFANA  ... (last class)

NICE explantion for :  .. 2) Explain your experience in kubernetes?? .......  18:00
my question: when is hosted zones created

4:00     ................... watch it
you would be ask what you have been able to use k8 to solve in your environment
ITS A Common experience when it comes to k8, either they want u to explain your experience or they just want to discus some problems u may v encountered using k8 and hw you
were able to fix those problem. also when u apply k8 thersa lot of xperience u gather n der a lot of problem uve been able to solve, 
so the xpectation is for you to be able to state clearly the kind of problems uv been able to fix applying k8 in ur env

also at times , they ask to xplain  k8 architecture
this is a very imp aspect in k8                              


PROMETHEUS AND GRAFANA               *********************47:40  .............########### check powerpoint
if we want to ensure our server is function well we hv to install a software that will monitor the server and give us info abt it 
=====================
its an open source monitoring and alerting toolkit. prometheus consists of serveral components some of which are listed below:
***online***
Prometheus and Grafana are a popular combination used for application performance monitoring (APM). Prometheus collects and stores time-series data like metrics from 
applications, while Grafana visualizes this data in dashboards to provide real-time insights into performance, health, and potential issues. 

How they work together for APM
Prometheus: Gathers metrics from your applications. You can instrument your code using Prometheus client libraries to collect custom metrics, such as request latencies,
error rates, and business-specific KPIs. It then stores this data in a time-series database. 
Grafana: Connects to Prometheus as a data source to create customizable dashboards. These dashboards allow you to visualize the metrics collected by Prometheus, helping you 
track trends, identify bottlenecks, and set up alerts for performance anomalies. 



                              IMP OF MONITORING
importance os monitoring:
    avoid reactive panics  
    Increase uptime. webapp pod    in k8, automatically it wil restart the pod or if the pod goes down it will recreate a new pod replace it. 
    Improve hardware and software performance.
    Plan for the future by making the best use of your resources.
    get an almost immediate return on investment

************if ur server is abt to go down or is running short of resources, it may get to a point wher the appl can no longer run in the server 


                   INstall promethus & grafana using helm
                 THIS DEPLOYMENT comes with 6 other services which includes an alert manager

PREREQUISITE
*********************************************** we need a k8 server with dynamic storage class  to deploy prometheus n grafana 
2) Kubernetes nodes iwt mini 4GB RAM with 2 core processor
3) server which has kubectl $ helm configured


THREE ASPECTS OF HELM

deploy prometheus using helm:
helm comes with the :
  helm: CLI/charts/repository     .....these are the 3aspects of helm
 helm-charts      : predefined manifest files we can use to deploy both custon and 3rd partyapplications
helm-cli           : the command line utility
helm-repository     : where the charts are found


what we shud tk note of is that 
helm instal promethus came with all these micro services

1. alertmanager-0       -- statefulset
2. kube-state-metrics   -- deployment     ********** 
3. node-exporter        -- DaemonSet     ********* collects metrics from all the nodes so we nid each pod to be runnin in each node thats why its deployed as a daemonset
4. pushgateway           - deployment
5. prometheus-server   ( deployed along side vol concept)  - statefulset/database     ***** this is deployed as a statefulset bc it comes wit a database




  GRAFANA HAS NEW ADMIN PASSWD ANYTIME ITS REDEPLOYED
whenevr you deploy grafana make sure you get the admin PW
helm I
The pW wil be diff from what we got before



                               DEPLOY NGINX & CREATE INGRESS RULE FOR PROMETHUS & GRAFANA & alERT MANAGER

         Also; CREATE A RECORD FOR PROMETHUS & GRAFANA & alERT MANAGER



    GRAFANA DASHBOARD IS PREFERRED  
he said the promethus UI isnt the best option to use so we hv to use the grafana dashboard
but we still couldnt access it
with this error am unable to xplain hw all of this guy will give us the expected result
he said we couldnt acces it yet bc it takes a while 
finally, we can now acceess it , the grafana dashboard pops up
2.21.00   so we hve deployed our cluster


                                                   SETTING UP GRAFANA
then we entered the username and PW we generated earlier when we installed grafana 
 the grafana dashboard has a data source and the source is promethus   ...................**************2:22:21
name : promethus server
save and test
connection: http://promethus server
successfully connected to promethus
so its building a dashboard 
we can see the grafana dashboard wich is communucarting with the prometus server
we can import a dashboard
in the dashboard he typed 7249
connection: promethus server
now we see the dashboard ,n whatever is happening in your cl8 can be visualized in your promethus dashboard
so whtaever issues we ar hv  e.g running low on memmory ,on our promethus UI we have alert


                                                             SAMPLE DASHBOARDS TO IMPORT
when u configure promethus as we hv done, there are some sample dashboard to watch out for 
e.g
Sample Dash Board IDS: 
  3119, 7249,  8919, 6417 ,11074
we can import any of these dashboards
it gives us info on memeory usage and 
all of these inf can also be understood by non IT GUYS
these are dashboards that have been designed by other developer
***************so promenthus is monitorng my entire cl8
the mamory uasge the pods that are ruuning their metris, whatever that is happenin
there are diff dashboard that we can check
all my k8 objects, frm deployment, the pods,replicaset , node, controller
ther is an alert manager wich we hv created certain alert: like when memory is low,
POd not ready, pod crashing, persistent vol error, these are the alerts
there are congifuration for the different issues for which we will receive alert
so this promethus



EFK =  
 Ther is elastic file , fibric and kunberner whic i will be sharing the videO with with you guys thats the final stuff to master, but before we look at that 
LET me xplain smt else, thers what we call rancher


                                    RANCHER

Rancher and kubernetes:
PROVIDE A Kubernetes platform  
With Rancher we can import and manage multiple kubernetes clusters    
We can create clusters like EKS / AKS / GKE, etc.   
Rancher provide a dasborad for kubernetes
Rancher is deployed using docker  


***online
Rancher is an open-source platform that provides a unified interface for managing container clusters, especially Kubernetes. It simplifies the deployment, management, and 
scaling of containerized applications across various environments, including multi-cloud and on-premises. Rancher adds value by centralizing authentication, providing tools 
for monitoring and logging, and integrating with other DevOps tools to manage the entire application lifecycle. 
Key functions
Cluster management: Allows users to manage multiple Kubernetes clusters from a single dashboard, whether they are in the cloud or on-premise.
Centralized control: Provides a single point for managing user access, authentication, and security policies across all clusters.
Application deployment: Simplifies the deployment and management of applications on Kubernetes, including integrations with tools like Helm.
Monitoring and logging: Includes features for detailed monitoring, alerting, and shipping logs to external providers.
Hybrid and multi-cloud support: Enables consistent governance and policy enforcement across different cloud providers and on-premises infrastructure. 
Other meanings
Rancher (person): In a general or dictionary sense, a rancher is a person who owns or works on a ranch, typically raising livestock like cattle or sheep







KUB 17                                             ELASTIC STACK
                     
                          check the EFK/ ELK powerpoint
WE ARE GOING TO SEE HOW TO USE ELASTICSEARCH, KIBANA AND BEATS

   
               TO ACCESS APPLICATION THAT IS RUNNING

1) WHEN WE DONT HAVE MANY APPLICATION:
    ***** run kubectl get pod, first before running kubectl get logs tdapp)
when u want to access an applictaion that is running, in k8 we can check the logs
kubectl get pod
we see the pods that are running in the defaulf dev space
NAME                        READY     STATUS
hello-app-6hztm              1/1         running
tdapp-759f7c6687              1/1         running


we can check the logs of each of the appl
we can describe the pod
kubectl describe  tdapp-759f7c6687     
OR check/get the logs  of the pod
kubectl logs tdapp-759f7c6687     
we can see the appl was deployed using the spring booth frame work
if u look at the appl, you can visualize the logs
********************if ders anytin wrong with your appl you can check the logs it will tell whats happening


 2) WHEN WE HAVE MULTIPLE APPLICATIONS :  kubectl logs cluster-autoscaler-6cf74d69f7 -n kube-system    (kubectl logs app-(app name) | gre errors)                     
but when u have multiple appl and ther are issues,
kubectl get pod -A        * TO get all the pods in all the name spaces
we see all the pods running
NAMESPACE            NAME    
apm                 grafana
apm                 promethus
kube-system         cluster-autoscaler-6cf74d69f7
grafana pod is running

 so if we hv 100 pods we cant be running kubectl for all of the pods, that will be a tidious task
kubectl logs cluster-autoscaler-6cf74d69f7
ERROR form server not found clustr-autoscaler      ************************** this is bc we didnt check in the correct namespace (we nid to pass the namespace)
kubectl logs cluster-autoscaler-6cf74d69f7 -n kube-system
the logs are successfully generated                     8:25


the alertmanger   , incharge of sending alert, if smt is going wrong 
kubestate -maetrics ttaht gathr all the data, whatever is happening within our k8 objects eg pods, deplyment, rs, rc,  even nodes
node exporter is also gathering whatererv is happening in the nodes, the metrics  node exporter is gathering them 
promrthus sererv pod , it will scrap all of this data into the promethus  time series databasea nd once the dta has been scrapped we can visualise what is hapening using 
grafana



****online..
No, the Elastic Stack is a collection of tools that includes Elastic APM, which is an application performance monitoring system. 
The Elastic Stack is the foundation that Elastic APM is built upon, consisting of other components like Elasticsearch, Logstash, and Kibana. 


DIFF BETWEEN PROMETHUS/GRAFANA & ELASTIC STACK
Prometheus, Grafana, and the Elastic Stack serve different primary functions in the realm of observability, though their roles can overlap. 
Prometheus is for metrics monitoring, the Elastic Stack (ELK) is for log management and analysis, and Grafana is a versatile visualization tool that can sit on top of both





                           INSTALL EFK/ELK stack K8's cluster  USING HELM


OBJECTIVE:

Monitoring, alerting & log aggregation are essential for the smooth functioning of a productive grade kubernetes cluster.

Appplication & system logs help us to understand what is happening inside a cluster, the logs are particularly useful for debugging problems and monitoring cluster activity.

In microservices architecture, a single business operation might trigger a chain of downstream microservices calls, which acn be pretty challenging to debug.
Things, however can be easier when the logs of all microservices are centralized and each log event contains details that allow us trace thr interaction between the application.
we can use Elastic stack along with Docker to collect, process, store, index, and visualize logs of microservices.


HOW ARE WE ABLE TO CENTRALIZE OUR LOG MGT  ************************ powerpoint 12:40
we can use Elastic stack along with Docker to collect, process, store and visualize logs of microservices




WHAT IS ELASTIC STACK  .........................the vendor is ELASTIC
its an open source app from elastic designed to take data From any source and in any format and then search, analyze,and visualize that data in real time.
it was formely known as ELK stack, in wich the letters in the name stood for the appl in the grp. Elasticsearch, logstack,and kibana. A fourth app, Beats, was 
subsquently added to the stack.

ELASTIC search: is a real time, distributed storage, JSON-based search, and analytics engine designed for horizontal scalability, maximum reliability & easy management. it can
be used for many purposes, but one context where it excels is indexing streams of semi-structured data, suscha s logs or decoded ntwrk packets.


WHAT IS ELASTIC STACK  .........................the vendor is ELASTIC
its an open source app from elastic designed to take data From any source and in any format and then search, analyze,and visualize that data in real time.
it was formely known as ELK stack, in wich the letters in the name stood for the appl in the grp. Elasticsearch, logstack,and kibana. A fourth app, Beats, was 
subsquently added to the stack.

ELASTIC search: ie now we can use elastic stack to store data and to index the data, indexing streamas of semi-structured data, e.g logs or decoded ntwrk packets.
************* so elastic search will act as a container, it will store all the logs  ...... this i sthe use of elastic stack, it also does indexing.
it will index the logs , so when the logs are inside elastic search or the semi structure data n some ntwrk packages are inside elasticsearch waht happens is 
elasticsearch can do indexing, it will index the logs , it will perform indexing ,indexing is just like a pattern , lets assume it wants to cfreate an index which is goin
to like be displaying logs on a timestamp basis so that whatever is happening in the cl8 , u can use ur timestamp to know aht is goin on, so that can tk place, therfore
elastic search is goin to act as a store n a kind of an analyzer.. thats what elastic search is goin to do.
once the logs are stored, the next thing is that we want to visuaize these logs and we can use kibana  .... (THE SAMe way we saw grafana)

KIBANA: is an open source analytics and visualizationplatform designed to work with elasticsearch. it can be used to search,view n interact wit data stored in elasticsearc
indices, allowing sadvanced dtaa analysis n visualing dta in a variety of charts, tables n maps

the nxt componant is called beats
BEATS: open source data shipprs that can be installed as agents on servers to send operational data directly to Elasticsearch or via logstash, wher it can be further
processed n enhanced.
Therea re a number of beats for diff purposes.
FILEbaet    , logfile
metricbeat  , metrics   e.g metric data like cpu, memory
packetbeta , netwrk data
heartbeat: uptime monitoring

WE ARE GOING TO SEE HOW TO USE ELASTICSEARCH, KIBANA AND BEATS

in few words
filebeats collects data from the log files and sends it to logstash
logstssh enhances the data and sends it to elasticsearch
elasticsearch stores and indexe the data
kibana displays the data stored in elasticsearch


                          ELK Stack Architecture
           2) data processing        3)storage           4)Visualize
1)Log

            logstash                 elatsicsearch      kibana

however one more component is needed or data collection called beats. this led Elatic to rename ELK as the Elastic stack.

            Data collection          data processing        3)storage           4)Visualize
1)Log

              beats                       logstash                 elatsicsearch         kibana (can also be used to share)




                CAN RETAIN LOGS IN ELASTIC SEARCH/ S3 BUCKET
we can decide to retain logs in elastic search for like a month  cos u dnt wnat to keep logs ther for so long. bc logs are  goin to be generated constantly so our retainment
policy can just be for a month  ,,, bt it also depends , it can be decided by mgt ,,, so in INTERVIEW, we can say that logs are kept for one month afetr wich we can decide to 
move the logs from elastic search n save them in my aws s3 bucket
i just went to data stream, we can visualize data in a format wher even non IT folks can underatnd, 


                           LOG
A log is a variable file, which means whatsoevr is hapening, they keep on changing, they are goin to be constantly changing
its just like when u run  : top

                                       TOPS
tops tells u whats happening in ur system , hw many users, the processes that are runing , the values keep changing
the events that are taking place are variable events 



   LS THERs one ASPECT  OF K8  WHich I HVAE NT COVERD WITH YOU GUYS n some other aspects WHICH I WILL COVER IN BOOT CAMP , bt the aspect i was supose 
 to cover i decided to share the video with u guys bc i want to shed some light in the 2nd half of the class, on certain terraform concepts...... k8 security n rbac




****************************** K8 TALKING POINT



Summary 
   Talking points:
1. Container Orchestra     ... ie
      kubernetes Orchestrate containerise applications 
      kubernetes Orchestrate containerise micro-service applications 

******* we choose k8 bc

2. Self healing capacity, scalability, Disaster recovery, LB,  
   
3. Automated rollouts and rollbacks

we use k8 objects to deploy workload in our cl8
imp objs to tk note of 
services for service discovery
pods
rc , rs, 
satefulset  ,, which we just saw today
daemonset
deployment
volumes
configmaps
secrets
namespace concept
ResourceQuota
nodeAffinity
podAffinity
security

*********these are very imp concepts for  to b able to xplain them
before the objects, we spoke abt the architecture
in the architecture, we saw the controlpaane, wich is also called the master node
apiserver
etcd
scheduler
controller-managers
kubernetes-cni  :
kube-proxy
kube-dns
workernodes

Kubernetes architecture:
  control plane /master-node:
    apiServer        ************ this receives the api call  eg         kubectl get  ,        helm install   
       kubectl get  
       helm install   
       ui       *********************  using the k8 dashboard with  some ui commands , u wil be communiatoin wit the APiserver, once it receives it it persist it in etcd
    etcd [db]   (db or keyvalue store)   ,,,, then whatevar info has been stored here the scheduler is notified that some wrk nids to be done
    scheduler         ************ then transfer d inf to d workernode ,kublet receives d info n immediaterly, d scheduler schedules d pod in d worker node n d resorce  
    controller-managers                                                                                              is created
    kubernetes-cni:
      kube-proxy 
      kube-dns 
  worker-nodes:                                                                                    1:21:10
    container-runtime[docker, containerD]  ,, cud be docker in the past or today cntainerD ...i pu docker in d past bc docker was depreciated jst last yr n ur xperience 
    kubelet                                                                                           is nt  docker was only 
    kubernetes-cni:        the cni is also ruuning in the worker node like in the master node
      kube-proxy 
      kube-dns 
Kubernetes objects:
  services = service discovery :   our app are running inside a pod so to acces them u must go tru a service
    users/admin/app--->SVC--->pod[APPcontainer]          user,admin , app can access the pod tru a service e.g spring talk to the db pod tru the service 
    service-TYPES:                                                                               the pod to d service is an endpoint n d service performs this task does lb
      ClusterIP :              the default service in k8                                                                          that is service discovery
        spec:         
          type       1:33:00 ...  how will a service identify that it nids to route service to a po dvia service discovery, under service we hv api version, metadata, spec, 
          selector:                   under spec seletor, in the selector we pass smt like this ,app: myapp bc d selector must match
            app: myapp 
      NodePort         *********** external
      LoadBalancer      ****************** external
      ExternalName       **************** we didnt demo this, used to cmm with app outised ur cls8
      ingress            ***has  alot of advantages .in security its xplained as additional layer of security, we can use it to deploy app using clusterIP wich increses 
         ClusterIP                                  security in our cl8
  pods---> houses containers       ************* wher appl are run
     SingleContainerPods         *********** truout this course we hadjst had singlecontainer pod bt we can v some  pod that act as ultilty  pod to the main pod 
     MultiContainerPods       ************    and in that case we can hv multi container pod
  PodTemplate:
    metadata:
      labels:
        app: myapp 
    spec:
      containers
  ReplicationControllers:   (rs,staefulset, daemonset,deployment) therfore, we dnt deploy d containers directly inside the pod dirctly bc  d pod lifecycle is very short
    spec:                                      so we depoy them using controllers, so wit the help of controlers we cqn deploy our appl
      selector:                                 bt how will d controller knw that they are incaharge of a pod,  remeber that in d pod ther is a template file,
        app: myapp                                 and in the pod ders a template ders a metadata wher we pass some labels like app: myapp
                                  for replication controllers we hv selector is looking for app:myapp, so selector knw that app=myapp, so d controllers ar able to identify
                                                      their pod usin labels n selectors, so this  replicationcontroller knows that ist responsible for this guy app:myapp
ReplicaSet:                                       similarly,if it was for rs, daemonset , deployment , these are the new generation or the big guys bc 
    selector:                                         they jst dnt use equalitybased selector
      matchExpressions [set-based]                under selector they add anoda layer : e.g match label for equality base
      matchLabels: [eqaulity based]            1:32:00           this is the same pattern for rs, stateful set , daemonset, deployment
        app: myapp 
  StatefulSet    .... used to deploy db appl
  DaemonSet          ... to deploy appl that wil run in all your node like exporter we deployed yesterday n filebead  we deployed today
  Deployment        ... recommended k8  obj to deploy in k8 bc it has rollback , rollout n it also maintains all d replicaset for all d deployment, it kips a recrd of it
  volumes                   imp  for deploying db appl ,we lokked at  PV wher a piece of storage can b created withinin our cl8 n managed by cl8 by d kube service                                                                    dats y u can rolbk n rollout
  configMaps      .......  since we create portable containerise applications , wich means our Developers don't hard code , since they dnt hardcode 
  if they are tryin to pass
    db-username: ${username}     ie in the dockerfile this info are not hardcoded, so we use config maps to pass this inf whiledeploying our contaainerized appl in k8
    db-password: ${password}            so config is goin 2 be used for us to pass additional config that were not nt passed in d docker file or any additonal config
    dockerHub-credentials , ssh-keys                that the container need to run effectively . its used to pass non-confidntial data e.g we v smt like db-password: ${password}
                                               this is a secret info, hw do we pass dis info in our cls8 or we wnt to dpeloy appl n our images ar in private repositories
                                we nid our dockerhub credentials, hw ar we goin to pass d credentials for dockerhub, or we wnt to pass some ssh keys all of dis will be passe
  secrets                                                                            using secrets
  namespace :       *********** helps us to create virtual cl8 within our k8 cl8, e.g u can create ns for dev env, prod , stagin  n ts also gud for security, e.g u can decide
    virtual clusters within our k8s.                                           that developers wil only wrk in the dev ns, they v nothing to do with prod ns

ResourceQuota    ********* also a gud k8 obj, since we v ns we can restrict hw much resources each ns can create to uptimize our resource usage in k8 ,,, using resourcequota
  node   ********* very imp, bc dis is wher all d workload are deployed, thay are dpeloyed in worker node , we v nodeselector, nodeaffinity, node affinity 
  what happens with node is that:  by default the scheduler assigns pods to be 
    created in node with sufficient resources 
  nodeSelector   : if we wnt a pod to be created only in a particular node (nodeselector or nodeaffinity)
    use to restrict the node(s) where pods should be created     
  nodeAffinity 
    use to restrict the node(s) where pods should be created 
  podAffinity  : this is used to prioritize     ,, use pod affinity to assign high priority to the pod   ..    (INTERVIEW QUESTION)
    PriorityClasses     ...................    lets assume u ar deployin multiplse appl, with d help of affinity ur prod app wil tk priority, 
      prod-app [high priority]  
      dev-app  [low priority]
      test-app [meduim priority]
    dev-app is running   , you are deployin prod app bt its 
    prod-app is pending due to insufficient resources  bc u hv deploy dev app already
    If podAffinity is enforced dev-app pods will be evicted and    
    prod-app pods will be running  ,, bc my server doesnt hv suficient resources

1:46:45             ,, if the pod is nt ready liveness will restart it to make sure it comes up 
Health Checkers:
  liveness probes/
  readiness probes/   lets assume we v 4containers running, b4 traffic is routed to a pod a health check is performed using readiness probe, it mksure trafic is nt routed
     users/apps/admins --->appSVC---> 4pods[appsContainer]                                                      to a pod thats nt ready to receive trafic
     http:
        /java-web-app
    curl -v localhost:8080/java-web-app    e.g to run the curl command on ur command line, to run a test if ur appl is running, so this is like a health test as well
                                        so readiness probe wil mk sure that b4 trafic is routed to a pod by a service, the pod is ready to receive trafic
                                       so if a pod is nt ready, readines probe wil remove the pod frm the loadservice algorithm bc the appSVC cant route trafic to the pod
   
startup probes        ..... for containers that are slow to start if we dnt wnt the container to fail, we can use startup probes

###################### this are imp stuff to tk note of    #############



                                      S3 BUCKET

1:54:40
moving logs  into s3 bucket

simon@k8:~/efk$ aws s3 ls                 ********* now we see the buckets we already hv 
class27adevops
landmark27
simon@k8:~/efk$          ******* so if  simon@k8:~/efk$   is our elastic search server and i v some log files, we can copy the logs into any of the buckets i v chosen to
                                                             move my log file into , so that implies we can store our log files in elastic search for 1month after 1mth
                                                                  we can move the logs to s3 bucket.


elastic-search:
  storage for log files 
    nodes 
    pods   

s3:
  objects  storage ,
objects like
    log files  
    audio files   
    video files 
    archive files 
  storage classes 




                               KUB 18      SECURITY

