OTHER IMP ASPECTS OF VOL, CONFIG MAP, SECRETS, OPTIONS FOR CREATING SECRET, OPTIONS FOR CREATING  CONFIGMAP, CREATING CONFIGMAP USING FILE/DECLARATIVE APPROACH,
KIND FOR THE MANIFEST FILE FOR CONFIGMAP, CREATE A SECRET USING FILE/DECLARATIVE APPROACH, DEPLOY AN APPLICATION WITH CONFIG MAP AND SECRET USING 
FILE/DECLARATIVE APPROACH, APPLICATION DEPLOYMENT,CREATING CONFIG MAP WITH EXTERNAL VOLS/ CONFIGMAP WITH FILE DATA FOR TOMCAT, EDIT/CHANGE SERVICE TYPE IN A 
FILE FROM NODEPORT TO LB, INSTALL KOBS & DEPLOYING OUR KOP CLUSTER, DEPLOYING AN APPLICATION USING KOBS, 
HOW DOES TRAFFIC GET TO THE PODS/CONTAINERS RUNNING IN KUBERNETES; FROM AN INFRASTRUTURE PERSPECTIVE, QUETSIONS


**1.56.00
now any service u create in k8 must create a cluster service ip as well

 
there are 2kinds of appl we generally deploy in k8
stateless
stateful   : we always need to maintain its state bc it captures data and we nid to ensure the data is stored and retrievable and vol concept in k8 is what we can use to achieve
that process

we also realize that why we v dif option when it cm to vol der are some other imp aspect to consider like:
diff vol option:
secret
nfs
hostpath
azuredisk
configmaps

4:48
env variables are  like secret variable ,, because the inf is stored in our source code mgt and so we shouldnt pass the env variable like this 
 env:
           - name: MONGO_DB_HOSTNAME
             value: mongosvc
           - name: MONGO_DB_USERNAME
             value: devdb
           - name: MONGO_DB_PASSWORD
             value: devdb@123





6:29      CONFIG MAP
######  HOW CAN WE DEPLOY THE ENV variable/file in a more secured way

kubernetes volumes :
==========================
==========================  by using:
Config Maps & Secrets
======================
We can create ConfigMap & Secretes in the Cluster using command or also using yml.
********************ConfigMaps:
  are used to passed non confidential information in key:value  pair that weren't 
  hardcoded/included in the Dockerfiles/code by Developers
   E.G we can use config map to pass stuff like 
    HOSTNAME
    USERNAME 
8.25 ...also if developers are required nt to hardcode, we are then required to use config maps to pass/verify to the variables

e.g we are tryin to deloy an appl and we are using a kind of a tomcat image  n in that tomcat image hw do we access n manage tomcat
we hv a file called tomcat users.xml, so  we could create a config map for our tomat users
   e.g  lets assume we hv diff users in diff , with diff roles env we are not hardcoding, these are dynamic variables
tomcat-users.xml  [  ]  
      dev = <user username="tomcat" password="tomcat" roles="admin-gui,manager-gui"/>
      uat = <user username="paul" password="admin123" roles="admin-gui,manager-gui"/>

****** we can now use config map such that when we wnt to deploy for each env we knw we hv diff users , in each of that env we can use configmap n secret for
that purpose bc we ar managing our appl n it has diff values for diff env 

   the file also has this aspect as well , so we can use config map & secret for this purpose
tomcat-users.xml  
    <?xml version='1.0' encoding='utf-8'?>
      <tomcat-users xmlns="http://tomcat.apache.org/xml"
                      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                      xsi:schemaLocation="http://tomcat.apache.org/xml tomcat-users.xsd"
                      version="1.0">
       
       <user username="tomcat" password="tomcat" roles="admin-gui,manager-gui"/>
    </tomcat-users>


11.00
                           SECRETS
****************************Secrets:
  are used to passed confidential information in base64 format e.g 
    PASSWORD
    SSH_PRIVATE_KEYS  
    dockerHub LOGIN password  
    tls certificate  

                 
                                     OPTIONS FOR CREATING SECRET
***me
   1)Secret Using Command: 
kubectl create secret generic springappsecret --from-literal=mongodbpassword=devdb@123 

2) Create Secret Using files/declarative approach

                 
                              OPTIONS FOR CREATING  CONFIGMAP:
1)Create ConfigMap Using Command:
  kubectl create configmap springappconfig --from-literal=db-username=devdb 
  kubectl create configmap springappconfig --from-literal=db-hostname=mongosvc 

2)
Create ConfigMap Using files/declarative approach

11.43 
eg we are trying to deploy this application, as part of this deployment, we hv env variables
i can decide to create a config map for stuff like hostnamename, username, bc config map is used to pass non confidential values in key value pair
& we could create it in this manner, creating config map using commands... 
17.53 .. but its not recommended to use config map to create PW, rather we use secret
should we use imperative or declarative approach to create config maps? we shud use files/declarative approach

                                     CREATING CONFIGMAP USING FILE/DECLARATIVE APPROACH
13.48..
yaml:
=====
kind: ConfigMap  
apiVersion: v1        14.57 ... to know API version: #kubectl api-resources ... this will give u all the API versions
metadata:                 **16.46 we want to create additional configuration, so under meta data; name
  name: springappconfig   
data:                           for data, we want to assign values to this
  db-hostname: mongosvc   
  db-username: devdb  
  db-password: devdb123       ************shud we create pw using configmap, ofcourse we can do that bt it is nt recommended;
                                                           ill just put here devd123, am creating pw here using configmap bt dis is nt recommended
      for us to create PW, WHICH OBJ SHUD WE be using ? kind , so what shud we use to create secret, we can create secret using command like this:
Secret Using Command: 
kubectl create secret generic springappsecret --from-literal=mongodbpassword=devdb@123 


                   KIND FOR THE MANIFEST FILE FOR CONFIGMAP 
the kind is; configmap


                                   KIND FOR THE MANIFEST FILE FOR SECRET
the kind is; secret




kubectl api-resources                         15.16
kubectl api-resources | grep -i configmap           .... to get a particular api version resource, to get extract it from this file, i can pipe it & grep 
                          this can be used for any kubernetes resource  




                                CREATE A SECRET USING FILE/DECLARATIVE APPROACH
                  THIS IS A SECRET obj we are creating ............19:00
---                                                 
apiVersion: v1
kind: Secret
metadata:
  name: springappsecret
type: Opaque
stringData:   # We can define multiple key value pairs. Like passwd
  mongodbpassword: devdb@123

19.23**** so we can create this configmap & secret & use them


                                   
                        DEPLOY AN APPLICATION WITH CONFIG MAP AND SECRET USING FILE/DECLARATIVE APPROACH 
kubectl get secret  ............. to see our secrets
kubectl get cm   .... to get config maps

vi cm-secret.yml    and paste  ,   

kind: ConfigMap  
apiVersion: v1     
metadata:
  name: springappconfig   
data:
  db-hostname: mongosvc   
  db-username: devdb  
  db-password: devdb123
-----
apiVersion: v1
kind: Secret
metadata:
  name: springappsecret
type: Opaque
stringData:   # We can define multiple key value pairs.
  mongodbpassword: devdb@123

....................to create the secret object9 : kubectl apply -f secret.yml
configmap/springappconfig created
secret/springapppsecret created
###########now we hv our secret n configmap for our configuration

we wnt deploy our springapp, we we hv our secret n configmap so we can mk some changes in the env of our manifaet file 
**************************************22:00
kind: Deployment
apiVersion: apps/v1
metadata:
   name: spingapp
spec:
   replicas: 2
   selector:
     matchLabels:
       app: springapp
   template:
      metadata:
         labels:
            app: springapp
      spec:
         containers:
         - name: app
           image: mylandmarktech/spring-boot-mongo
           ports:
           - containerPort: 8080
           env:
           - name: MONGO_DB_HOSTNAME               
             valueFrom: 
               configMapKeyRef:                        *************we dnt pass any values,
                  name: springappconfig
                  key: db-hostname                     it wil read the value frmthe config map that hv been created
           - name: MONGO_DB_USERNAME
             valueFrom: 
               configMapKeyRef:
                  name: springappconfig
                  key: db-username
           - name: MONGO_DB_PASSWORD   ************ even though we can create PW using configmaps but for best practice we use secret cos secret is for confidential data
             valueFrom:                   the value is coming frm the secret for best practice
               secretKeyRef:
                  name: springappsecret
                  key: mongodbpassword 

******************* now we can delete the previous springapp we deployed from previous class and deploy this file
vi spring.yml
kubectl apply-f spring.yml
now we can write data in the data base bc for us to deploy this appl we are making use of multiple objects  .....31:20



*****************************    APPLICATION DEPLOYMENT
if we are going to deploy an application what is involved????
LETS ASSUME the we are deploying the database as a stateful set , database stateful set or as we saw as a replica set
what else are we deploying as part of the database, we nid a storageclass, wich we wil be sseeing verysoon , thats part of vol, also persistent vol, pvc , 
this is one appl we are deploying, we cud nid configmaps and secret ,  so one deployment requires all of this.. we'l also nid to deploy a service to mk this
database discoverable

Stateful applications mongodb :
  - statefulsets/RS  
  - storageClass 
  - PersistentVolume 
  - PersistentVolumeClaim
  - configMaps 
  - Secrets   
  - service = ClusterIP   
stateless applications:  
  - deployment       .....recommended obj is deployment 
  - configMaps 
  - Secrets   
  - service - most of the time this could be NodePort / LoadBalancer     .......35:40

#######pls take note this is  very imp this is a key aspect  to underline and thats one thing that we are covering to mk sure u have a gud understanding when 
it cm to configmap n secrets


36.00
                                           CREATING CONFIG MAP WITH EXTERNAL VOLS/ CONFIGMAP WITH FILE DATA FOR TOMCAT
****************
WE HAVE ANOTHER EXAMPLE OF CONFIG MAP, THAT I WANT TO CREATE A CONFIGMAP WITH EXTERNAL VOLS, Create anoda configmap obj like this:
---
ConigMap As Volume Example   ... but he didnt create any vol
=========================
-# ConfigMap with file data
---  e.g am deploying an appl, i cud create a java webapp config like this, wher this tomcatuser.xml file,  am passing this as configmap, so u can deploy a 
Configmap like this and we ar goin to hv this config map in the system and if u want to access ur tomcat on the browser u can use this new pw and username to 
access it

apiVersion: v1
kind: ConfigMap
metadata:
  name: javawebappconfig
data:
  tomcat-users.xml: |
    <?xml version='1.0' encoding='utf-8'?>
      <tomcat-users xmlns="http://tomcat.apache.org/xml"
                      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                      xsi:schemaLocation="http://tomcat.apache.org/xml tomcat-users.xsd"
                      version="1.0">
       <user username="devadmin" password="tomcat123" roles="admin-gui,manager-gui"/>
    </tomcat-users>



***************************


37.00 ... Now back to our cluster, we hv deployed the application above and we are able to use it
unbuntu@master:~$  kubectl get sc   ..... to get storage class

unbuntu@master:~$ kubectl get svc
NAME               TYPE              CLUSTER-IP              EXTERNAL-IP      PORTS          AGE
mongosvc          ClusterIP         10.110.26.165               none
springappsvc       NodePort         10.100.100.234              none           80:32136/TCP 



                              EDIT/CHANGE SERVICE TYPE IN A FILE FROM NODEPORT TO LB
we can edit the service
unbuntu@master:~$ kubectl edit svc springappsvc 
we saw the manifest file and he changed the service type from nodeport to loadbalancer



unbuntu@master:~$ kubectl get svc
NAME               TYPE              CLUSTER-IP              EXTERNAL-IP      PORTS          AGE
mongosvc          ClusterIP         10.110.26.165               none                                 what are some of the changes here?
springappsvc       LoadBalancer       10.100.100.234             pending      80:32136/TCP     WE stil hv the samenodeport no. here****it is waiting for an external ip wich wil nt cm  ....38:58
                                                                                   & this is the problem we ar goin to be facin with self manged k8 cluster.... 39:05


** our service type was nodeport before but now it has changed to loadbalancer, 
some of the cahnges is that it stil hv the same nodeport number, its waiting for an external ip, which will not come & this is the problem we will be facing
with self manged kubernetes cluster

... so lets look at a situation where we hv a kubernetes cluster using kobs
why are we talking about using kops to deploy our kubernetes cluster ... thats what we will be looking at 



**online 
whats the diff between selfmanged kubernetes cluster and kops
A self-managed Kubernetes cluster is a general concept where an organization is responsible for all aspects of deploying and operating Kubernetes on its own
infrastructure. kOps, on the other hand, is a specific, 
open-source command-line tool used to automate the deployment and management of these self-managed clusters, primarily on cloud providers like AWS. 



                                DEPLOYING A K8 CLUSTER USING KOBS    ...... 40:50  ,, read  the  file in github account

       KOBS IS A SOFTWARE used to deploy a production ready k8 cluster
here is our github account wich we'l be using to create a kobs cluster
kobs is ifaac

kops:
Ticket07: Install a kops cluster using company documentation
          https://github.com/LandmakTechnology/kops-k8s

.kops is a software use to create production ready k8s cluster in a cloud provider like AWS.
kOPS SUPPORTS MULTIPLE CLOUD PROVIDERS
Kops compete with managed kubernestes services like EKS, AKS and GKE
Kops is cheaper than the others.
Kops create production ready K8S.
KOPS create resources like: LoadBalancers, ASG, Launch Configuration, woker node Master node (CONTROL PLANE.
KOPS is IaaC



**************lets see hw to manage kops .  ..... DEPLOYING OUR KOP CLUSTER .......... 44:00
1) create ubuntuEC2instance in AWS
he used the master node we already created

2) CREATE KOPS user          42.40*** when you are creating user in an ubuntu server, its diff from redhat
  sudo adduser kops
  sudo echo kops ALL =NOPASSWD:ALL | SUDO TEE /ET          ..... TO GRANT sudo right
SUDO SU -KOPS
2a)  instal AWS cli  ............     see the file in the github link for full steps 
 ******************* we are tryin to install kobs , it is a software taht wil help us to deploy a production ready k8 cluster
   ** we can also install aws cli uing the script ie the 2b) option but we can skip that bc we already used the  2a) option; the straight line

4) we already hv kubectl installed 
5 create iam role

6) aws s3 ls  .. to list the buckets but we access denied
he had to go to aws to create iam role for an aws service 50:00
then clicked on the master node, under action,security, modify iam role, and assigned the  iamrole we jsut created and update it
now when we run aws s3 ls , we no longer get permission denied bc we hv assigned iam role to the server.
###### now we create the bucket   .... aws s3 mb s3://kops33   (kops33 bucket name, i bliv)    ............53:59

6b) vi bashrc  and paste the name of crated bucket n exit then
source . bashrc to refresh the file
now if we run 
kops@master:~$  echo $NAME    ,, WE get a response
class33.k8s.local
kops@masteer:~$ echo $KOPS _STATE_STORE
s3://kops33
kops@masteer:~$    aws ls s3   .......   to ensure it exists 
it exists

7)  ssh-keygen      ............  57:27
8)57:40

59.30                             EVERYTHING THAT KOPS CREATED
It created all of these objects listed here
the rule for kobs is that whatever u are craeting must end with .k8s.local
we can scroll down to see everything/objects kobs will create , including a vpc , autoscaling grp fro master n worker node, lb for the api server,even some
aws resources, vpc, subnet route gatway, evrytin wil be created

                                
**********is this similar to what we saw in terrafrom as terraform plan???  in terraform when you do terrafrom plan it wil tell u evrtin tht WIL BE Created
it will create EBS Volume.

next;
####copy the sshkey so that we can ssh to our master node  ....1:01:39

it gave us some suggested commands
Suggestions:  we can edit the cluster   ..........1:01:47 
he used one of the commands which took us into a file & it is similar to the manifest files we hav been creating, 
& he said we can also change the max number of replicas . if we want

1.04.38
9) kops update cluster ${NAME} --yes

Suggestions:
 * validate cluster: kops validate cluster --wait 10m
 * list nodes: kubectl get nodes --show-labels
 * ssh to the master: ssh -i ~/.ssh/id_rsa ubuntu@api.class33.k8s.local

10b)   1:14:55 
   kops export kubecfg $NAME --admin
kubectl get node  ,,  we get a response


**** so we hv deployed our kops cluster 

how many clusters do we hv now?
now we hv kubadbm **(i bliv the previous one when we first install multi node cluster & ran sudo kubeadm init) and kops cluster, we v deployed a self manged
cluster using kops
1:18:05 *********lets go to our github and deploy a particular application



                                DEPLOYING AN APPLICATION USING KOBS

1:19:47
vi app.yml
# Complete Manifest Where in single yml we defined Deployment 
#& Service for SpringApp & PVC(with default  StorageClass),
#ReplicaSet & Service For Mongo.
apiVersion: apps/v1                     ******* springapp deployment
kind: Deployment
metadata:
  name: springappdeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      name: springapppod
      labels:
        app: springapp
    spec:
      containers:
      - name: springappcontainer
        image: mylandmarktech/spring-boot-mongo
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_USERNAME
          value: devdb
        - name: MONGO_DB_PASSWORD
          value: devdb@123
        - name: MONGO_DB_HOSTNAME
          value: mongo 
---
apiVersion: v1             ************ service
kind: Service
metadata:
  name: springapp
spec:
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080
  type: LoadBalancer
---
apiVersion: v1                     ************ persistent vol claim, we are nt creating persistent vol
kind: PersistentVolumeClaim
metadata:
  name: mongodbpvc 
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: apps/v1
kind: ReplicaSet                ***************** replicaset for mongo database rs
metadata:
  name: mongodbrs
spec:
  selector:
    matchLabels:
      app: mongodb
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongodb
     spec:
       volumes:
       - name: pvc
         persistentVolumeClaim:
           claimName: mongodbpvc     
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: pvc
           mountPath: /data/db   
---
apiVersion: v1               ************** service for mongo called mongo
kind: Service
metadata:
  name: mongo
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort: 27017

1:22:30........... we ar nt creatin persistent vol  bt let me show u smt b4 we deploy this appl

kops@master:~$ kubectl get pvc
nO resourecs found in defaullt namespace   ... there is no pvc
kops@master:~$ kubectl get sc
we hv a default storage class  ... with aws EBS provisioner, so when u craeted using kops,what do u realise? kops created EBS vol for us, this did nt happen
when we deployed kubeadm cluster and since ther was no storage class u must manually proviison it, manually provision ur pv and pvc bt when ther is a storage
class thatcan be automatic,that becomes dynamic 

kops@master:~$ kubectl apply-f app.yml  ...... 1:24:57
created 
kops@master:~$ kubectl get po   , it screating the pods
created 
kops@master:~$ kubectl get pv            the pv has been mounted,  a mount point has been craeted, it has been bound to our db pod
 it has created the persistent vol since der is a default storage class
kops@master:~$ kubectl get pvc   , thsi persistent vol claim has been mounted, so a mount point 
cretaed
kops@master:~$ kubectl get svc
we can see it created a lb in aws 
when we used kubeabdm, when we did a similar task using kubeabdm it wasnt able to create a lb in aws, it is stil show pending, waitin for external ip......... 1:26:30
*********  he put the appl ip in google n we are able to write data

the lb that has been created is manged nt self managed bc lb created in aws is manged by aws

managed or self managed  LB 
if i create a self mangaed lb, wit the self managed LB, i may create a server n in the server, i wil install a software like ngnix afterwich i wil deter hw 
service is routed
self managed =
   NGINX   
                            1:31:00
but in a manged cluster, aws wil craete  the lb for u and in the background the LB which is our service will be performing dns resolution n routing traffic 
accordingly to the backend port of the containers craeted on the nodes, this is what is goin to happen 
we can check lb in aws and all of this stuff was automatically generated, we can see the target instance(the backend instance, i bliv the containers ip), its
routing traffic to these servers and thats where our pods are running

but now, so this lb hw is it able to knw wher the traffic is been routed????
ns lookup means nameserver look, what is the name server that is linked to this load balncer (below is our lb link frm aws)
************* nslookup 6773018340fe4c89b2cbc41d4db7611-1100378697.us-east-1.elb.amazonaws.com
when u run this in ur browser :6773018340fe4c89b2cbc41d4db7611-1100378697.us-east-1.elb.amazonaws.com , it is linked to a particular nameserver



                  HOW DOES TRAFFIC GET TO THE PODS/CONTAINERS RUNNING IN KUBERNETES; FROM AN INFRASTRUTURE PERSPECTIVE

 1:36:00  .....   frm an infrastruture perspective
How does traffic get to the PODS/containers running in kubernetes.
we v 50m users whaen they try to access this appl, they are typing the lb dns name , based on our deployment we v deployed a cluster wich has private n public 
subnet in our public subnet der is an Elb , SO endusers are trying to access the appl tru the ELB guy. insid ethe cluster we v node1 and 9 , frm an infrastruture 
perspective, we v the frontend and we hv the backend , now  when ensuders tyep the lb url in the background a command wil be executed, the reQ  frm endusers wil 
query global dns servers searching for the nameserver so in d background nslookup is searching for the nameserver, this is taking mini seconds, some of d 
global dns service providers it wil check is godaddy, google,aws route53, once nslookup is executed it wil immediately route traffic  via the ELS url n then 
traffic gets into our cluster  .........1:56:00 (&inside our cluster we v a service called springSVC
n its routing traffic to backend pod , now any service u create in k8 must create a cluster service ip as well, he ran kubectl get svc n we cud see a clusterip
attached to the loadbalancer service that was created when we deployed the springapp service above,) so frm the elb, traffic gets into the cluster n for it to
get to the cluster it wil hv to get to the node n since its now in the cluster it wil identify which service the traffic is meant for n via the service traffic
gets to the pods, bc traffic cannot go to the pod directly the pods are discovered via the service n as such traffic wil get to the backend pod accordinly and
as part of the spring app  we also deployed a database pod , for the db we hv another service.. .. all our servers are runing in private subnet master n worker
while ELB is in the  public subnet

**mee** enusers traffic (they type lb url, the REQ, queries global dns & in the background, nslookup searches for the nameserver in eg godaddy,google, aws
route53) once nslookup is executed, traffic is routed via the elb (clusterip)into our cluster to the backend pod (appserver/db), inside the cluster the elb 
identifies which pod (app or db) the traffic is meant for & the pod is discovered via service discovery


*********************DNS = Domain Name service to create A rcords  
TO avoid giving this to a client :6773018340fe4c89b2cbc41d4db7611-1100378697.us-east-1.elb.amazonaws.com

we wnt to create a record , the cord can say:  .....*********** in aws we can achieve this with a service called route53 ............1:43:00
app.com ---> 6773018340fe4c89b2cbc41d4db7611-1100378697.us-east-1.elb.amazonaws.com
so instaed of the long link we use the alias name app.com
kk --->  https://github.com/LandmakTechnology/kops-k8s 
landmarkapp.net---> 6773018340fe4c89b2cbc41d4db7611-1100378697.us-east-1.elb.amazonaws.com
https://github.com/LandmakTechnology/kops-k8s
remote add kk https://github.com/LandmakTechnology/kops-k8s 

1:50:00 explanation




    2:05
 IQ: Explain your experience in kubernetes? 
I have over 6 years experience in kubernetes performing the following;
- setting up a multi-node self managed kubernetes cluster using kubeadm    ,kubeadm this is self managed
- setting up a multi-node production ready kubernetes cluster using  kops     ,, kops is also self managed bt it comes wit other aws services
- setting up a single-node self managed cluster using minikube and Docker Desktop for testing. (if u want to test certain appl you can deploy them on a single
node cluster
- setting up a multi-node managed production ready k8s cluster
  using amazon eks  ******* may in a video i can show us how to
*****if ther are ant issues in these appl am i able to fix them? 
- troubleshooting issues from k8s setup/configuration or installation. ,, so v been able to trpubleshoot issues like trying to stup my kubeadm n its failing,
or cant add worker node to master node, v had issues like tht n hv been able to fix it by creatin a token in the master runing in the workernode n having them
join the cluster
- maintaining, monitoring and upgrading the cluster components E.G   :
  scheduler, etcd, controllerManagers, kube-proxy, kubectl, kubelet,
  container-D, Kubernetes-cni[weave, flannel], kubectl-csi, apiServer  
  kops export kubecfg $NAME --admin 
- deploying applications and workloads using kubernetes objects:
    - pods/ReplicationControllers/ReplicaSets/DaemonSets,
      Deployments/StatefulSets/PersistentVolume/ConfigMaps and
      secrets
- using deployment as a choice kubernetes objects for stateless apps  
- using replicasets, volumes with persistenvolumes for Stateful apps  
- using statefulsets to deploy Stateful applications  
- rollouts and rollbacks of Deployments  ********** we can deploy new versions of the appl bc we knw what to do
- deploying applications using controllerManagers [RC/RS/DS/STS/deploy]
- setting up Jenkins-kubernetes integration pipeline for full automation ,, we wil also look at this
- deploying both Stateful applications and Stateless applications  
- making use of objects like; PV,PVC and dynamic storage classes to 
  persist data for Stateful applications [mongodb/ES/prometheus/jenkins]
- using configmaps and secrets for a secured application deployment   
- using probes for Health checks configuration in our deployments     
- using RBAC/namespaces/IAM for a secure access in the k8s. 

#### these are some few things i shud be able to talk about ..........

scheduler for scheduling pods on nodes  , this is the function of the scheduler n if for some reasn the sscheduler is nt running we wont be able to deploy any application
we can also check the sheduler pod :
ubuntu@master:~$ kubectl get pod -n kube-system    :  .... the kubectl has very imp pod that is used to run the cluster
NAME            READY       STATUS          RESTARTS             AGE
etcd-master      1/1      running            ....... we can see the etcd pod is running, if its nt running ther wil be a problem, it means we wont be able to have record bc it
                                       is our key value store, whatevr is happenin in d cl8 is stored here,like if u create a pod here, the record of the pod is found in etcd
 kube-apiserver-master     running                the apisever is the main administrative component of the cl8, the entrypoint to d cl8, if its nt running we cant mk api calls
                                               bc we cant run any of the kubectl commands, these are calls we ar making, so the apiserver needs to be healthy.

kube proxy .. if  its nt running all the networking will fail if the proxy is not working
kuectl : if it is nt installed, we can make api calls
kublet : if its nt running appsl cannot be deployed, its the main agent runing in our worker node that comm with the master
cointainerD:  k8is managing containers, if our container runtime like containerD is nt installed, we cant create and stsrt containers
cni: container netwrk interface, e.g weave ntwrk, flannel. if these netwrk interfaces are nt running, our nodes wil nt be ready
csi: container storage interface: if its nt running we cant deploy our stateful appl
********* all of these we hv to check if they ar working or nt and have been able to check all of these  2:14:00


we still hv to cover;
- helm  
its a package manger for k8 
such that we can run : [ helm create springapp ]  : to create the springapp
    
schedulling:
- scheduling in kubernetes  [ NodeSelectors / NodeAffinity / PodAffinity ] 
- EKS  
- EFK   
- Prometheus/Grafana  

 we wil look at hw to instal eks cluster

  



parameteres to consider when it comes to scheduling 
1)the default is resource 
2)node selector bt thers a problem with selector so we have
3) node afffirmity  : this is like selecting a node with some flexibility .. :with this we hv prefferred n required
  this is the new generation of node selector
4) pod affirnity: 

2:22:28
when an api call is made fro pods to be made,d scheduler wil wil schedule the pod on the node with sufficient resources but when all the nodes dont have 
sufficient resources (this is the default expectation) the pod wil be in pending state..
the default is resource thata re available but ther are other options:
node selector : for e.g i wnat to create a pod wich is my fronend app,in this deployment, if i want to ensure thatbthe pod is only scheduled on the node that 
is optimized for frontend appl, i can select the node by using node selector, so i will select it based on the label, so the scheduler wil schedule the pod on
the right node i want but if the pod i want doesnt have sufficient resource and thers anoda pod that has sufficient resource the scheduler wont schedule it 
bc the selection doesnt match that label therfor my pod wil be left in pending state.

3)node afffirmity  : this is like selecting a node with some flexibility .. :with this we hv prefferred n required
e.g i can say i prefer u host my frontend appl on node2 but with node affirnity, if node2 doesnt hv sufficient resource the node wil stil be scheduled bc it
now depend on the constraint, bc if the constraint says it is preferred to schedule this pod on node2 bt if node2 lack sufficient resources we can schedule 
the pod on any other node with sufficient resources... this is affirnity
one other thing is possible here, we have preferred during scheduling and;
required during education   .................. bc with affirnity its like selecting a node with some flexibility

4(pod affirnity: we can decide that is one pod is in us1east, the other one shud go to useast2 etc
based on our topology, it says that let the pod be distributed accross diff availabilty zones , therfore if we ar deploying replicas, all of them wil nt be in
one AZ, IT WIL be distributed.


####        ######## he wil stil send the video


we still have helm
if you run:
helm create springapp ], it will create charts
lets assune u wnat to deploy an application, charts is like manifest files all i will do is just cahnge some information
 charts-manifest files [ 
it wil create a deployment for springapp: 
        deployment.yml,
if it reqs service , it wil create
service.yml,
if it reqs ingress, it wil create 
ingress.yml,
horizontal auto scale hpa.yml,
if my deployment reQS security, it wil create 
rbac.yml 

so evrytin that is needed  so once u run helm create, it wil create all my chart and nd all i may need to chnage cud be the pod,the tag, the imgage name bt 
everytin chart wil be created for me 
helm is a package manager for k8, we wil look at it in nxt class




1:09:21 
when we use kops
kops create cluster : since our target is aws cloud, it gets into the aws cloud and in the aws cloud this command execute/achieves , it creates a series of 
resources e.g vpc n subnets, re.g subnet1& subnet2 , LB, autoscaling grp managing some worker noder, masternode, we didnt need to create server manually 
with terraform we cud cretae resources in a cloud provide



2:36:35      QUESTIONS
1)FROM THE CL8 WE created using kobs i unsderstood it but, i knw we shh tru d ec2 insatnce, the server wher wecarried out the whole operation is der a way we 
can ssh into the master node using mobaxterm or vscode remote host
****ANS: we did nt nid to sh into the master node bc we created a kops sever bt tk note that the kops server is nt our k8 master node, so frm the kops server
we cud jst remotely mange our kops cl8, we did nt nid to ssh into the kops bt if we had to do that then we can stil do that

student : cos i knw we did nt create a key pair dasts why am asking, hw do we then do it ?
ANS: WE did nt nid to create a key pair bt we craeted sssh keys while deploying our kops command , when we ran the ssh gen key command and we had exported that
key into the kops as a secret n we can use the key.

 2) when u tal abt hw the appl comm in the cl8 like d springapp n the dtabase,, hw does dis comm is it tru the programm that mayb the springapp like the  one
we were using like when u put ur data automatically u see it showing in the database,,,,, is it the programmer that do the database that wil create the 
interface?   
ANS:  
the fact is that we are using k8 to orchestrate containerized appl,in our team we v programers who are writing code they hv written d software that wil permit
users to write their name , dob number ,, they hv done that in the backend, we as devops engr we hv been able to build that code, create pipelines n we v been
able to containerize the appl by creatin an image and pushin the image to dockerhub,, in k8 we can pull that image n deploy the appl, onec it is deployed we v
an appl that is running which reQ usr to enter their inf and thos info needs to be captured in a database ..... how can it be captured ina database???? that 
becaomes a serious issue that is why we are now deploying a db appl called mongo in this case,,, 
HW DOES THAT APPL COMM WITH EACH OTHER IN K8 ??? it is done via service discovery and within the cl8 we use the cl8 ip service.

STUDENT: so frm what ur saying, ders a reference in the code that will link the info straight to yhe database ryt
ANS: .. of course

3) i nw for k8 our container runtime is containerD, is ther  a particular reason why we using it,, is der a preference??
ANS: 
we learnt docker bc we use docker to containerize appl , once we use docker, we write the docker file, with the help of the file, we build images, the images
are our appl that have been packaged, now these packages we hv shared n distributed them to our image registry like docker hub for example, now der are 2tins
involved here: containerizin and deploying the appl ,, for u to deploy the appl it means that u hv to create the conatiner n start the container.. so the 
containerD in k8 is simply doing 2thing: it create and start the container that is all bc thats why containerD is running in the workernode that create n 
start the container so in the past we used to sue docker to achieve that purpose but k8 has deprecated dockcer tahts why we no londer use docker, we sue 
conatinerD

4) when u ran kops u put a pW, HW do u knw the PW to use 
ANS:
when i was creatin my kops user , user add kops
when u craete a user with ubuntu, ubuntu wil request for u to assign a PW to the user automatically , so when u ar createin a user, it wil REQ for u to assign
a PW as well

5) whne u use kops to provison ther was an autoscaling grp that was craeted , hw does the autoscaling grp works, how is it dif frm the k8 horizontal pod 
autoscaler
ANS:    
...... very imp question, i thin i missed that when i was explaining.....let me go bk and explain that
how is the autoscalin grp diff from frm what we did When we deployed a swelf managed cl8 with kubeabdm  that did nt cm with any autoscaler ??????

now if u look at our ec2 instances lets observe smt  ...... 2:44:41
now am in west virgina, ther are 2worker nodes, if i delete this worker nodes what will happen?
ok , am deleting the 2instances .. obsever what happens when we shut down the instances, now lets check (in the ubuntuserver, he switched to kops user)
unbutu@master:~$ su - kops
;we can see only the master is ready , the 2worker nodes shows not ready bc we are shutting down the other2 nodes 
when we run kubectl get node : we see the 2nodes are now completely offline bc they v beeen terminated
bt now if i check my ec2 dashboard 2nodes are running and a new node is coming up (wit a diff ip address) bc we v an autoscaling grp taht was created wit min
and max and it is what is managing our node lifecycle.
we v an autoscaling grp for the master node that says hw many master node must be ther at al times, so if u delete the master nod ethe auto sacling grp wil 
craete anoda one and we also hv an autoscaling grp for worker node it works the same way .. thats why we call it production ready k8 cluster.

unbutu@master:~$  kubectl get svc
we hv a (springapp)lb that was created in aws
i can delete it in aws , i  can als run cli command to delete it 
kubectl delete svc springapp
once it is deleted , checking in aws  we see it says (ur resources cud nt be retrieved bc the lbwas only created when we created a type loadbalancer ssrvice
kubectl get service : but we c\nt see the (springapp)lb service again
if we run kubectl egt node, we see the master node n the 2nodes are all running now but if i want to remove the nodes completely, the only thing i can do is
to wrk on my autoscaler  .... he went to autoscaling grp in aws and he changed the desired min to zero, max to 0 and update .... the autoscaler will 
bring/shut down the nodes like ryt now we hv only the master running
*****so if u wnt to delete/stop/reduce your node just go to ur autoscaling grp and update it bring it down to zero
the same applies to the masternode .. u can udate the autoscaling grp for master

### bt am just doing it for e.g bc ,u cant be working for bank of america n just go n bring down their server bt since we ar just practising n i wnt to reduce
my cost i can do that

6) where do we store the configmap and screts
ans; 
we store them in our local env nt in the scm

7) i need clarity for the internal n external lb, wher dey are placed cos frm the diagram u ilustrated above, u said we place the lb in the public subnet pls
xplain more

ANS\;
if u hv an internal lb u can put it in the private subnet that is ok n the external wil be in the public bc the external is internet facing so all those that
wnt to access ur env frm the internet they go tru the external














