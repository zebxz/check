NICE explantion for :  .. 2) Explain your experience in kubernetes?? .......  18:00
my question: when is hosted zones created

4:00     ................... watch it
you would be ask what you have been able to use k8 to solve in your environment
ITS A Common experience when it comes to k8, either they want u to explain your experience or they just want todiscus some problems u may v encountered using k8 and hw you
were able to fix those problem. also when u apply k8 thersa lot of xperience u gather n der a lot of problem uve been able to solve, 
so the xpectation is for you to be able to state clearly the kind of problems uv been able to fix applying k8 in ur env

also at times , they ask to xplain  k8 architecture
this is a very imp aspect in k8                              



helm to install and deploy:
  P/G  
  EFK  
  Rancher    

6:00
this is a very imp aspect in k8 n u shud be able to xpln dem                        

1) Explain the kubernetes architecture??  
   masterNodes/controlPlane:
made up of 
      apiServer  : wich is the main adminstartor, the entry point into our cl8
/etcd/scheduler/controllerManagers
   WorkerNodes:
      kubelet/kube-proxy/containerRuntime-Container-d/  
these are the component
we also have the
   kubernetesClient|: 
      kubectl / UI- Kuberenets dashboard
********thes are what we can use to mg8 task in k8 when it comes to architecture

2) Explain your experience in kubernetes??

3)What problems have you encountered applying kubernetes in environment?
we kn the k8 mg8 containerized appl, if ders a problem in the app 
How can you troubleshoot application related issues in kubernetes??
our appl are running in pods so if ders a problem, we v a few things we can do , we also saw dat when it comes to docker 
now if u look at k8 n docker, if ders a problem in doker in our container we can run the below , also for k8 we run the command below
kubernetes manages containerised applications:
  kubernetes                                                    docker    :
  kubectl get pods                                          docker ps or docker ps -a      
  kubectl describe pods podName                             docker inspect containerName/ID  
  kubectl get svc/service  
  kubectl get endpoint/ep  
  kubectl logs podName                                        docker logs containerName/ID 
  kubectl exec podName  
exec to execute command inside the pod ..... 24:00                       docker exec containerName/id 
  kubectl top podName                                            docker stats/top   
we can also pipe n grep for errors
kubectl logs app-(app name) | gre errors
 
#############more xplanation  .................13:05
kubectl get po 
he tried to access his kops cl8 bt it didnt wrk so  he said 
when we v dis rror msg   to get it fixed we go to our kops cl8 setup n export the kube config file that is the file we used for kops inatallation
kops export kubecfg $NAME  --admin
then run 
kubectl get pod 
so if ders a problmr in our appl we can run that and we can also describe the pod 
kubectl describe po (pod name)

IN K8 appl IS EXPOSED USING SERVICE DISCOVERY
 service discovery:
    webAppservice ---> 7webAppPods
lets assume i hv 7 webApppods
THEREfore i will create a webapp service wich will lb traffic to the 7pods
############for me to knw my webappservice is routing traffic to all of this pod i can run
kubectl get svc
impotrantly we can get the endpoints/ ep

#############   also  .................................................NICE .........................################################18.00
we run kubectl get deploy  to see the app we deployed
kubectl get ep 
WE SEE THE CURRENT EP is 2 
kubectl scale deply (app name) ----- replica 4
we run kubectl get ep again 
now we see Ep is 4
############### there fore we can rightly say that the service is routing traffic as expected
so i can be abl rto xpalin this in real time bc it is working as expected
#########we can aslo check kubestl get svc 
to see if we can accesss the app and also check online

********** we can also get logs for the appl
kubectl logs app-(app name) | gre errors
if ders an error the error msg is able to tell us 

########################################we cud also have these errors ............27:00       our images are found in dockerhub or amazonECR
WE can run kubectl get events
in the process of pulling the image
creating the container and
strating the container there could be a problm inbtw this proceses
iys either we are pulling the wrong image
we cud hv the right image but wrong authentication with the image pull sercert when pulling frm a private repo

common errors in kubernetes:  
#  pods are in pending  state 
  pullimage error / imagepulloff   
   WRONG  image: mylandmarktech/hello:20     
          mylandmarktech/hello:22   
# authenticationerror:
  mylandmarktech/nodejs-fe-app:2  
  imagePullSecrets:
  - dockerhubcred
#KOPS CLUSTER DEPLOYMENT ISSUES: like
  IAM user not authorise     ............. we nid this to create a kops cl8 in aws
    create an IAM role with required permissions/policy  
       VPCFULLACCESS/EC2FULLACCESS/S3FULLACCESS and attached
       to the kops control server 
    Attached required IAM policies to the IAM user/group and   
    run aws configure  

  #vpc limit reached / :............ by default when u run a vpc tier account, the max numbe rof vpc u can crtae in one regionis 5, so when u reach this u can request#
    5 vpc running in production  
    Request additional vpc quota from aws  
  deploying a kops cluster [ a new vpc,s3, ec2-instances, ELB, ASG ]

node notReady: when u deploy self mg8 cl8, u hv to deploy ur node , u hv to mk sure that ur nodes are ready bt  mg8 cl8 like  EKS we dnt worry abt d node nt being ready
  kops/kubeadm = kubernetes CNI [flannel, weave, calico] is not deployed our node will nt be ready
NODES not ready cud be bc of k8 cli, k8 netwrk interfaces nt installed 
  ensure that the kubelet service is running [systemctl start kubelet]   if not node wil nt be ready    
  node is stopped/terminated in aws , in this case restart it 
 OR  Place the nodes behind an ASG with a ELB for health check  
  use kops/eks over kubeadm    to manage such problem
#crashloopbackoff 
  # pod repeatedly crashing
#ReadinessProbeFailed or 
#LivenessProbeFailed
#Insufficient resources
#persistent volume mount failures
#InvalidNamespace
#cni plugin error
 
# error converting YAML to json
=============================

k8 manifest file can be written in json or yaml
bt whenu run kubectl apply, it has to us ethe file 
e.g   ............. this is an erroe bc we dnt hv any kind as devops
kind: Devops
apiVersion: PODS    
metadata  
spec

kubectl api-resources | grep devops  
but no result
 bt grep for pod n we v result
kubectl api-resources | grep i pod



# what about issues when all pods are running but the
applications are unable to communicate?
this cud be as a result of misconfigurations.
for example: for external access we use a nodeport service type
pod.yml   
-------
kind: Pod    
apiVersion: v1   
metadata:  
   name: myapp  
   labels:
      tier: be  
spec:
   containers:
   - name: myapp   
     image: mylandmarktech/hello   
     ports:
     - containerPort: 80

svc.yml  
-------
kind: Service
apiVersion: v1
metadata:
  name: websvc
  namespace: dev
  labels:
    app: web
spec:
  type: NodePort # ClusterIP
  selector:
    app: myapp # labels= tier:be
  ports:
  - port: 80
  - targetPort: 8080  # containerPort80/8080
  - nodePort: 40200   # 30000 - 32676
we dnt hv port range of 40200, its out of our service range

################################ so if u go for an interview and u are asked what is wrong with this script, why cant we route traffic what cud be the problem
 its just misconfiguration   , also our target port has to be the same with container port, also my selector has to match to my labels

*********** so if am given a scenario like this i identify problems and fixed them

so if u are asked did u encounter any problem using k8 ????
the answe is yes
did u suceesd in fixing them ?
most of the time yes




PROMETHEUS AND GRAFANA               *********************47:40  .............########### check powerpoint
if we want to ensure our server is function well we hv to install a software that will monitor the server and give us info abt it 
=====================
its an open source monitoring aaaaand alerting toolkit. prometheus consists of serveral components some of which are listed below:

importance os monitoring:
  •  avoid reactive panics  
  •  Increase uptime. webapp pod    in k8, automatically it wil restart the pod or if the pod goes down it will recreate a new pod replace it. 
  •  Improve hardware and software performance.
  •  Plan for the future by making the best use of your resources.
  •  get an almost immediate return on investment

************if ur server is abt to go doen or is running short of resources, it may get to a point wher the appl can no longe rrun in the server 
e.g 
server [mem4GB ] --- maven[2BG], at all  time i need 2gb for my maven service to run                                             to avoid reactive panic
  OTHER SERVICES/processes are using 3G , then maven will nt run , therefore no bill can be done appl can run n dats a server so to avoid this we can monitor the server
  BOA.COM  / aa.com / cnn.com /     to avoid reactive panic  ,, imagine this server goes down, so we can monitor the env so that even b4 cx notice it we ar already aware 
  facebook.com  / 3B active users                                                                                        n fix it b4 they even notice 
  whatsapp.com /  3B active users 

  accessBank = 25B Naira as at Jan 1, 2023 
    Jan 1, 2023 - assets = 25,000,000,000/400 = $62.5m
  November = 62.5 * 1400 = 87.5B Naira 

WITH monitoring:
 Date       assets  NAIARA   USD   
 Jan 2023            25b    62.5m   
 Dec 2023            87.5b  62.5m  

WITHOUT monitoring:
 Date       RATES  NAIARA   USD-Asset   
 Jan 2023   400      25b    62.5m   
 Dec 2023   1400     25B    17.9m  

=====================================
Prometheus/Grafana 1:09   ........................... ************************ powercycle ......... prometheus configuration
=====================================
application performance monitoring = APM   
monitoring (micro-service) applications with  
Prometheus and Grafana
======================================

********************************deploy prometheus using helm
helm repo add prometheus https://prometheus-community.github.io/helm-charts
helm install prometheus prometheus/prometheus
helm show values prometheus/prometheus
helm template prometheus/prometheus

helm install prometheus prometheus/prometheus
helm upgrade prometheus prometheus/prometheus -f prometheus-values.yml -n dev

we acn still deploy promethus cloning this , bc its in our github repository ............ and folloeing the instruction but thats not what we ar using
git clone https://github.com/myLandmakTechnology/prometheus-grafana-ELK-EFK

PREREQUISITE
*********************************************** w eneed a k8 server with dynamic storage class  to deploy prometheus n grafana 
2) Kubernetes nodes iwt mini 4GB RAM with 2 core processor
3) server which has kubectl $ helm configured


deploy grafana using helm   ************************
==========================
****************************************so we are deployin promtheus n grafana in our cl8 and we wil be using k8 package manager whcich is helm   ...... 1:16:15
                                                                                     so if i dnt hv helm installed i can install helm3
deploy prometheus using helm:
helm comes with the :
  helm: CLI/charts/repository     .....these are the 3aspects of helm
 helm-charts      : predefined manifest files we can use to deploy both custon and 3rd partyapplications
helm-cli           : the command line utility
helm-repository     : where the charts are found

******as far as helm is concerned helm charts for:
nginx-ingress   https://helm.nginx.com/stable
grafana         https://grafana.github.io/helm-charts
prometheus      https://prometheus-community.github.io/helm-charts

******************SO WE want to deploy
nginx-ingress
grafana
prometheus

######we saw nginx ingress b4, bt lets see hw we can deploy nginx ingress using helm


#################first thing to do is to ensure that helm is installed

1)  ****************************make sure that helm3 is installed   
================================
helm installation:copy this 3line command and execute in my cl8:
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
chmod 700 get_helm.sh
 ./get_helm.sh

*********************   helm ls ........... to see we v helm installed 
*****************  helm repo ls ........... 

2)..........................#############ADD Helm chart FOR  ngninx, promrtheus, grafana
deploy33.sh    (he didndt use dis, jst copy and paste the 3links)
helm repo add nginx-ingress https://helm.nginx.com/stable
helm repo add prometheus https://prometheus-community.github.io/helm-charts 
helm repo add grafana https://grafana.github.io/helm-charts 

helm repo ls >>>>>>> we hv the repositories
we can also upadte the repositort: ........helm update repo

helm install prometheus  prometheus/prometheus 

i) ******************deploy prometheus using helm   
==========================
this promethus has some chart, so we wnt to deploy this using the chart called promrtheus/promtheus
we can search this repository
e.g helm search repo promethus and we can see so many charts 
but we are using chart prometheus/prometheus
the cahart has some value faile  so we can run:
helm show values prometheus/prometheus
now we can see values for my prometheus configuration, these are default values
value files are default values
everytin has been configured
############  we also have templates              *************1:25:00
from this values file waht is going to be installed
so we can run helm template promethus/promethus
lets see the diff k8 objs that will be 
we hv a long list of obj
we are deployin promethus as a statefulm set, 
the number of replca is 1
we hv are also cretain a deployment and this is happening in the default namespace
we hv her promethus non exporter , its a daemon set
we hv the service of promethus
***************i can also cahnge some info on my file
#################################################################****************** i dont understand this file hes referring to ....................???????????????

there is a values file here, i hv di svalues file  ...: promethus-file.yml, hv done some configurations   (hv shared it in the class)
in my deployment here
vi promethus-file.yml                  *******************************1:27:00
am just changin some info on my values file for promethus
in this info whats applicable in the changes
the alert manger file is goin to be changing, we are goin to be sending alert via email notification to my landmarkech. gmail.com
and this is the interval, gmial wil be notified   .......... the grp interval is goi to be 2minutes
so when do we send alert?
serverfils:
  alerting_rules.yml:
      groups:
      name:NodeDown
when our node is down for 2mins we will send alert 
when memory is less than 85%
when our pods nt ready
when persisten vol error
etc
*************************************************pls make sure you copy this guy and create a file for it ( i bliv the alert file) ?????????????????

######################################## NOW TO DEPLOY PROMETHUS
we could decide to create a namespcae
kubectl get ns
kubectl create ns dev
ill use d dev ns for this deployment

helm repo add prometheus https://prometheus-community.github.io/helm-charts        **************this is goin to deploy base on the default value file
we can still deploy
but now i hv a custom value file which is 
prometheus prometheus/prometheus -f prometheus-values.yml -n dev
deployin promethus in the dev ns
kubectl get po -n dev
nothing in the dev ns 
so lets deploy promethus in the dev ns
helm install prometheus prometheus/prometheus -f prometheus-values.yml -n dev
kubectl get po -n dev
now we see its being deployed in the dev ns
its creating promethus alert manger
promethus node exporter
nod eexporter need to scrap metrix     ***************************1:31:22 ................ i dnt understand   ????????????????????

kubectl get svc -n dev
its creating service like alert mAnager -headless
it has this headless service bc this is deployed as a stateful set
all our services a cl8 ip


lets deploy grafana
ii) ************************************************************* DEPLOY GRAFANA USING HELM ................. 1:32

grafana   https://grafana.github.io/helm-charts             ********************we only hv this helm chart
  helm repo add grafana https://grafana.github.io/helm-charts
  helm install grafana grafana/grafana -n dev                    *******************IN THE DEV NS

#########ONCe installed get admin user by runnin this command  (to get admin user psswd)
kubectl get secret --namespace dev grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

  username = admin  
  password = 5ibxy9DOfkwuTjyWaW34T5wKAmA5sQ7Dp7aBkvmh

#################### whta do we use to access grafana externally , its a service right?
we need the service
lets check the dev ns 
kubectl get svc -n dev
we see grafana service is clusterIP 
we cant use clusterIP for external service so we can edith the grafana service 
kubectl edit svc grafana -n dev
since am using kops cl8 i can change it to service type loadbalancer and its going to create a loadbalancer in aws
but loadbalancer are not free 
kubectl get svc -n default   (ie default ns)
i have this loadbalancer i created before
ls
i see the previous deployments  ...... e.g app.yml , javaweb , mongo
vi into app.yml  ,,, which has the springapp svc
i see the service type is lb so i change it to clusterip
kubectl apply -f app.yml
service type has been changed
so now we hv the loadbalancer service in the dev ns bt its nt free
so i wnat to maintain my clusterIP which is most secured
helm ls -n dev
we see the deployments in the dev ns
lets delete the grafana deployment
helm uninstall grafana -n dev
when i deploy grafan it created a lb in aws 
now redeploy grafana
################################# i want to show you smt that is very important /...............................1:41
why do i wnat to use only clusterIP services
whenevr you deploy grafana make sure you get the admin PW
helm I
The pW wil be diff from what we had before

lets also deploy ngnix ingress
i can search the repository and get the charts that are there
so lets search this repository and get the right helm chart to use 
ngnix ingress will enable us get a layer7 lb in our cluster which we can use to route traffic to multiple applications

helm charts for:
nginx-ingress   https://helm.nginx.com/stable   ....................THIS is our online repository
so lets search this repository and get the right helm chart to use 
helm search repo  nginx-ingress    
 nginx  nginx-ingress/nginx-ingress       >>>>>>>>>>>>>>>>>>>>>>>  ... i v this helm chart
helm install nginx  nginx-ingress/nginx-ingress -n dev      . .... in the dev ns
#########no wthe nginx ingress controller has been insatlled, 
kubectl get svc -n dev
it has created a lb in aws
we check the lb in aws and we see the hosted zone    ... ie ( landmarkapp.net, grafana.landmarkapp.net)
i bliv hosted zones were creayted earlier
let me show u guys smt
since we hv this lb in aws i can  go to aws and create  Arecords ................1:46:00
mk sure to cretae the right loadbalancer while making the Arecord
now we create a file (the ingress file)

'.....................########## ingress file using the host based routing 
vi ingress33.yml
 

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress33  
  namespace: default  
spec:
  ingressClassName: nginx
  rules:
  - host: springapp.landmarkapp.net
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: springapp                   ************* routin to our springapp svc
            port:
              number: 80
  - host: java.landmarkapp.net
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: javawebappsvc         *********************** routing to javawebapp svc
            port:
              number: 80

kubectl apply -f ingress33.yml
deployed
kubestl get ingress
kubectl describe ingress  
we see its in the dev ns
we see the backend
springapp.landmarkapp.net
and its routing traffic when we check online 
one lb is routing traffic to multiple applications

####lets create another file 
cp ingress33.yml ingress3.yml
vi ingress3.yml
                                       1:5700
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingresspg
  namespace: dev
spec:
  ingressClassName: nginx
  rules:
  - host: grafana.landmarkapp.net
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: grafana  ***************************  routing to grafana
            port:
              number: 80
  - host: prometheus.landmarkapp.net
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: prometheus-server
            port:
              number: 80
  - host: alerts.landmarkapp.net
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: prometheus-alertmanager     **************   service name for alert manager
            port:
              number: 9093

********************************************* kubectl apply -f ingress3.yml
we caheck online: prometheus.landmarkapp.net
we can see the alerts that have been configured
************ we tried to access prometheus.landmarkapp.net
but couldnt bc we havent created Arecords for alert manager service
after the ARecord created we can access it online 
we caould also access grafana.landmarkapp.net 
he said the promethus UI isnt the best option to use so we hv to use the grafana dashboard
but we still couldnt access it
with this erroe am unable to xplain hw all of this guy will give us the expected result
he said we couldnt acces it yet bc it takes a while 
finally, we can now acceess it , the grafana dashboard pops up
then we entered the username and PW we generated earlier when we installed grafana 
 the grafana dashboard hasa a data source and the sourec is promethus   ...................**************2:22:21
name : promethus server
save and test
connection: http://promethus server
successfully connected to promethus
so its building a dashboard 
we can see the grafana dashboard wich is communucarting with the prometus server
we can import a dashboard
in the dashboard he typed 7249
connection: promethus server
now we see the dashboard ,n whatever is happening in your cl8 can be visualized in your promethus dashboard
so whtaever issues we ar hv  e.g running low on memmory ,on our promethus UI we have alert
when u configure promethus as we hv done, there are some sample dashboard to watch out for 
e.g
Sample Dash Board IDS: 
  3119, 7249,  8919, 6417 ,11074
we can import any of these dashboards
it gives us info on memeory usage and 
all of these inf can also be understood by non IT GUYS
these are dashboards that have been designed by other developer
***************so promenthus is monitorng my entire cl8
the mamory uasge the pods that are ruuning their metris, whatever that is happenin
there are diff dashboard that we can check
all my k8 objects, frm deployment, the pods,replicaset , node, controller
ther is an alert manager wich we hv created certain alert like when memory is low 
there are congifuration for the different issues for which we will receive alert
so this promethus

********************when we deployed promethus , some items were deployed 
kubectl get po -n dev
didnt get exactly what he wanted so 
sudo su - kops
kubectl get po -n dev
we can see the items 
remember we deployed garafana independently
when we deployed prometus, it deployed 
promrthus alert manager
promethus kube state metrics
promethus node exporter
promtheus push gateway
promethus server 
etc
################# but how did it deploy my alert manager
kubectl get deploy -n dev
NAME
grafana
nginx-nginx ingress controller
promethus kube state metrics
promethu promthus push gateway
promethus seevrer     ************************  all these as a deployment

how about staeful staeful
kubectl get sts -n dev 
NAME                              READY
promethus -alertmanger             1/1
******************** PROMETHUS ALERT manger was deployed as a statefulset with one replica
######################## lets check daemon set
kubectl get ds -ndaemonset 
promthus promthus node exporter       *************** this was deployed as ad aemonset 


what we shud tk note of is that 
helm instal promethus came with all these micro services

1. alertmanager-0       -- statefulset
2. kube-state-metrics   -- deployment     ********** 
3. node-exporter        -- DaemonSet     ********* collects metrics from all the nodes so we nid each pod to be runnin in ecah node thats why its deployed as a daemonset
4. pushgateway           - deployment
5. prometheus-server   ( deployed alongside vol concept)  - statefulset/database     ***** this is deployed as a statefulset bc it comes wit abdatabase


EFK =  
 Ther is elastic file , fibric and kunberner whic i will be sgharing the vidoe with with you guys but before we look at that 
thats the final stuff to master bt befor ethen rancher

Rancher and kubernetes:
PROVIDE A Kubernetes platform  
With Rancher we can import and manage multiple kubernetes clusters    
We can create clusters like EKS / AKS / GKE, etc.   
Rancher provide a dasborad for kubernetes
Rancher is deployed using docker  

we will deploy rancher using docker  ................................ 19.08
checking if he has docker server to deploy rancher
didnt hv , so creating docker server using ubuntu

#!/bin/sh  
# Add Docker's official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
sudo apt install docker.io -y  
sudo docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher

sudo docker logs  container-id  2>&1 | grep "Bootstrap Password:"
sudo docker logs  b3e4c58c646c  2>&1 | grep "Bootstrap Password:"
b3e4c58c646c



  https://18.119.126.168

  security and EFK  

kops@master:~$ kubectl get po -n dev
NAME                                                READY   STATUS    RESTARTS   AGE
grafana-948dd5956-xcn5q                             1/1     Running   0          35m
nginx-nginx-ingress-controller-7f48d57b4c-7tghw     1/1     Running   0          43m

