\1:27 ........we studied devops with aws, linux and k8  
****************************************************************************************************
1:51:20   ****he said we are talking abt hw we deploy applictaions in k8 
kubernetes Service , FQDN = Fully Qualified Domain name.,,what is Static Pods ?,, controllers, CONTROL MANAGERS, HOW TO CREATE A REPLICATION controller, that wil be creating a particular pod

HOW POD IS MANGED ,create a secret for dockerhub


********************************************************************************************************************

ubuntu@master:~$ kubectl get po 
no resources found in the default namespace    ****************************** this is set to my current context , its my current name space

ubuntu@master:~$ kubectl get po -n dev    ............**************** we can see some pod that v been  deployed in the dev namespace
NAME      READY      STATUS       RESTARTS      AGE
hello     1/1       running         0            8m38s
webapp    1/1       running         0             12s

 WHEN I RUN  kubectl get po  i want it TO LIST THE pods in my (dev namespace) in the default namespace
----
at the moment the default namespace is my current namespace, thats why when i execute  kubectl get po , it says no resources in the default namespace bt once i run :
kubectl config set-context --current --namespace=dev       **v modified my current workspace such that if i execute  kubectl get po it lists d pods in the dev namespace

ubuntu@master:~$ kubectl get po     ....******************************  kubectl get po it lists d pods in the dev namespace, so d dev namespace has become my current  
NAME      READY      STATUS       RESTARTS      AGE                                                                                    namespace
hello     1/1       running         0            8m38s
webapp    1/1       running         0             12s

**************originally, it was the default namespace that was the current namspace but i v changed my context to dev


########################### MORE PODS COMMANDS ............7:25
kubectl get all
kubectl get events
kubectl delete po (podname)
kubectl delete all --all 
kubectl delete all --all -n dev     
kubectl delete pod --all                                                                                               interpretin , creatin &mangin d pod 
kubectl get pods                               Anotation is a metadata info abt ur pod that is set by d sys/k8 service n it is what d sys wil understan when it comes to
kubectl get pods -o yaml   ...   will present the pod in that are runnin in our clustera yml file, we can see annotation,bt when we made d file to create d pod der was no anotation
kubectl get pods --show-labels
kubectl get pods -o wide           **************** gives more inf abt the pods
kubectl get pods -o wide --show-labels
kubectl get node
kubectl get node -o wide    *************** we see d nodes we v , the k8 version, the kernel version and the runtime which is containerd and not docker .(tk note)
kubectl delete node node5   ******************* to delete node5

in docker we run: docker inspect  to get more info abt the container
kubectl  describe pod <podName>      *********to get more info abt the pod 
kubectl  describe pod <podName> -n <namespace>

16:10:00
kubectl run app --image=mylandmarktech/hello --port=80              ... dis wil deploy a pod called hello


hello.yml       >>>>>>>>>>>>>>>>   using the declaractive approach  ....  (20:50) *************WE want to creat/deploy an appl called pod (i think he meant hello)
=========   kams     ........ this is the acronmy, it stands for kind,apiversion, metadata, spec
kind: Pod   
apiVersion: v1    
metadata: 
   name: hello
   labels:
     app: hello     
spec:
   containers:
   - name: hello   
     image: mylandmarktech/hello  
     ports:                                          *********with dis definition file, we can deploy our application ,,,
     - containerPort: 80      #######  we did nt declacre our  namespace so.. but what is our current namespace???/ wher wil d app wil be deployed (dev namespace bc we made)
                                                                                                        it our default namespace by modifing it above
***********************************************************************************************************************************************************
kubectl api-resources   *********** to get the apiresources for our pod,, we can see so many api resources , ...
kubectl api-resources | grep pod      ....  so to specify n grep for pod , we see we can either call it pods or po and the apiversion is v1
******************************************************************************************

########## to deploy the manifestfile  we cfeated above 
mkdir manifests
vi manifest/hello.yml  and paste the file 
kubectl apply -f manifest

ubuntu@master:~$ kubectl apply -f manifest
error: unable to edcode "manifest/hello.yml:json cannot unmarshla array into 
       GO struct field ObjectMeta.metadata.labels of type map[string}string

************************so he vi again into the file and saw taht he listed metadata bt its suppose to be a dictionary not a list 
    metadata: 
   name: hello
   labels:
     -app: hello     .....so he corrected it to  app: hello
  
ubuntu@master:~$ kubectl apply -f manifest
pod/hello created
ubuntu@master:~$ kubectl get all
NAME            READY       STATUS      RESTARTS       AGE
Pod/hello         1/1        Running     0              18s               *********** the pod is running bt hw do we discover the pod in our clutser?  service discovery


ServiceDicovery:
==============
*********the first service type is:
ClusterIP is the default kubernetes service type that support
communication within the cluster. 
************** to communicate within our cluster, we will use a clusterip service type                                             LEts cal it 
e.g we v the hello app on node1,& d webapp on node9 ,, for webapp to comm with hekllo app, we v to create a service for our helloapp, helloSVC n D sevice type is clusterip
if u look at the pod we created in the definition file , we v a label; app is mapped to hello
POD=labels:
  app: hello            *******........... app is mapped to hello
Serice = selectors:  
   app: hello  

********************* for a service to deiscove rany pod, its going to be using labels, so what is the label of the pod
so we v labelas n we also v selectors...  so ders a label that says app is mappsed to label , its under the pod, so we v pot label
under service, we ar goin to v sevice selectors
in my k8 cluster der wil b many sevices,  hw wil the servoice be able to identify this pod,, it nids to create a selector that matches this pod like that so ... (hello)
#########so in the hello service (helloSVC), ther will be a selector, n the selector s saying app, bc this selector depends on the pod, so in the app pod, we v a label n app
is mapped to hello 
*******************
webService                                 hello
hellSVC                                    labels
selector                                   app:hello
app:hello               d app has a container port  80
ClusteriP                 
targetport80       when u create a service, wher u v the container port will be represented by the target port, so d servixe is targeting port80
 so the webapp wants to com with thw helloapp , d webbapp can nt comm with d helloapp directly in the cluster, its nt supported in k8, so hw is the com to be established?,
via service discovery..

lets assume the webapp pod has its own label:
webapp
app:web
8080           container port
so the webapp wants to com with thw helloapp, the comm is goin to be est via the helloSVC ,webapp is goin to route traffic to the service that is controlling the hello pod
once it does that, then the service will comm with the pod in this order
this is service within the cluster
in k8 we can create a service and we will look at theta very soon 

lets talk abt k8 SVC:  .........,30:52

kubernetes Service:
  In Kubernetes Service makes our pods accessible/discoverable 
  within the cluster or exposing them outside  the cluster.
  service will identify pods using it's labels And Selector. 
  Whenever we create a service a ClusterIP (virtual IP) Address 
  will be allocated for that serivce and DNS entry will be created for that IP.
  So internally we can access using service name(DNS).

2174  2409
########CREATING SERVICE FOR HELLoAPP
hellosvc.yml  
=========== kams    ..... .....acronmy
kind: Service  
apiVersion: v1  
metadata:
  name: hellosvc    ... i can assign a servicename
spec:
   type: ClusterIP    ...service type
   selector:   ... hw wil the service be identified or hw will it identify the app that its suppose to route traffic to ., it wil be based on the label of the appl
      app: hello     .... its hello bc refferring to the manifest/identifictaion file we used to create the hello pod above ,, the label says app:hello
   ports:                                                                              therefore, my selector will be hello, so it can route traffic to the appl
   - port: 80           ......service port  bc our container port in the manifest file is we used to create the pod is 80
     targetPort: 80        

#############we can create a new file or add it to the existing manifest file we already have  by ,, (--- to seperate the files)

******************************************************************

kind: Pod   
apiVersion: v1    
metadata: 
   name: hello
   labels:
     app: hello     
spec:
   containers:
   - name: hello   
     image: mylandmarktech/hello  
     ports: 
---
kind: Service  
apiVersion: v1  
metadata:
  name: hellosvc    
spec:
   type: ClusterIP    
   selector:   
      app: hello     
   ports:                                                                              
   - port: 80           
     targetPort: 80        

**********************************************************************
ubuntu@master:~$ kubectl apply -f manifest/
pod/hello unchanged
service/hellosvc created
ubuntu@master:~$ kubectl get all 
we see both opd n service 
ubuntu@master:~$  kubectl get events        38:00
with the help of the container runtime, w esee it created n started the container 

We can check the service 
   Kubectl  get svc 
NAME                TYPE               CLUSTER-IP                EXTERNAL-IP       PORTS      AGE         
Hellosvc           ClusterIP           10.105.209.246              none              80/TCP      97s
          
To get the end point 
Kubectl get ep
NAME                    ENDPOINTS                  AGE
Hellosvc                  10.36.0.1.:80             105s           .... it has just one end point  43:40

Kubectl  get svc –o wide
NAME                TYPE               CLUSTER-IP                EXTERNAL-IP       PORTS      AGE         SELECTOR
Hellosvc           ClusterIP           10.105.209.246              none              80/TCP      116s         app=hello
          
 (which is the service ip)
Curl  cluster-ip   10.105.209.246        ,, and we successfully communicated with the app usin d service ip address
When I wnt to curl, we curl on port 80 cos if my service port was smt else then I v to write the port number but since it is 80 , I can curl without it

So my hellosvc is a clusterip service  and it has a service ip address, so when the webapp is trying to comm wit it its going to use this service n the webapp talks to the
service on the service port, port80 and the service is routing the traffic to the appl on the appl port number , d container port number wich is also port80 in this case

Other commands regarding service

kubectl get svc -n <namespace>       (svc is service)
kubectl get svc -n dev  


kubectl get svc -n dev  
kubectl describe svc  
kubectl get ep      ... we can get the end point.. one/ A single service can route traffic/LB to multiple replicas of an appl  43:20
kubectl describe svc     gives more info abt the service
kubectl delete svc

ubuntu@master:~$  kubectl describe svc hellosvc
Name:             hellosvc
Nmaespace;         dev
Labels;            none
selector          app=hello
IP                 10.1 05.209.246
IPS:               10.105.209.246
Endpoints:          10.26.0.1:80

ubuntu@master:~$  curl  10.26.0.1:80   --------> 10.36.0.1:80 
when we curl on port 80, the service route the traffic to the container IP address  ie we curl to acces the container on that iP address


                     #########################  FQDN

*******************************
What is FQDN?               ..........           45:21   in kubernetes by default we create containers in the samenamespace to enable them comm wit each oda, however, 
FQDN = Fully Qualified Domain name.                                                     if pods ar nt in the same namespace we have to use the FQDN
If one POD need to access service & which are in different names space 
we have to use the FQDN of the service.
******************Syntax: <serivceName>.<namespace>.svc.cluster.local
                 ex: myappsvc.dev.svc.cluster.local

 ubuntu@master:~$    curl hellosvc.dev.svc.cluster.local 
curl: (6) could not resove host: hellosvc.dev.svc.cluster.local                                                                47:30
*************this is bc this is nt hw am suppose to run this command, am suppose to run it in a diff appli bc that is going to be within the clutser,, if we v more than one
container or more than one pod running we can execute this command in one other pod so that we ar able to access the appl that runs in a completely diff Namespace.

serviceNAME  = hellosvc   
service FQDN = hellosvc.dev.svc.cluster.local    

    ############################################################TO USE FQDN 

let me jst use a command to create a new pod   in the default namespace
kubectl run web --image=mylandmarktech/java-web-app --port=8080 -n default  

ubuntu@master:~$ kubectl get po
NAME            READY       STATUS      RESTARTS       AGE
hello            1/1         Running      0              4s           
web               1/1        running     0               10s       .... we see web running

ubuntu@master:~$  kubectl get po -0 wide -n default 
NAME            READY      STATUS      RESTARTS      AGE     IP            NODE      NOMINATED NODE
web                1/1      running      0            48s    10.44.0.1      node1

ubuntu@master:~$ kubectl get svc 
NAME                TYPE               CLUSTER-IP                EXTERNAL-IP       PORTS      AGE         
Hellosvc           ClusterIP           10.105.209.246              none              80/TCP    14m

********i cud a command in this pod  just like docker  kubectl exec web,  or interactive  (kubectl exec -it web) its the same in k8
ubuntu@master:~$   kubectl exec -it web -n default bash         .....  now, i v a pod, the name of my container is web, in default namespace
kubectl exec [POD] {COMMAND] is DEPRECIATED and will be removed in a future version.
root@web:/usr/local/tomcat#                *************now we are inside the container and it is running in the default name space 
     #### i want to comm with my hello container using the FQDN

root@web:/usr/local/tomcat#  curl hellosvc.dev.svc.cluster.local        52:47   
we successfully commu with the container (they are in 2diff name space)

root@web:/usr/local/tomcat#  curl hellosvc
curl: (6) could not resolve host: hellosvc    ,,,  whne i use just the service name am unable bc they ar in diff namespaces, so u hv to use the FQDN when d containers ar 
running in  diff namespaces.
            
                                                                                                                                                                  container
root@web:/usr/local/tomcat#  exit              54:10  ####remember earlier we config our default namespace to be dev n now we didnt indicate default so its in dev lik hello
ubuntu@master:~$  kubectl exec -it web bash             ......these are now containers in the same namespace n using just the service name am able to comm
root@web:/usr/local/tomcat# curl hellosvc        ****now am accessing a container that are in the same namespace .. in the same namespace wit just d service name it wil work n 
successfully communicated                                                                 also with the FQDN it wil also work ..

*******so am inside the webapp pod , i wnat to acces the helloapp, am doing that through the hello service


54:45
root@web:/usr/local/tomcat#  exit
ubuntu@master:~$ kubectl get po
NAME            READY       STATUS      RESTARTS       AGE
hello            1/1         Running      0             32m
web               1/1        running     0               5m30s

ubuntu@master:~$ kubectl delete po web
pod web deleted 

ubuntu@master:~$ kubectl get po
NAME            READY       STATUS      RESTARTS       AGE
hello            1/1         Running      0             32m

********web wasnt recreated bc when u use pod to deploy k8 objects, pods cannot be recreaed and it cannot be scaled
e.g we can decide to have more replicas of hello if we ar going to be using pod
in k8, the pod lifecycle is very short so if we ar going to deploy appl, we shud nt use pod    .....
(we shud use controllers like Replica Sets, Deployment, Deamon sets to keep pod alive)

56:20 ##################  READ UP POWERPOINT

************************************* STATIC PODS***************

IQ: what is Static Pods ?
    Static Pods are controlled by the kubelet service  

creating static pod
sudo vi /etc/kubernetes/manifests/file.yml            ************we ar creating it in the k8 manifest dir

kind: Pod    
apiVersion: v1   
metadata: 
   name: myapp 
   namespace: dev     .. we wnt to create a pod in the dev namespace
spec:   
   containers:
      - name: myapp   
        image: mylandmarktech/java-web-app   
        ports:
        - containerPort: 8080   

ubuntu@master:~$ sudo vi /etc/kubernetes/manifests/file.yml 
then paste the file
ubuntu@master:~$ kubectl get po
NAME            READY       STATUS      RESTARTS       AGE
hello            1/1         Running      0             37m
myapp-master      1/1        running     0              21s      ..... the file was created but we didnt need to run kubectl apply -f /etc/kubernetes/manifests/file.yml 
 so the static pod is running                                                                       we ar going to get permisson denied

ubuntu@master:~$ kubectl get po
NAME            READY       STATUS      RESTARTS       AGE
hello            1/1         Running      0             37m
myapp-master      1/1        running     0              21s  
my app              1/1       running     0             64s        ***** immediately the file was created, a static pod was created.

ubuntu@master:~$  kubectl describe po myapp
ubuntu@master:~$ kubectl delete po --all
pod hello deleted
pod myapp deleted
pod myapp-master deleted    ************ all pods deleted 

ubuntu@master:~$ kubectl get po
NAME            READY       STATUS      RESTARTS       AGE                the pod was recreated
myapp-master     1/1         Running      0             8s     ...  this pod is still running bc it is a static pod wich is controlled by the kuberlete service 
                                                                  so long as the manifest file exists, it wil be restarted

************to delete a static pod , we v to delete the manifest file..   .... I BLIV HE DELEted the manifest file
---
NB:
We should not create pods directly to deploy applications.
If a node  goes down in which pods are running, Pods will not be rescheduled.
We have to create pods using controllers which manages the POD life cycle.
                ######################CONTROLLERS****************************************1:05:00
controllerManagers:
  ReplicationControllers 
  ReplicaSets, 
  Deployments, 
  DaemonSets  

A workload is an application running on Kubernetes consisting of a single 
component or several components that work together inside a set of pods. 
In Kubernetes, a Pod represents a set of running containers on your cluster.

Kubernetes pods have a defined lifecycle. 
For example, once a pod is running in your 
cluster and the node hosting the pod fails then pods running on the node
will fail. Kubernetes treats that level of failure as final. 
You would need to create a new Pod to recover,even if the node later becomes healthy.
************Therfore we nid to use controller mangers such that if this  node goes down a controler manger will ensure that this pod is rescheduled on another node 

1:07:00   ################ HOW TO CREATE A REPLICATION controller, that wil be creating a particular pod

ReplicationControllers = rc 
=========================== kams 
kind: ReplicationController      ......keypair
apiVersion: v1   
metadata:    ..... dictionary
   name: apprc                    .....app replication controller ,,it wil control d state of my pod,for it to select d pod, for selector,app is mapped to web, so the RC wil
   namespace: dev                                            be selectin any pod dat has the label app=web n it wil ensure 3replicas are running  ...1:14:53
   labels:
      app: apprc          ..... this is my replication controller label
spec:
   selector:
      app: web  
   replicas: 3    
   template:     #podTemplate   ... this is my pod template
      metadata: 
         name: webapp       ...... my pod here is webapp          1;10;56
         labels: 
           app: web    
      spec:
         containers:
         - name: app  
           image: mylandmarktech/java-web-app       ......  .... THIS IMAGE is on a tomcat base image  n the container port is 8080 
           ports:                                  this replication controller we ar creating called apprc, its menat to mange this pod info here ie the pod template ..
           - containerPort: 8080                                 now for replica we want 3 n d pod label is web,so selector label is also web
---                                
kind: Service                              ###### AS part of this deployment, we wnt to create a service as well           
apiVersion: v1  
metadata:
  name: webappsvc       **********name of service 
spec:
   type: NodePort         *********this app nids to be accessed externally so we change this from ClusterIP to NodePort  ... . ..1:17:25
   selector: 
      app: web             ********this is alos based on the pod template
   ports:
   - port: 80  
     targetPort: 8080               >>************target port is the smae as container port
     nodePort: 31000 #30000-32676            *****isnce we hv node port, i can also decide the node pod to us so 31000 .... nodport range frm 30000 - 32676
                                                    if i dnt choose a pod n #nodePort:, the system wil dynamically assign a port n it will be with the port range

vi manifests/app.yml   and paste file 
kubectl apply -f manifest/app.yml     .......deploy application
successful

ubuntu@master:~$ kubectl get all
NAME                 READY       STATUS      RESTARTS       AGE
pod/apprc-kgh2g        1/1         Running      0             20m
pod/apprc-tmlz8        1/1        running      0              20s  
pod/apprc- zncll          1/1       running     0             20s       

#############AL the pods are running ,, 3pods are running bc in the manifest file we defined 3replicas

ubuntu@master:~$  kubectl get rc 
NAME        DESIRED    CURRENT      READY        AGE 
apprc        3            3           3          80s

ubuntu@master:~$  kubectl describe rc apprc       ..... to see more info
ubuntu@master:~$    kubectl get events
ubuntu@master:~$   kubeectle get svc -0 wide 
NAME            TYPE           CLUSTERIP            EXTERNAL     PORTS        AGE          
hellosvc       clusterip       10.105.209.246         none         80/TCP
webappsvc       Nodeport         10.99.250.229        none        80:31000/TCP
                            this is the service ip

ubuntu@master:~$ kubectl get ep
NAME             ENDPOINTS
hellosvc            none               **** the hello pod is nt running bc we deleted the pods so this service doesnt v any endpoint
webappsvc          10.36.0.1:8080, 10.36.2:8080, 10.44.0.2:8080      *********** webappsc has 3endpoints , we hv 3ports here if i wnt to comm wit them it has to be done
                                                                       via the sservice
ubuntu@master:~$ kubectl get all
NAME                 READY       STATUS      RESTARTS       AGE
pod/apprc-kgh2g        1/1         Running      0             3m9s
pod/apprc-tmlz8        1/1        running      0              3m9s  
pod/apprc- zncll          1/1       running     0             3m9s       

ubuntu@master:~$  curl 10.99.250.229  (this is the serviceip)
successful

&************* we hv 3ports here if i wnt to comm wit them it has to be done via the sservice  but we v a nod eport that has been created
with  the node port service called webappSVC
WE V 3REPLICAS E.G     webapp1 , webapp2, webapp3    ,, bt when der ar some many names der can be a problem,, but when it comes to our service that is routing traffic to 
all dis pods ,,, the target port is 8080 and we v a master node as part of the cluster and our service is nodeport and this menas it can process external traffic 
e.g der ar 25million users trying to acces the appli , what happens is that each of this node has an IP address (a node IP address) he login to aws n got the ip adress
(we had deleted node5 earlier so now he deleted it in aws)                         (nodeport)
1:34:18  .. so the users are typing any of the node ip e.g masterIP = 3.128.206.139:31000 , now when traffic gets into this cluster , rmeber that in our clusters, we v 
kubeproxy running and as such, it will immediately identify that this traffic is meant for a particular service n so it wil be routed immediately to the service concerned 
frm the service, which  now has endpoints ie our pods/containers, so traffic gets to the endpoints

in our cluster we have
masterIP = 3.128.206.139
NODE9-IP= 3.148.106.135
                                  *************this is service disCOVERY, WHICH IS AVERY KEY aspect to take note of 
3.148.106.135:31000  ... accessed online successful, momentarily, we can see d trafic routed as it switches btw d pod names: pod/apprc-kgh2g ,pod/apprc-tmlz8,pod/apprc- zncll 

ubuntu@master:~$ kubectl get all
NAME                 READY       STATUS      RESTARTS         AGE       IP           NODE         NOMINATED NODE       READINESS
pod/apprc-kgh2g        1/1         Running      0             15m      10.36.0.2      ndde9         none                none
pod/apprc-tmlz8        1/1        running      0              15m      10.36.0.1      node9          none
pod/apprc- zncll          1/1       running     0             15m      10.44.0.2       node1             none

1:39:44 ###################### # ##########READ UP
Kubernetes Objects
NodePort - Exposes the service on each Node's IP at a static port. A ClusterIP service, to which the NodePort service will route, is automatically created. You'll be ableito contact the NodePort service, from outside the cluster, by using
"<NodelP>:<NodePort›".


 ##########more COMMANDS FOR REPLICATOR CONTROLLERS
labels:    app: web   = address: gra  
selectors: app: web   = address: gra  
replicas = 3  

kubectl apply -f <filename.yml>
kubectl apply -f rc.yml
kubectl get rc 
kubectl get rc -o wide  
kubectl get rc -n <namespace>
kubectl get all
kubectl scale rc <rcName> --replicas <noOfReplicas>   ..     **************** ..... WE CAN ASLO SCALE the replication controller 

kubectl scale rc apprc --replicas 4     
kubectl describe rc <rcName>
kubectl delete rc <rcName>

###################   **********.................. EXAMPLE
ubuntu@master:~$ kubectl delete po --all
deleted    ************ all pods deleted 
pod/apprc-kgh2g        deleted
pod/apprc-tmlz8        deleted
pod/apprc- zncll       deleted

ubuntu@master:~$ kubectl get po
NAME                 READY       STATUS      RESTARTS       AGE
pod/apprc- 7jr9m      1/1         Running      0            12s
pod/apprc-ffv25       1/1        running      0             12s  
pod/apprc- hdd8r      1/1       running     0            12s    ............. all the pods have been rescheduled/recreated bc my pods are been manged by a replication
                                                                      controller n the RC has a label app:web n the RC has a selector that has selected the label, once it is
                                                            selected, the pod labels hv define dthat the replicas is 3, so at all times, the RC must maintain d desired state  
                                                         of my appli which is 3 


####################################WE CAN SCALE ..... SO IN deploying i shud nt deploy using a pod as an object  but rather controllers managers like RC

ubuntu@master: ~$
ubuntu@master:~$ kubectl scale rc appre -- replicas 4
replicationcontroller/appre scaled
ubuntu@master: ~$
ubuntu@master: ~$ kubectl get po
NAME            READY       STATUS           RESTARTS           AGE
apprc -64d27     1/1      Running                               7s
apprc-7jr9m       1/1     Runn ing                              2m3s
apprc-ffv25       1/1      Running                              2m3s
appre-hdd8r       1/1      Running                               2m3s

ubuntu@master: ~$
ubuntu@master:~$ kubectl scale rc appre --replicas 0
replicationcontroller/apprc scaled
ubuntu@master:~$
ubuntu@master:~$
ubuntu@master: ~$
ubuntu@master:~$
ubuntu@master: ~$ kubectl get po
No resources found in dev namespace.
 ubuntu@master: ~$
ubuntu@master: ~$
ubuntu@master: ~$
ubuntu@master:~$ kubectl scale rc appre -- replicas 2
replicationcontroller/appre scaled
ubuntu@master: ~$
ubuntu@master: ~$ kubectl get po
NAME            READY       STATUS           RESTARTS           AGE
apprc -fsfsk     1/1        Running                                 4s
apprc-lzmdn      1/1       Runn ing                                4s


kubectl scale  pod app --replicas 4    = NO        ,, we cant scale pod ..................1:46:41
kubectl scale  rc apprc --replicas 1   = YES  
kubectl scale  rc apprc --replicas 4   = YES  
kubectl delete rc apprc


###################******************* we can scle depending on demand 
1 replicas running and serving 10m requests from users       ... ...****** if request incresases to 50m users we can then scale up to 5
5 replicas running and serving 50m requests from users   
spike 
     but we cant sacel in docker , thats why in our env, we use docker to:
  docker is use to containerise                    
  kubernetes is used to Orchestrate containers     .. .. .with k8, we can scale,, if a junior engr mistakenly delete our pods, k8 will automatically recreate them 

1:49:30**************** ..... there is no component in docker that is capable of recreating containers,, so we shudnt use a software taht is limited ,but docker is nt 
completely useless, so we are studying docker bc we dnt need to use k8 to create docker files and build images, k8 does nt use docker files to build n share images in image 
registry so we nid docker to be able to create a docker file,build images and share those images to docker registry so that k8 can pull the images to deploy application


1:51:22
########### hw we deploy applictaions in k8 , which isa very key aspect when it comes to my understnading of k8, 
where are applications deployed in k8?/??......     we shud be able to est the fact that applictaions/containers are running in pods   
        HOW POD IS MANGED
pods can be deployed/managed by using:
 1. pods as a kubernetes objects  
 2. controllerManagers kubernetes objects   
  How POD is manage  

ReplicaSet = RS :   ...  ****************HOW TO USE REPLICA SET TO deploy our application    ......1:53:50
==========
What is difference b/w replicaset and replication controller?
RS is the next generation of replication controllers 
The only difference as now is the selector support.

RC --> Supports only equality based selectors.    ......equality based is equal conditions
key == value(Equal Condition)    
selector:
    app: javawebapp
    tier: fe    
    client: tesla  
###while
RS --> Supports eqaulity based selectors and set based selectors.  ,(set based is that we can hv keys in multiple volumes)
eqaulity based:
key == value(Equal Condition)  
set based:
  key in [ value1, value2, value3 ]   

      e.g for replication controller
selector:
   matchLabels:   -# Equality Based
    key: value
    app: javawebapp
    tier: fe    
    client: tesla
WHILE RS                                for set base we can have one key that is equal to multiple values or a set of values
   matchExpressions: -# Set Based  ,   under selectors: it can either be  matchlabels which is Equality conditions OR matchexpression which is set based where we can have
   - key: app              ********* we can hv key, operator, values , so 'app' alone can match to javawebapp,myapp and fe  ... .....1:56:50
     operator: in
     values:
     - javawebpp
     - myapp  
     - fe  
**************************************************************************************************************
rs.yml  = kams      ******************************** CREATING a rePLICA SET  .. this is the template
--------------
kind: ReplicaSet
apiVersion: apps/v1  
metadata : 
   name: rsName  
spec:                                      spec, can start with template bt for best practice we also hv it in this order...............1:58:12
   replicas: noOfReplicas  
   selector:                *********************   we can either hv equality based selector or set based selector
      matchLabels:
         <key>: <value>
         <app>: myapp
      matchExpressions:
      - key: <key>
        operator: <in / not in>
        values:
        - <value1>
        - <value2>
        - <value3>  
   template: #PODtemplate  
     metadata:
       name: podName
       labels:
         <key1>: <value1>
         <key2>: <value2>     
     spec:  
       containers: 
       - name: <containerName>
         image: <imageName:tag>
         ports:
         - containerPort: containerPortNumber  
******************************************************************************************************DEPLOYING appl using RS
---                            WE wnat to DEploy rs.yml
rs.yml   
kind: ReplicaSet
apiVersion: apps/v1  
metadata:
  name: pythonapprs            ... we are deploying a python web applictaion
spec:
  replicas: 1       ... if we dnt pass any replica, its going to crreate 1replica
  selector:
    matchLabels:
      app: python    
  template: 
    metadata:
      name: webapp  
      labels:
        app: python  
    spec:
      containers:
      - name: web 
        image: mylandmarktech/python-flask-app:2       .. we are puling this frm docker hub
        ports:
        - containerPort: 5000        ... as seen in the file in docker hub
---
kind: Service          ********* we wnt to create a service that can access this appli externally
apiVersion: v1  
metadata:
  name: pythonsvc    
spec:
   type: NodePort  
   selector: 
      app: python       
   ports:
   - port: 80  
     targetPort: 5000   ...... shud be the smae as container port
     nodePort: 31100 #30000-32676  ......... we already used port 31000 earlier, so now we use can use a diff port like 31100

now ,,, vi rs.yml  and paste the file ..........  (its a yml file, manifest file are written in a language called yml)
kubectl apply -f rs.yml  
successful

ubuntu@master:~$  get all 
NAME                          READY       STATUS                RESTARTS           AGE
pod/apprc -986bl                  1/1        Running                  0               4s
pod/apprc-ffmtc                   1/1       Runn ing                  0            4s
pod/pythonapprs-94dfl            0/1      containerCreating           0

ubuntu@master:~$  kubectl get ep
NAME             ENDPOINTS                 AGE 
hellosvc          none                     112m
pythonsvc         10.36.0.2:5000            23s
webappsvc     10.36.0.1:8080,10.44.0.2:8080      69m

ubuntu@master:~$  kubectl get po -o  wide

ubuntu@master:~$  kubectl get svc -o wide
NAME                TYPE               CLUSTER-IP                EXTERNAL-IP       PORTS      AGE         
Hellosvc           ClusterIP           10.105.209.246              none              80/TCP    114m
python svc          nodeport             10.99.232.74               none
webappsvc           nodeport            10.99.250.229               none

***********nternally, i created a nodeport service for python but that nodeport  service also has a cluster IP assigned to it, so if i wnat to access my python appl, i 
can curl, using the clusterip ,to  access the appl internally

ubuntu@master:~$ curl 10.99.232.74
App Works!!! ubuntu@master:~$              .......2:10:06

i can also curl by using one of the IP addresses of my servers
in our cluster we have
masterIP = 3.128.206.139           ... i think we already deleted master  earlier2
NODE9-IP= 3.148.106.135

ubuntu@master:~$ curl 3.148.106.135:31100
App Works!!! ubuntu@master:~$   

also on the browser :3.148.106.135:31100
successful

***************appl successfully deployed using replica set 
*******************************


2:20:00 (he tried to pull an image from a private repository then he made teh repo public so that we can access it) 
he then created a new private repo.. pull the nodejs image frm github, tag and pushed the same image to the new repo he created  
2:26:32  .......   my job with docker is to containerize


'###############ALL this is done in the docker server 
docker pull mylandmarktech/nodejs-app:2  ............           .2:24:00
docker tag mylandmarktech/nodejs-app:2 mylandmarktech/nodeapp
docker push mylandmarktech/nodeapp  


######################## IN the k8 server, we ar deploying in k8 , docker only help us containerize n share the images in image registry
node-rs.yml            
==========                            deploying using replica set
kind: ReplicaSet
apiVersion: apps/v1  
metadata:
  name: node-rs    
spec:
  replicas: 1 
  selector:
    matchLabels:
      app: node       
  template: 
    metadata:
      name: nodeapp  
      labels:
        app: node    
    spec:
      imagePullSecrets:
      - name: dockerhublogin
      containers:
      - name: nodeapp  
        image: mylandmarktech/nodeapp   
        ports:
        - containerPort: 9981  
---
kind: Service  
apiVersion: v1  
metadata:
  name: nodesvc    
spec:
   type: NodePort  
   selector: 
      app: node         
   ports:
   - port: 80  
     targetPort: 9981
     nodePort: 30500   #30000-32676  
---
2:31:28
ubuntu@master:~$  vi node-rs.yml 
replicaset.apps/node-rs created
service/nodesvc created
buntu@master:~$
ubuntu@master:~$ get po
NAME                          READY       STATUS                RESTARTS           AGE
pod/apprc -986bl                  1/1        Running                  0             64m
pod/apprc-ffmtc                   1/1       Runn ing                  0             64m      #########error  pulling image
node-rs-cx9t2                     0/1        ErrImagepull             0             10s
pythonapprs-94dfl                 1/1        Running                  0
buntu@master:~$
buntu@master:~$  kubectl decribe po node-rs-cz9t2
##############we the pod is controlled by 'a ReplicaSet'....can see the pod is in pending state bc of the image pull error: failed to pull image; access denied, may reQ 
authorization and this is bc we ar tyring to pull the image froom a private image registry .. therefore we nid to create a secret for dockerhub

   
kubectl create secret docker-registry regcred     ...****************** docker-registry is the name of the secret which we call regcred 
 --docker-server=<your-registry-server>  docker.io
 --docker-username=<your-name>  mylandmarktech
 --docker-password=<your-pword>   admin123
 --docker-email=<your-email>     ********************************can enter email but without email it will work


*******i will run this in my k8  CLI server        ....................2:37:40

kubectl create secret docker-registry dockerhublogin \             **************      we called it dockerhublogin
    --docker-server=docker.io --docker-username=mylandmarktech \
    --docker-password=admin123  

ubuntu@master:~$ kubectl get secret
and now i can see the secret    .... now i can pull images
ubuntu@master:~$    kubectl describe secret dockerhublogin
i can also dercribe the secret

*************** i will go back to my file above and under spec, i will add :imagepullsecrets; name:dockerhublogin

ubuntu@master:~$  vi node-rs.yml 
replicaset.apps/node-rs configured
service/nodesvc unchanged
buntu@master:~$
ubuntu@master:~$ get po
NAME                          READY       STATUS                RESTARTS           AGE
pod/apprc -986bl                  1/1        Running                  0             64m
pod/apprc-ffmtc                   1/1       Runn ing                  0             64m    
node-rs-cx9t2                     0/1        ErrImagepull             0             10s
pythonapprs-94dfl                 1/1        Running                  0
buntu@master:~$
buntu@master:~$  kubectl delete po node-rs-cz9t2
pod"node-rs-cz9t2" deleted
buntu@master:~$
buntu@master:~$  kubectl get po
NAME                          READY       STATUS                RESTARTS           AGE
pod/apprc -986bl                  1/1        Running                  0             73m
pod/apprc-ffmtc                   1/1       Runn ing                  0             73m    
node-rs-jtmrb                     0/1        containerCreating             0             8s          ..... the new container is being created n later we see its running
pythonapprs-94dfl                 1/1        Running                  0              33m

################################################sUCCESSFULLY deployed..........  2:41:15

ubuntu@master:~$  kubectl get svc -o wide
NAME                TYPE               CLUSTER-IP                EXTERNAL-IP       PORTS      AGE         
Hellosvc           ClusterIP           10.105.209.246              none              80/TCP    114m
python svc          nodeport             10.99.232.74               none
webappsvc           nodeport            10.99.250.229               none
nodesvc             Nodeport             10.99.250.229              none           80/TCP


ubuntu@master:~$  kubectl get ep
NAME             ENDPOINTS                 AGE 
hellosvc          none                     112m
pythonsvc         10.36.0.2:5000            23s
webappsvc     10.36.0.1:8080,10.44.0.2:8080      69m
nodesvc         10.44.0.3:9981                10m            ************* the nodesvc  routing traffic to this endpoint


############if we ar trying to access the nodesvc,  we can check in github, he checked in the repo we pulled the image frm, we wnt to access our nodejs appl, the ppl has
some resful API

 RESTFULAPIs:
   /landmarktechnologies
   /html
   /jsonData

  curl 3.148.106.135:30500/landmarktechnologies   

    curl 3.148.106.135:30500/jsonData   



   boa.com/login   
   boa.com/transfers  
   boa.com/mortgages  

apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: regcred

kubectl get rs 
kubectl get rs -n <namespace>
kubectl get rs -o wide  
kubectl get all
kubectl scale rs <rsName> --replicas <noOfReplicas>

kubectl describe rs <rsName>
kubectl delete rs <rsName>

kubectl scale rs nodeapp --replicas 3 

  landmarktechnologies

   http://34.219.16.213:31500/landmarktechnologies

    http://34.219.16.213:32000


deployment of applications using ReplicaSet in kubernetes:
ReplicaSet:
   selector:
      matchLabels:
         app:  node    
   template:
      metadata:
         name: webapp  
         labels:
            app: node  
      spec:
         containers: 
         - name: webapp  
           image: mylandmarktech/hello  
           ports: 
           - containerPort: 80  
---
spec:
   type: NodePort  
   selector:
      app: node   


DaemonSet:
==========
https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/
