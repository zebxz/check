RECAP , PROVISIONERS AND BACKENDSRECAP,
PROVISIONERS, TYPES OF PROVISIONERS, LETS SEE HOW REMOTE EXEC WORKS, CREATING AN EC2 INSTANCE, subnet, REFACTORING, PASSING A PROVIDER 
 create a subnet/subnets inside of the vpc , ACCESSINg ATTRIBUTE IN A MODULE, IMPLICIT DEpendency, LOCAL BACKEND, .TerrAFRM DIR, terfrm state file, READING FRM A STATEFILE,
REMOTE STATE DATA SOURCE ,Elastic ip, NULL RESOURCE, INLINE command and explanation of the scripts been transferred into the instance , AWS_key_pair, TERRAFORM CONSOLE
USING LOOP , USING FUNCTION, SLICE FUNCTION  ,, COMPLETE VPC TO CREATE , REMOTE BACKEND.. , READING STATEFILE IN S3 BUCKET , STATE LOOKING ... 

   **** in the main.tf file (resource block for instance) we referenced the key variable to pass the key BUT in the null resource, we used a file to pass sthe key ???/
 ********** why did you have to go on the console to delete the vpc, when we got an error that vpc maximum exceeded??? 
you tried to use trfrm to delete it but couldnt then went on the console .. ??? 

diff btw output and datasource  
https://stackoverflow.com/questions/75246774/difference-between-data-source-and-output-block-in-terraform
 




   ****start


i bliv by now uve all configured your env interms of you hv ur ides, cloned ur code & installed terfrm & uve been playing around with terfrm 


looking at what we ;ve covered so far, last week we looked at terfrm meta arguments
so what are some of the meta arguments we've used


what is meta arguments ??

tefrm gives you what are called mata arguments
these are arguments that alter the normal behaviur of our resources 
by default resources that we create in trfrm has a certain convention/behaior
the normal convention is that if i declare 1resource block , it wil cratea 1 resource bt if i want to reuse d same code to create multiple resources of d same tin then i can
pass in an arguemt thta wil manipulate the behavior ... that is what is a mata argument 

WHILE , 
an ARGUMENT, acts as an input, if u  wnt to input inf into a resource ,u can pass an argument so that it becomes part of the input into your resource
so we put in arguments 

BUT, 
ATTRIBUTES , is what we need to get out of our resource

A MODULE:
we say that you can create a module and a module is just resources (terfrm resources) that you can put inside a dir, now that becomes a module and we can ( call that) can
create 
those rsources just by using the module block
so this is one of the trfrm blks among the 10 tfrrm blks that we hv used so far
so u can use a module blk to actually create resources

Tonight we wil continue wit our exploration of teRRAFORM

we'll go over PROVISIONERS AND BACKENDS



                                          PROVISIONERS:
You can use provisioners to model specific actions on the local machine or on a remote machine  
in order to prepare servers or other infrastructure objects for service

9:15
Explanation:
i have my laptop here , and i want to craete an ec2 instance, in my aws account, i can use trfrm in my laptop to provision the ec2 instance in aws
i can have a resource blk, run trefrm init, terfrm apply , it wil create the ec2 instance
***************** but what if, lets say i have a file in (laptop) my local env and i want it transferred into my ec2 instance once that instance has been created and launchd
and bootstrapped. this now becosmes a process of configuration, am tryin to config this ec2 instance by transfering a file into it , or i want to run a script once the 
instance has created

********11:20                         OPTION TO TRANSFER FILES TO THE INSTANCE AFTER IT IS CREATED
one way we saw we can run a script is by using user data, we can use user data to run a script, that is a porcess of configuraton, this is hw i can configure my instance
using tfrm by using user data
but what if i have a script on my local env that am nt passing as user data bt i wnat such that after my instance has created i want to transfer the script into my instance
then once this script has been transferred run this script on the instance
so trfrm gives you what is called provisioners, u can use provisioners to do that
so i have used this process of provisioners to craete my instance bt then am using what we call a provisioner to do what we call configuration


                                                   PROVISIONERS ARE OUR LAST RESORT
############ when u look at the documentaion, trfrm gives a strong warning that *************** PROVISIONERS ARE OUR LAST RESORT****************
WHEN ur using provioners, it is recommended that only use them when neccesssary jst bc of the nature of hw provisioners wrk
onof the things u wil realise as u work with provisioners is that if a provisioner fails e.g  if d script ur trying to install fails then the provisioner wil b deleted
and when this happens , if the provisiner was working on a resource like an ec2 instance , the instance will be deleted and recreated
thats why we hv to use provisioners as last resort.

TYPES OF PROVISIONERS

1) file provisioner
The file provisioner copies files or directories from the machine running Terraform to the newly created resource. The file provisioner supports both ssh and winrm
type connections.
that is :if i have a file on my local env, i can use a file provisoner that will copy the file and tk it to the destination which is the remote machine (the machine that is
been provioned)

2) LOCAL-EXEC PROVISIONER
the local-exec provisioner invokes a local executable after a resource is created. 
This invokes a process on the machine running Terraform, not on the resource. See the
remote-execprovisioner to run commands on the resource.

Note that even though the resource will be fully created when the provisioner is run, there is no guarantee that it will be in an operable state - for example system 
services such as sshd may not be started yet on compute resources.

Explanaton:
A local exec basically runs on your local env ,i can run a command on my local env after my remote instance has been craeted
and we will see exactly what i mean once we use it 

3)  REMOTE- EXEC
The remote-exec provisioner invokes a script on a remote resource after it is created. This can be used to run a configuration management tool, bootstrap into a cluster, 
etc.
To invoke a local process, see the local-exec provisioner instead. The remote-exec provisioner requires a connection and
supports both ssh and winrm.

Explanation:
it perfroms a task on your remote resource, if am creatin an ec2 instance , i can use a remote exec, i can run commands on the remote instance ie the instance hv jst created
if i use a remote exec provisioner but for me to be able to run command inside of the instance(the remote instance), iil need to connect to it 
hw do i connect to an instance , il use ssh , iil need to ssh into that particular instance for me to be able to conect to the instance after wich am then able to run either
my script or command after the connection.


    
16.50 **
resource "aws_instance" "web" {
  # ...

  # Establishes connection to be used by all
  # generic remote provisioners (i.e. file/remote-exec)
  connection {
    type     = "ssh"
    user     = "root"
    password = var.root_password
    host     = self.public_ip
  }

  provisioner "remote-exec" {
    inline = [
      "puppet apply",
      "consul join ${aws_instance.web.private_ip}",
    ]


 
^^^^^^^^^^^^^^^ 16;00

                                LETS SEE HOW REMOTE EXEC WORKS
we have this tefrm provisioners file 

illustration:
                  ************   STEPS TO CREATE OUR EC2 INSTANCE IN A CUSTOM VPC

this ec2 instance we will use a data source
we hv dis data source which is creating an ubuntu server so we wil use this data source to get our AMI 


STEP1) DATA SOURCE FOR AMI:   this wil gnerate an AMI for ubnutu


          data.tf file

data "aws_ami" "ubuntu" {
  most_recent = true
  owners    = { 09879009}

filter {
   name    = "aws"
   values  = 
}
 filter {
   name = 


step2) RESOURCE BLK FOR INSTANCE  ***********  once we get AMI, we are goin to input/use it in our  (instance) resource block


                     main.tf

resource "aws_instance" "my_ekl_instance" {
ami           = data.aws_ami.ubuntu.id                             *** am reading my instance type frm  a variabale
instance type = var.myInstance                          **i can use what we call camal casing (he changed d i in instance to I and he changed it in the var file too)
subnet_id        =data.taerraform_remote_state.                ********* am passin a subnet id bt d subnet is , i dnt hv d subnet yet
key_name         =                                         19:15  so lets try n create d vpc first, so bc am in dis provisoner lets create anoda dir here n create a 
                                                                                                      module for my vpc
 


STEP 3)   VARiable.tf file     (variable for the instance region/type/key/path/use)

varaiable "region" {
  type      = list(string)
  default   = ["us-west1", "us-west-2", "us-east-1"]
}
varaiable "myInstance" {
    type = list(string)
    default = ["t2.micro", "t2.medium"]
}
varaiable "my_key" {
    type     = list(string)
    default  =  "eks-instance"
}
varaiable "my_path" {
    description   = "private key path"
    default  =  "mykey/elk-instance.pem"
}
varaiable "instance_user" {
    default  =  "ubuntu"
}



STEP 4: RESOURCE BLK FOR VPC
19:15
 ********* AS see above, am passin a subnet id bt d subnet is ,i dnt hv d subnet yet so lets try n create d vpc first, so bc am in dis provisoner lets create anoda dir here
n create a module for my vpc, i  now hv a dir here called vpc
now ill go to the trfrm registry ND TYPES aws_vpc as a resource 
he copied the basic usage :
    
resource "aws_vpc" "main" {
   cidr_block = "10.0.0.0/16"

in the vpc dir, he created a new file (vpc.tf)
and pasted it
so this will craete my vpc
so on this vpc he added some tags



     ********* vpc.tf file                   *****VPC/TAG

resource "aws_vpc" "main" {
   cidr_block = "10.0.0.0/16"

tags = {
   Name = " Demo-vpc"}


                                  REFACTORING/CREATING VARIABLES for VPC cidr & Tag
STEP 5)                    ******** then let , REFACTOR this code, ie i can pass this as variables 


vpc.tf file

resource "aws_vpc" "main" {
   cidr_block = "10.0.0.0/16"       
 

tags = {                               
   Name = " Demo-vpc"}
 }
}

varaiable "cidr" {                  ***variable for cidr
    type  =  "string"
Default   = "10.0.0.0/16"
}

varaiable "tags" {                  ***variable for tag
    type  =  "string"
Default   = "Demo-vpc"
}

5a)    *********** referencing the variable cidr and vpc tag   ( he usually just goes back to NO.4 and refernce it by doing the changes e.g "10.0.0.0/16" to "var.cidr"

 resource "aws_vpc" "main" {
   cidr_block = "var.cidr"       *********** so we are referencing the CIDR variable above

 tags = {
   Name = " var.tags"}          ****************** referencing the vpc tag above



  STEP 6)                                                   PASSING MY PROVIDER 
  23:08  ************* so have declared 2 varaiables...... 

after have done this, for this particular vpc  i need to pass a provider 
i NEED  to PASS MY PROVIDER block , am using AWS 

provider "aws" {
   region = "us-west-1"      ********so this is where am creating my vpc 


************* so now inside of the vpc dir, i can create anoda file and call it varaiables.tf
so ill remove the variables from the vpc file and paste them inside its own file (the varaiables.tf file) .....********** the is the process called REFACTORING******** 
SO THAT it is easy to read , 
am making sure i only have resources in one file
providers in one file, so now ill create a file for providers ........................     25.03

STEP 7)      variables.tf file 

varaiable "cidr" {
    type  =  "string"
Default   = "10.0.0.0/16"
}

varaiable "tags" {
    type  =  "string"
Default   = "Demo-vpc"
}


STEP 8)      remove the provider from my vpc and put it on its file
  provider.tf

provider "aws" {
   region = "us-west-1"


******************** so bc i have declared my provider and variables and vpc on their own file , so this in itself and bc its inside of the vpc dir 
so i can call this as my vpc module bc everything in this vpc dir has to do with jsut the vpc only
so i can go ahead and create the vpc first
bt before i create the vpc, i realise that in my code (resourec block) to create my instance
i need a certain subnet
bt ill wnat to get the subnet frm this vpc that am craeting 
now since this vpc is a module on its on, for u to get something out of a module you must create an output, 
there must be an output for you to access the attribute of the module
right now we are creating this vpc but we do not hv any output, so ders no way of accessing the attributes of the vpc

9)             CREATING THE OUTPUT BLK FOR THE VPC id INSIDE THE VPC FILE
so for us to be able to access the attributes we will need to craete an output 


vpc.tf file

resource "aws_vpc" "main" {
   cidr_block = "var.cidr"

tags = {
   Name = " var.tag"}
 }
}
     NO.9                                 **VPC id, OUTPUT BLK
output "vpc_id" {                                   ************so this output will return the vpc id 
  value = "aws_vpc.main.id                              so am using an output to access the attribute of that value (an attribute of the vpc)
                                                         so i have to declare this particular output

28:25    ********** havin created that particular output, i probably want to create a subnet/subnets inside of this particular vpc 


10)             HOW TO CREATE A SUBNET 

 in the registry he copied the resource block and pasted in the vpc file 

resource "aws_vpc" "main" {
   cidr_block = "var.cidr"

tags = {
   Name = " var.tag"}
 }
}

output "vpc_id" {                                 
  value = "aws_vpc.main.id 

}    NO.10                                   SUBNET RESOURCE BLK
resource "aws_subnet" "main" {             ****** am creating a reosurce called a subnet,i wnat to craete the subnet inside this vpc,  so ill nid to pass this vpc id
    vpc_id   = aws_vpc.main.id                       (aws_vpc.main.id) this will return the id of the vpc,  so i need a vpc id where the subnet will be created
    cidr_block = "10.0.1.0/24"             ********as we looked at d varaiable, my vpc cidr 10.0.0.0/16, so my subnet cidr has to b a subset of that, so we can leave it at 
                                                                                          this 
                              
tag = { 
  Name = "Demo - subnet"
 }
}

11) ***********   CREATING VARIABLES for SUBNET Cidr & Tag again  ...... so i can pass this as varaiables 
   now i can create 2variables (this is again refactoring)

varaiable "sub_cidr" {
    type      =  "string"
    Default   = "10.0.1.0/24"


************* variable for the subnet tag
     varaiable "sub_tag" {
    type      =  "string"
    Default   = "Demo-subnet"

 11a)    ************************    31:44
    Referencing the variable for the sub_cidr  and subnet tag 
 ( he usually just goes back to NO.10 and refernce it by doing the changes e.g "10.0.1.0/24" to "var.sub_cidr"


resource "aws_subnet" "main" {             
    vpc_id   = aws_vpc.main.id                       
    cidr_block = "var.sub_cidr"         ******************referencing the variable sub_cidr

tag = { 
  Name = "sub_tag"         ******************referencing the variable sub_tag

       so we have refactored our code 
    

     33:05  *********** 
now he also removed the variables for the subnets from the vpc file and paste them in the variables file 
so all the variableS are in one file 

also, for the output, he created a file , output.tf and pasted the output block

         
N0.12)      ****************************
     I ALSO NEED to create and output for my subnet, i need the subnet id bc when am creating this instance the argument that is passed is subnet id  so i need the id 
how do i get the subnet id 
  if i go to my vpc file, this is my subnet :aws_subnet" "main"

     output "subnet_id" {                                 
         value = "aws_subnet.main.id            ************ i want to get the id of this subnet after it has been created
   

NO.12a    ******************  now ill remove the output for the subnet id and paste it inside the output file 
  so ill get my output for my vpc and my output for my subnet

# VPC
    output "vpc_id" {                                 
         value = "aws_vpc.main.id 

  # Subnet
    output "subnet_id" {                                 
         value = "aws_subnet.main.id 

    
36:32   ************** so hv created this 2output for this 2resources
      with this am able to access these 2 output , bc ill need this subnet id to use it when am creating my resource


13) NOW THIS IS HOW OUR VPC FILE LOOKS LIKE

   resource "aws_vpc" "main" {
   cidr_block = "var.cidr"

tags = {
   Name = " var.tag"}
 }

}    
resource "aws_subnet" "main" {             
    vpc_id   = aws_vpc.main.id                       
    cidr_block = "var.sub_cidr"         

tag = { 
  Name = "sub_tag"
tag = { 
  Name = "Demo - subnet"


NO.14
 36:32   ********************      IMPLICIT DEPENDENCY 

      now let me go to my vpc and create this vpc first
by looking at the output which resource do you think will be taking first ?
 
38:41 
ANS: this is what we talked abt implicit dependencies 
so trefrm is able to look at your resources and determine which resources should be created first bc of implicit dependencies
but if we wnat to make it dependence explicit,  we use depends on
if  we ant to make our explicit dependency,  then we can pass a depends on meta argument here
we will say that the subset depends on the vpc and  bc we v explicitly declared that, it just means the vpc wil have to be created first  bc the subnet depends on it

  ******** but bc of how the subnet requires a vpc id, so this inherently is implicit dependency
                        so this is an example of implicit dependency in terrafrm 

so just understand the difference

   ALL THESE ARE INTERVIEW QUESTIONS
WHAT IS IMPLICIT dependency... ?
what is explicit dependency   ?
how does trfrm achieve implicit dependency  ... ?    it is just by how resources depends on each other 
how does trfrm achieve explicit dependency   ..... ?    you have to pass a dependency 


      *********** now that we have this, we can go ahead and create our vpc 



NO.15                        LOCAL BACKEND 

40:43  in mobaxaterm, watch how he arrived at the vpc dir  by running cd command
inside vpc dir 
run terrafrm init   .....  to initialize the backend  
this backend that is initializing , its downloading pluggins and providers, basically what its doing, its downloading those particular providers pluggins in your local env
when that is downlaoded in ur local env, this becaomes a local backend 
inside of my vpc we see that it created for us this .terrafrm dir, it downloaded frm the registry, harshicop registry, it downloaded your aws provider, that is the version
of your provider 
                           
                             .TERRAFORM DIR

so that is the purpose of the .terrafrm dir, it downloads your provider pluggins and if u hv any module but it also generates for you this file called terrfrm log file
this is a log file that shows the provider pluggin that was downloaded
this ae the harsh keys of that particular provider/pluggin that was downloaded
 ***** so thats what terfrm init has done

NO.16

trfrm validate 
the configuartion is valid 

terrafm plan
we see the resoureces that wil be created
subnet and vpc 
we see the information (cidr block, tag) we passed as a variable has been extrapolated from the variable and interpolated into the resource 

terrafr apply -auto-apporve
it wil re run the plan first
then creates the vpc 
then crates the subnet bc the subnet depends on the vpc
once that is done
it also output the subnet id and vpc id, 
this is bc we declared an output 


46:50
 NO.17 ***********      BECAUSE IT HAS RETURNED THIS INFRORMATION, HOW CAN WE NOW USE THIS INFORMATION IN PROVISIONING OF OUR INFRASTRUCTURE

                   tERRAFRM.tfstate
  IN this inf that has returned, it has also generated a statefile 
the atatefile has the resources that were created
we see the specs too ( tag, attribute)
now if u had any sensitive inf bc as we said trfrm wil spit out everytin in plain text
so bc of that we seee that there is no senesitive inf that was passed so we dnt have anything to worry abt the vpc
     so this is our statefile

************* we want to get this inf,  we wnt to be able to read this inf and get our id for our subnet inorder for us to provision our ec2 instance 


   NO.18
48:11       HOW DO WE DO THAT???


   ***online..    REMOTE STATE DATA SOURCE
In Terraform, a remote data source, specifically the built-in terraform_remote_state data source, allows a configuration to retrieve output values from the state file of 
another, separate Terraform configuration.
This mechanism facilitates the modularity and decoupling of large infrastructure projects into smaller, manageable components, even when different teams manage them. 
Purpose and Use Cases
Sharing Data Between Configurations: A common scenario is when a core infrastructure team manages the network setup (VPC, subnets, security groups) in one configuration,
and an application team needs those specific VPC and subnet IDs to deploy their application in a separate configuration.
Modular Infrastructure: It allows you to break down your infrastructure into logically separated parts (e.g., a networking project and a compute project) without managing 
everything in one large state file.
Avoiding Hardcoding: Instead of manually copying and pasting IDs or names between projects, the data source dynamically fetches the current values, ensuring consistency and 
reducing errors. 

How It Works
To use terraform_remote_state, you must first ensure that the source configuration's state is stored in a remote backend like AWS S3, Azure Blob Storage, or Terraform Cloud. 
You then define the data source in your current configuration using a data block, specifying the backend and the location of the source state file: 


                                             REMOTE STATE DATA SOURCE
 TRFRM gives you an option of a data source, u can use the data source to read a statefile called remote state data source
 CAn go to the documentation:
The terraform_remote_state data source uses the latest state snapshot from a specified state backend to retrieve the root module output values from some other Terraform 
configuration.

 so we can use the datasource to read our statefile and once that is done we are able to get the info that we need frm that particular statefile

for example:
in your work env as an engr u might be asked to provision
your netwrk will always be to provison first   *** i bliv he means the custom vpc has to already exist in Aws ie your vpc is already set so that you can provison the instance
so your vpc wil be set, u go to work and your vpc has already been set 
bc your vpc has been created, ur told can u go and provison an ec2 instance in this particular vpc

  ********* NOW HOW DO YOU DO THAT ??????
you need to be able to read the statefile that manages bc evrytin is bein managed by terefrm
so u need to be able to read the statefile inorder for you to return the subnet, to get the attribute thata re captured in that particular statefile.

*************** bc this statefile is on your local env, it is local, so this backend wher terfrm is doing its operation is called a local backend
bc evrytin is on your local env 
therefore for us to read this statefile, we are going to , he closed the vpc dir 



NO.19           INSIDE THE DATA SOURCE FILE
*********51:18   .... opened  .. data.tf file 
       AM using this data source :it is called "terraform_remote_state, 
this is the name that you must pass, this is the data source that is used to read statefile  
then i can give it any name, it doesnt matter, this is just a name (vpc) that wil be unique to terfrm

 ****** so if am calling this remote state called vpc, bc its goin to read the staefile of my vpc

data "terraform_remote_state" "vpc" {
  backend = "local"                                ********* the type of backend am using is local 
  config = {                                       what is the configuration ******** config here is the path, i need to tell tefrm the path wher to find that statefile
       path    = "../vpc/terraform.tfstate"         ******** the data.tf is here , bc the vpc dir/file is in the same dir as the data.tf file , am goin to give it a path
                                                        so it needs to go into a dir called vpc , while inside ther,you wil find a statefile called tefrm.tf
                                                     it is in the Present dir of a dir called vpc n inside of that particualar dir there is whta we call a terrafrom.tf state

 ************** so am using this datasource to be able to read this particular statefile
but once i read the statefile , am interested to get an output that was declared inside the atatefile, am interested toget an output of a subnet id

  54:08
********************************
     inside my instance resource block
main.tf  (the same file as NO.2 above)

resource "aws_instance" "my_ekl_instance" {
ami           = data.aws_ami.amzlinux2.id                            
instance type = var.myInstance                                                      we are passing the subnet id:
subnet_id        =data.taerraform_remote_state.vpc.outputs.subnet_id       **** my subnet id wil b read frm the remote state data source (vpc) n in the statfile, am      
key_name         = var. my_key                                      interested in d output, we always refer an output by its name (subnet_id) then trfm wil return the value
                                                                         so we are reading this subnet frm the statefile (frm our vpc module)

       ********* we are creating our resource inside of a custom vpc, our netwrk already exists 
  so we are creating this instance inside of this subnet id 

NO.20
***********now we are also passing a key 
lets say we hv this particular key, we wnat to pass the key, for us to pass the key 
if i look at my variables (No.3 above) i have passed my key (elk-instance)
again as we said last time, whatever key you pass needs to exist in the particular region u trying to creating your resource

    resource "aws_instance" "my_ekl_instance" {
ami           = data.aws_ami.amzlinux2.id                            
instance type = var.myInstance [1]                                                 
subnet_id        =data.taerraform_remote_state.vpc.outputs.subnet_id            
key_name         = var. my_key                         *************** key we are passing 



58:30 **************Also in NO.3 above (variable file), WE have a list of default values (t2 micro, t2 medium)
    if  we are to create a t2 medium, how can we reference it???
by indexing it, we are reading it frm a list:


NO.21
 **********  in the main.tf file, we are also passing a security grp , this is also an argument that we can pass 

vpc_security_group_ids = [
  aws_security_group.elk_sg.id,   ********** bc we nid this sg id, so it means this sg has to b created first before our instance can be created dats why we ar pasing
 ]                                                    depends on, if the instance creates first before the sg then the instance wil nt bootstrap well
]                                                    thats why we are doin an explicit depsncy so that terfrm orders hw d resourecs wil be created

  depends_on = [aws_security_group.elk_sg]    * we are also passing an explicit dependency using depends on  

NO.22
************ also, we are creating an elastic ip , an we wil attach it on our instance, so our instance wil v an elastic ip, it wil nt just be automatically assigned a
public ip bc we dnt wnat the ip to change so we are creating an elastic ip 

#EIP
resource "aws_eip" "ip" {
  instance = aws_instance.my_elk_instance.id     ....this is the instance id we are associating it with 

******** A backend is basically where terfm operates from, wher trfrm coammnds are executed frm, wit this particualr backend, we hv backend on our env so therefrer  
this is what is called a local backend , we are goin to refernce this in our code also we ar goin to have a provisiner, once we create that instance


 
NO.23   ******************************************************
null-resource.tf
#create a null resource and provisioners

 resource "null_resource" "my_null_resource" {
   depends_on = [time_sleep.wait_for_instance}       *********** the null resource depends on the time sleep (NO.26)
 # connection Block for provisioners to connect to the ec2 instance

connection {                        ********** we will connect to the instance by using ssh,we wil ssh into the instance
  host  = aws_eip.ip.public_ip     *********bc it is elastic, it wont change, it wil be d same ip that wil b attached on the instance, we wil us the elasic aws ip we cretaed
  type  = "ssh"             ** normally we wil pass a key to ssh into an instance, its d same tin we ar doing here,trfrm converted it to connection blk wen usin provisioners            
  user  = var.instance_user                                        e.g of the normal convention: ssh -i key ubuntu@76.89.56
 private_key = file    ("${path.module}/mykey/elk-instance.pem"}
                                      e.g of the normal convention: ssh -i key ubuntu@76.89.56
                                    ********* this is the user we ar connecting to : ubuntu@76.89.56, and our key is this particular key that we are using: ssh -i key
}                                       *********** the protocol we are using, the type is ssh     
                              terrfrm has converted this: the normal convention: ssh -i key ubuntu@76.89.56,  into connection blocks when ur using provisioners
provisioner "file {
   source    =  "script/elasticsearch.yml"
   destination = "/tmp/elasticsearch.yml"
}

provisioners  "file" {
   source  =  "scripts/kibana.yml"
   destination = "/tmp/kibana.yml"
}
provisioners  "file" {
   source  =  "scripts/apache-01.conf"
   destination = "/tmp/apache-01.conf"    **********  temporary dir
}
provisioners  "file" {
   source  =  "scripts/instalLELK.sh"
   destination = "/tmp/installELK.sh"
}
provisioners  "remote-exec" {
   inline = [
      "chmod +x    "/tmp/installELK.sh",     (i wnat to give executable permison to that script)
      "sudo sed -i -e 's/\r$//' /tmp/installELK.sh", #Remove the spurious CR characters. ( i wnat modify some chracters using some set commands)
      "sudo /tmp/installELK.sh",        ********** then i wnat to run the script
}

 1:04:16
    *********** in this particular case, we are also using what is called a null resource




   INTERVIEW QUESTION
 What is a null resource ??
this is just a place holder, its a dormy/empty resource, it doesnt create anything.. bt we can use it to enable us establish our connection n we ar connecting to this 
particular inastance we are creating :aws_eip.ip.public_ip

now when ur creatin g this particular instance, once we hv connection with it, we want to provision, we wnt to transfer a/some file , we hv some scrptt, the source is inside 
this dir called script, i wnt to connect to that instance and transfer this file 
the first file i want to transfer is elastic search
the nxt is kibana
3rd is apache configuration
4th is inatallation scripts  to install ELK 

              
              INLINE    ************* Explanation of the scripts we are transferring to the instance 

BUT the destination, its been transferred into an instance in a temporary dir, (tmp/kibana.yml) wit those particular names, so the names wil nt change, so once my instance
has created the provisioner will take over , this null resource wil be created to enable us connect to the instance and once we connect it wil transfer this files that ar in my 
local env to the instance , afer whic i wil use an remote exec provisiober , this is a provisioner that now runs commands on ur remote swerver, it wil run trfrm commands on 
the remote server, am goin to pass inline and LIline , basically just means the commands i wnat to be run 
so i wnat to give executable permison to that script then i wnat modify some chracters using some set commands inside of the script
then i wnat to run the script
once i run that particular script, this script 
it will do apt update
it will install elastic search, start it and we can access sit on port 9200
install logstash
kibana and start it 
install metricbeat n start the service            *************1:07:00
then it will start the service of logstash
this script once it 


****** this script once its transferred into the instance, the provisioner will run the script inside of that to be able to provison my ELK env
1:09:03
*****
i have my .pemkey compied here in his (#mykey file ), that wil be used for connection 
and obviously when ur using this code u hv to replace this key with ur own key or the code will nt wrk 
the key also exists on my AWS account... juts mk sure u copy the content of ur own key and replace it insid eof this dir
bc now how am referencing it, when u look at your null resource file 
private_key = path.module}/mykey/elk-instance.pem
am saying the connection , . private key is within the module, in a dir called my key and its called elk-instance.pem

*******and the user we want to connect to, I hv pass it as a variable in the variable file and the user/instance user is ubuntu

1:10:00
NO.24
****** and am also creating an output, its goin to output the public and private ip of the instance 
      (this file is diff from N0.12a , the vpc file for vpc output)

     output.tf
output "public_ip" {
 value = aws_instance.my_elk_instance.public_ip
}

output "private_ip"{
   value = aws_instanec.my_elk_instance.private_ip
}

NO.25
provider.tf

terraform {
  required_version = "~>  1.0 "
  required_provisioners {
    aws = {
       source = "harshicorp/aws"
       version = "~> 5.0"
    }
 }
                                  #### we comment the backend for now bc we havent used a remote backend yet 
# backend "s3" {
   bucket  = "landmark-automation-kenmak"
   key   =  "ec2/terraform.tfstate"
   region = "us-west-2"
 }
}
        let me declare my provider here
   #let me just put my provider 
provider "aws" {
   region = "us-west- 1          ************ am choosing this region bc this is where we created our vpc, we must be creating them in the same region
}

    ## so with that i have my vpc and i aslo hv my instance that i wnat to craete 


1:11:47
NO.21 .....  contd
 am also creating a security grp 
ill expalin to you whats goin on here

      sg.tf


bt here we are jst creating a security grp and we are creating it dynamically using for each , s
o this for each we hv created a locals blocks here n on the local block, we have declared an ingress config , which is a list, 
everytin is inside of this bracket, it is a list and inside the list we hv put whta we need to pass for elastic serach, the cidr blocks, the port, the protocol n description
and what we need to put for logstash and kibana as a map
so this is a list of maps , u see all these calibre,  this is a list of maps 
so we are usng for each to loop through
so for each of this map it is goin to loop through using the local.ingres 
as it loops through the description of the content will be this ingress which is calling this for ecah but we need the value
remember that for each uses each.value or each.key 
again, ill explain this when we are dealing with functions 
bt for now its just looping through all this ports then it wil dynamically create the security grp 
bc otherwise ill need to writebalmost like 100 lines of codes for each of this description/security grp
but now instaed  hv declared them as locals and am just looping through using for each meta argument and it will loop through this and create for me this ingress 
but then the ingress will remain the same 

so thats on the security grp


      1:14:36
NO.26
  ***********    tIME SLEEP
time slip is a resource that jsut runs time, its jsut like a counter,its jst counting down,i know taht when i create my instance, my instance wil take some time to bootstrap
for ssh connection to be available, if i run terfrm once my instance is created, if i try to ssh into it chances are the ssh might nt b up and so my connection wil fail so
bc of that am passing a time sleep resource just to wait for the instance to bootstrap first
hv passed 180s , this is abt 3minuutes, so once i start it this time sleep depends on the instance ie onsce the instance is careted then the time sleep wil run
have passed an explicit dependency , it depends on my instance , 
bt for the connection to happen, the null resourece depends on the time sleep 
so it means am ORDERING HOW  terfrm is goin to create the resources so it just means the instance wil be created first
then the timesleep , ther wil b time dealay of 3mins fo rthe instance to bootstrap after wich then my null resource wil start creating 
it can now ssh into it and provision the resources


  sleep.tf

 resource "time_sleep" "wait_for _instance" {
  create_duration = "180s"

  depends_on = [aws_instance.my_elk_instance]




#########################################################################################

   *********** 1:16:45 
let me now go into my env bc we have seen hw everytin is been configured and 
lets now create our resource
lets see how we can create our resources. 

***************1:17:00
now in i think mobazterm 

cd to the tf-file-provisioner 
terrafrm init 
trfrm validate
trfrm format   .... now all my codes hv been reformatted correctly 
trfrm plan 

the first thing that happens is the datasource , terAfrm remote state ,
it goes and read the state file of the vpc
data.terrafrom_remote_state.vpc: Reading
then
the data source for the ami , it goes and read and find the AMI and it returns the AMI id 
data.aws_ami.ubuntu: Reading 

once it does that what is been created is my elastic ip 
my instance the ami has been returned , that is the ami that will be created
the instance type t2 medium
the key name  elk-instance
its also returned our subnet id , ( ends wit 4e20)
if i scroll bk up where we created this 
output:
subnet_id = "subnet- ........ 4e20)                         .......... 1:20
so we hv used the datasource to read the statefile n return for us the subnet id wher we wnt to create our resource n after that the subnet id has been dynamically passed 
here and we knw that our resorce will be provisioned in that subnet 
we see the security grp
the null resource 
the time sleep

********* terrafrm apply -auto 


           **************************** 

NO.27
1:21:00
key refused
Not found key pair doesnt exists 
am trying to connect but bc i just changed my region to west, this keypair doesnt exist in west (in video 3), the keypair doenst exist in west
so lemme login in to my aws console n see what key pair i can use 
so he went to AWS for a diff key and change the name in variable file 
in mobaxterm, he jsut used the cp command to copy the key frm his downlaods to the pwd 
afterwhich we see the new key file in terrafrm and inside the file , the key content has been copied

       tf-file-provisioner cd mykey
       mykey cp -/Downloads/ansible-infra-test.pem
       mykey ls 
 ansible-infra-test.pem elk-instance.pem          ********** i can seee my key 

tefrm plan

terfrm apply

the first tin is creating ur instance bc w specifically passsed our depsndecy
then the time sleep bc it despend son the instance, it wil tk abt 3mins to provision
bt the API can create in parallel bc it doesnt hv any dependencies
bt the API needs the instance id and thats an implicit dependency
right now the time to slepe is happeing 
so if we go to our intances 
in this region we see thers an instance thats initializing
so this is the time that we are buying until our instance fully initializes b4 we can ssh into it
its goin tk abt 3mins to b on the save side , so its counting down 
once that is done the null resource will start proviosning 
bc the null resource depends on the timesleep , so the timesleep has to create first
after that creates the null resource will create 
it will do an ssh connection and transfer our files and start the configuration 
so right now what we are doing is provioning but once the null resource connects we now start doing configuration
we are configuring our instance by installing kiban, ELK, elatic serach on that particular instance
w eare using terfrm , we are using it with proviisoners 
provisioners are been used to do configuration
so this is another way we can configure our inatnces 
so we hv seen u can usee user data, which will just bootstrap 
but then if u want to ssh into an instance and taranser files from your local then it means we are goin to use a provisiner 


NO.27b
******** we have error : EIP

1:35:00    
                                           QUESTION
1)i was facinated on how you did the.pemkey cos i was wondering how we wil go and chabge the key bt thi sis really good
my quetsion is do we first go and create the key manually from our console is that what u did or is there a way we can automatically get the key pairs

ANS:
what v shown u is that that key already exists in AWS
 SO U can either do it either of the ways, u can either go on the console link a key that exists or u can create an ssh key
these keys that we use are jsut ssh keys 
so you can genertae your own ssh key on ur local env as long as you have your private and your public 
then in terfrm u are goin to use  , terfrm gives you everytin, u wil use AWs_key_pair (in google in typed it to go to documentation)
you wil use a resource called a key pair

                  (copied frm documentation)
 
    resource "aws_key_pair" "deployer" {       (you pass this key pair resource)
  key_name   = "deployer-key"            (you wil give it a name)   bt ther is a public key, so u wil copy d public key u generated using d ssh key gen command &paste 
                                          it here as a string, once that is done n u run/create this resource, dis key wil be ........
  public_key = "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD3F6tyPEFEzV0LX3X8BsXdMsQz1x2cEikKDEY0aIj41qgxMCP/iteneqXSIFZBp5vizPvaoIR3Um9xK7PGoW8giupGn+EPuxIA4cDM4vzOqOkiMPhz5XK
0whEjkVzTo4+S0puvDZuwIsdiW9mxhJc7tgBNL0cYlWSYVkz4G/fslNfRPW5mYAM49f4fhtxPb5ok4Q2Lg9dPKVHO/Bgeu5woMc7RY0p1ej6D4CKFE6lymSDJpW0YHX/wqE9+cfEauh7xZcG0q9t2ta6F6fmX0agvpFyZo8aFb
eUBr7osSCJNgvavWbM/06niWrOvYX2xwWdhXmXSrbX8ZbabVohBK41 email@example.com"
}


********   bt ther is a public key, so u wil copy d public key u generated using d ssh key gen command &paste it here as a string, once that is done n u run/create this 
resource, dis key wil be sent into ur instance as ur public key but when you use ssh key gen , u wil get both the public key and the private key so in ur env u will be left
with the private but this public key wil be sent to ur instance then u will use this private key to authenticate with the public key that uve already sent into ur instance
then if u want to do it dynamically then u can use this aws key pair to automatically generate your keys but once you do that ull ned to generate the ssh key frm your local
env then u can send it 
so thats the 2ways that you can send it , either generate it on the console manually then just copy the name , download the .pem key, download it on your env then u can do 
that or you can dynamically use this resource to generate the key 

 
2)  1:41:00
       QUestion on referencing values in a map 

 in this variable file, he change dthe dfault to a map 
      varaiable "map" {
    type = map(string)
    default = [
      "dev" = "t2-micro"
      "stage" = "t2-medium" 

then in the resource blk )main.tf) , we pass the key : stage  and it will reyurn the value 

                       
                                   TERRAFORM CONSOLE

************how you can easily confirm that in terafrm there is what is called terrafrm console
       once you run that it gives a prompt >, helps you to quickly test to know what the output are 
       i can use terraform console to test my expressions before i actually commit my code 

    > var.map["stage"]          ************ so am testing what its goin to return for me 
  "t2.medium"          ******* the output
                              ie, this function:var.map["stage"]    evaluates  to t2medium

 > subnet_id  = data.terraform_remote_state.vpc.outputs.subnets.subnets_arn
  "it returns the subnet i"

    ********** so i can use terraform console to test my expressions before i actually commit my code bc i want to know what its gonna return
  with thi sma able to evaluate/check my function

   1:47:00
ALso,                          USING A LOOP   (showing you how to use functions)

                              (main.tf)
         resource "aws_instance" "my_ekl_instance" {
ami              = data.aws_ami.amzlinux2.id  
instance type  = [for s in var.myinstance : lower(s)][1]  ******** for is a loop, (s) is jst an abitrary number , i can use a,k etc 
#instance type   = var.myInstance [1]                                                 
subnet_id        =data.taerraform_remote_state.vpc.outputs.subnet_id            
key_name         = var. my_key
    

         showing you how to use functions)

            varaiable "myInstance" {
               type = list(string)
               default = ["t2.MICRO" , "T2.MEDIUM", "T3.MICRO"]    ******** usually in AWS this will give an error bc everytin needs to be in lower case 
              


*******  for s in var.myinstance : lower(s)][1]
 so this means that for , for any element in this variable (var.myInstance) am passing a function ; lower then the value of (s) 
so for every value in this instance that value theres a function whatevr is in the right of the column side, there is a lower function that is goin to be applied
so if the variable is capital "T2 MICRO" , then am goin to apply a lower function ie it will be converted into a lower case 
and when it converts it am goin to select the first instance [1]  ie t2.medium


     ******* now test the function 

   > for s in var.myinstance : lower(s)][1]
     "t2.medium"

therefore, this function evaluates to t2.medium

   > for s in var.myinstance : lower(s)][0]
     "t2.micro"

########   buT 
                  when we try 2 we get error 
  > for s in var.myinstance : lower(s)][2]
     in valid index



   *******so am just showing you that we can use a function to convert
this is been proactive as an engr to mk sure in future if someone who isnt an expert is e.g creating an s3 bucket name and we knw that by convwntion AWS doesnt allow capital
letteers in s3 bucket names and your instance type need to be lower, doesnt allow capital letters so bc of that u become proactive and making sure that whatever a user 
passes you are using a function to convert, so eben if thay pass a capital, you code will nt fail becauase am using a function to ensure that i collapse it/convert it to
lower so that once its converted its goin to read and pass the code
 thats exactly what this function was doing


1:57:20
  *********** contd  from
  NO.27b
******** we have error : EIP
************* during our break now illmodify my vpc code , ill share my screen but it basically wont be recorded
bc i just created a basic vpc bt runing terfrm apply we got an error, 
it looks like it needs an AN for a network for the vpc to attach, so am going to add more code to give us a more complte vpc
then once we do that we will us ethe same datasource to create our instance


   QUESTION
1) i can see in the variable file you have key name as a variable then in the null resource, why didnt we use the variable  when we were creating the ssh connection??

   ANS:       *********  bc we are using a file
 connection {                        
  host  = aws_eip.ip.public_ip     
  type  = "ssh"                      
  user  = var.instance_user                                      
 private_key = file    ("${path.module}/mykey/elk-instance.pem"}      ************ we are using a file , when you read at the function, u hv to give it a path to where the
                                                                    file is, it has to be either relative or absolute path, file function wil nt extrapolate like variables
                                                                 so u hav eto give it either relative or absolute path 


        WHAT IS THE part.module 

  ANS: 
i explained taht on saturday
its just a terfrm way, the syntax that they use saying that this is within the path of the module 
where we are here, the part of your module is your pwd , so when u evalute this (path.module) it always returns your pwd 
(path.module) this is your pwd 
 so u can easily pass as  :   file = ./mykey   .... bc it will evaluate to the pwd
OR u can pass it as : file = mykey/ansible

  ***************all these are the same 

   ********* but according to the syntax, harshicop configuration lanuguage they prefer to use this, path to the module (path.module) then the location wher rthe file is 
/mykey/elk-instance.pem"




  when we return frm the break, we wil look at our backend 
we will create this 
we v already seen what a local backend looks like 
we have benn able to read the statefile here
but once we come back, 
once we create this ec2 module , we will create a backend remotely  and we will move our statefile to the remote backend and see how we can read a statefile from a remote 
backend 


   


#########################################################
   
           CLASS CONTINUES

        Contd ... frm 1:57:20    above 
2:09:25
   we had ceated a vpc but because the vpc was very basic the only thing we created was basically vpc with one subnet 
  bc of one subnet it was failing so i went ahead and updated my code 

so now we have a new dir (my-vpc) with a new file for our vpc (vpc.tf) and the old dir (vpc) with the file we wont use bc of basic configuration (vpc.tf)
also , there are other new file in the new dir ( variables.tf, provider.tf, output.tf)
*********** and dont worry about the code  you will have the code and you will have all this code and will have time to go thru all this 

     2:09:40
   
   NO.28
new vpc.tf

    resource "aws_vpc" "main" {                ************ we are creating a resource vpc
   cidr_block = "var.vpc_cidr"

tags = {
   Name = " my-vpc"
  }
}  

#2  create IGW                        ******** when u craete a vpc u nid to craete an IGW for your vpc to be able to  have access  to internet, 
resource "aws_internet_gateway" "igw" {         
     vpc_id =aws_vpc.main.id

     tags = {
        Name = "igw"
  }
}

#3  create EIP                            ************** elastic ip that we will use for our natgateway, this is like a 3tier kinda infrastructure 
   resource "aws_eip" "nat" {   

      tags = {
         Name = "nat" 
    }
   }

#4  create NAT gateway                     
 resource "aws_nat_gateway" "nat" {
  allocation_id = aws_eip.nat.id                        ************* once this is created we will allocate our elastic ip to the natgateway 
  subnet_id   =  aws_subnet.public[0].id

     tags = {
       Name = "nat"
}

 depends_on = [aws_internet_gateway.igw]         **************** the natgateway depends on igw
}

#5 create private subnet                   ********** dnt worry abt this code, am uisng a lot of functions to make it very comfortable
    resource "aws_subnet" "private" {                                                              
    count              = length(slice(var.private_cidr, 0, var.private_subnet_count))   ***** count meta argument bc i wnt to knw d number of private subnet i wnt to create
    vpc_id             = aws_vpc.main.id                                           
    cidr_block         = elements(var.private_cidr, 0, var.private_subnet_count)), count.index)
    availability zone  = element(var.availability_zones, count.index)

    tags = {
      "Name" = "private"
}

  2:12:00 

   ****** am using a count meta argument bc i wnt to knw d number of private subnet i wnt to create
   and am using a slice function bc its reading my private cidr
when u look at what a slice functions does, it reads
when you give it an input like a variable and you give it like the starting index, i wnat it to start from index[0] when i give it a list then what is the end index?
so that slice is smt similar to 
e.g
 i have a pile of multiple elements, so frm the slice perspective, i just want to get a slice of this pile
i hv 9 cidr blks , now am making this flexible bc i want to get a slice of the cidr blocks, i dnt want to hv to use all cidr 
so am saying begining from the first slice on index 0 to a certain count , mayb i want 2, so frm o to 2nd slice
so it is going to slice the pile to the portion of the 2nd cidr and return just that portion as the length
that means the count will be 2 bc , although i have given it a pile of 9cidr blocks  but iwant it to start from 0 and go up to 2
so , 0 i sthe first index but i wnat 2 slices , ie 1 & 2
so count wil return this 2slices and that will mean that i am creating 2 subnets inside of that vpc   :
   create private subnet                  
    resource "aws_subnet" "private" {                                                              
    count              = length(slice(var.private_cidr, 0, var.private_subnet_count))

*********** so the same thing will happen for the element bc am using count, so it returns some element from that particular list 


#6 create public subnet 
resource "aws_subnet" "public" {                                                              
    count              = length(slice(var.public_cidr, 0, var.public_subnet_count))   
    vpc_id             = aws_vpc.main.id                                           
    cidr_block         = elements(var.public_cidr, 0, var.public_subnet_count)), count.index)
    availability zone  = element(var.availability_zones, count.index)

    tags = {
      "Name" = "public"



#7 create private route table          *********** am creating routes within my vpc 
 resource "aws_route_table" "private" {              route for private 
    vpc_id = aws_vpc.main.id

  depends_on = [aws_subnet.private]

  tags = {
    Name = "private"
  }
}

#8 create public route table
resource  "aws_route_table" "public" {     
  vpc_id = aws_vpc.main.id

   depends_on = [aws_subnet.public]

  tag = {
    Name = "public "
 }
}

#9 create public routes                               ********also creating routes within the route table
resource "aws_route" "public_internet_gateway" {

  route_table_id         = aws_route_table.public.id    ******** for this route table that is public the destination is all:"0..0.0.0/0" , bc its public then its goin to igw 
  destination_cidr_block = "0..0.0.0/0"
  gateway_id             = aws_internet_gateway.igw.id         ****** so all traffic to igw

  depends_on = [aws_route_table.public]    ************ it depends on the public route table above (NO.8)
}

 #10
  pivate routes
resource "aws_route" "private_internet_gateway" {             ******* then for the private route table am using the nat gateway, 

  route_table_id         = aws_route_table.private.id
  gateway_id             = aws_internet_gateway.igw.id       ****** all traffic should point to the nat gatway 
  destination_cidr_block = "0..0.0.0/0"
  
  depends_on = [aws_route_table.private]               ******  it depsnds on the private (NO.7)

                                           
*************  2:17:07


#11 private route association                *********** we hv to associate our subnet with 

  resource "aws_route_table_association" "private" {
     count              = length(slice(var.private_cidr, 0, var.public_subnet_count)) 
    
     subnet_id           = elements(var.private_cidr, 0, var.private_subnet_count)), count.index)
      route_table_id         = aws_route_table.private.id                                          ****** associating it with the private route table 


      depends_on = [aws_route.private_nat_gateway, aws_subnet.private]
}

#12 public route association

    resource "aws_route_table_association" "public" {
     count              = length(slice(var.public_cidr, 0, var.public_subnet_count)) 
    
     subnet_id           = elements(var.private_cidr, 0, var.private_subnet_count)), count.index)
      route_table_id         = aws_route_table.public.id                                              ****** associating it with the private route table 


         depends_on = [aws_route.public_internet_gateway, aws_subnet.public]
   

   ########### now this is a complete vpc that am creating 




2:15:00 **********************

 NO.29
     ********** variable.tf
                             ********* in my varaibles, i have all these cidr blocks, am defining this ....

    variable "private_cidr" {     
       type = list(string)
     default = [
          "10.0.1.0/24"
          "10.0.2.0/24"      ** but i just want to create subnet in just these 2, so am making it flexible if i need to make more subnets then i can just increase my slice
          "10.0.3.0/24"
          "10.0.4.0/24"
          "10.0.5.0/24"
          "10.0.6.0/24"
          "10.0.7.0/24"
          "10.0.8.0/24"
  }
}
variable "availability_zones" {
   type    = list(string)
   default = ["us-east-1a", "us-east-1b"]
}

 variable "public_cidr" {
       type = list(string)
     default = [
          "10.0.101.0/24"
          "10.0.102.0/24"
          "10.0.103.0/24"
          "10.0.104.0/24"
          "10.0.105.0/24"
          "10.0.106.0/24"
          "10.0.107.0/24"
          "10.0.108.0/24"
 ]
}

variable "vpc_cidr" {
   type    = string
   default = "10..0.0.0/16"
}

 varaible "private_subnet_count" {         ********* my private subnet count am just creating 2
   type   = number 
   default = 2
}

 variable "public_subnet_count" {    ************** my public subnet am just creating 2
    type = number
    default = 2
}

 
  ############### **********   NO.30
                    provider.tf

provider "aws" {
   region = "us-west-1"
}

  
terraform {
  required_providers {
    aws = {
       source = "harshicorp/aws"
       version = "~> 5.0"
     }
   }
}

  

 NO.31
   output.tf
output "private" {                                  *** i want to return my private subnet
 value = aws_subnet.private.*.id
}

output "public"{                                   *** i want to return my public subnet
   value = aws_subnet.private.*.id                  ** bc  am returing using  * (splar) am creating 2subnets so am goin to return 2subnet ids n when i reference in my code 
}                                                                                                                         ***2:18:26                           (main.tf)

  output "vpc_id" {                                  *** i want to return the vpc id 
         value = "aws_vpc.main.id 

  output
    output "vpc_cidr" {                                   *********** and the cidr for my vpc
         value = "aws_vpc.main.cidr_block
}


           ********* but what am interested in is either this public or private subnet but ill create my instance inside ofthe public subnet 
  


  2:18:26

  
                           (main.tf)

         resource "aws_instance" "my_ekl_instance" {
ami              = data.aws_ami.amzlinux2.id  
instance type  = [for s in var.myinstance : lower(s)][1]  
#instance type   = var.myInstance [1]                                                 
subnet_id        =data.taerraform_remote_state.vpc.outputs.public[0]        ****** when i reference it in my code, am referening that public output wich returns 2 of them   
key_name         = var. my_key                                                

   bt i need the 1st one, am goin to create my instance in the first subnet on the list bc this
     wil return for me a list, a list of the 2 subnets, so it wil be subnet 1&2 bt i wnat the first 
      so am using the index[0] to get the first subnet, thats wher ill create my instance

   

meta data option {
   http_tokens      = "required"
   http_endpoint    = "enabled"
}

root_block_device {
  encrypted = true
}

 vpc_security_group_ids = [
  aws_security_group.elk_sg.id,   ********** 
 ]                                                   
]                                                    

  depends_on = [aws_security_group.elk_sg] 
}

#EIP
resource "aws_eip"ip" {
  instance = aws _instance.my_elk_instance.



2:19:25
    **************************#########################||#####################################

 i blieve now in mobaxterm:

   my-vpc ls
ouput.tf  provider.tf   varaibles.tf   vpc.tf

my-vpc   terrafrm init
so here am recreating the vpc 
am no longer goin to us ethat vpc

terfrm validate 
successful

terrafrm plan 
      ********** watch      ............... 2:20:25

terrafrm apply .auto.approve 

   error:  vpc limit is exceeded
i can either go into my console    OR
  use IAC 
 cos i created the vpc, it already existed in that same region
my-vpc cd  ../vpc               cd and go to vpc
  vpc ls
outputs.tf   provider.tf     terrafrm.tfstate varaibles.tf       ****** so i seee i have the state file , so lll destroy it cos they are in the same region
vpc terrafrm destroy -auto-approve

   ***** somebody aked yetserday, can we destroy a vpc with a resource in it ??
lets find out 
we created a vpc and once it was created  we already created an instance in the vpc , so are we able to destroy it
so once it resfreshes the state, it starts destroying that one subnet 
its saying its still destroying 
uwil find out that it wil continue goin that way until it times out bc u are trying to destroy a subnet bc it has a resources in it 
so it will nt allow you 
so even on the console it will give you an error , u cant destroy it 
bc of time ill just stop the process, bt if u hv time u can try it, u can wait as long as you wnat, it wil nt destroy 
unless i destroy my instance first
ill just inter route , contrl c 

we can see  error:
this subnet has dependencies and cannot be deleted

  ******* 2:24:20
 so ill  cd out , bk here wher i hv my instance and bc we are migrating subnet, this instance will be destroyed anyway 
bc we are moving it to a new instance, ill run terafrm destroy , ill destroy that instance 
 vpc cd ...
2. tf-file-provisioner terrafrm destroy -auto-approve

    ERROR 
he said its bc its still reading frm the data.tf file 

**********  26:00 
 SO he went into the aws console and deleted the default vpc  and the other vpc that had resources in it by overriding the warning
 so we dnt hv a default vpc here again , that means that anything thing i create i need to reference a certain custom vpc or you can also create a default vpc 

 
******** 2:31:18                           ############### troubleshooting 
 he went back to (NO.19) and in data.tf he changed vpc to my-vpc

       data.tf
data "terraform_remote_state" "vpc" {
  backend = "local"                               
  config = {                                       
       path    = "../vpc/terraform.tfstate" chnaged to   "../my-vpc/terraform.tfstate" 


*************  trfrem apply 
    error:  creating EC2 subnet   ..... west 1


  he went to  varaibles.tf to change region
      this vpc was in east, i forgot to change my variabels , the parameters is US-west 1a and US west 1b

  }
variable "availability_zones" {
   type    = list(string)
   default = ["us-west-1a", "us-west-1b"]
}


 2:32:40
   ******* terfrm apply 
so once the vpc creates then we will create our resource 
and once the vpc creates then we are able to read the statefile using the terrafrm remote stste 
and we are reading it frm the dir which is my-vpc
and what we are interested in the ststefile is the output for the public subnet bc we wil create our resource in that public subnet
and the subnet wil read it frm that remote data source 
the output is called public and we will select the first one [0]
so bc this instance already exists 

   ******** ERROR :
 subnet can only be created in us west 1a and us west ic 
so he went back to the variable file and changed it aagain 

   #### am glad that this errors are happening now bc this is exactly what you will be dealing with at work when ur working on a project and you will have to try out things
some things will fail but you hv to troubleshoot and find out , look at your errors , look at what the system is telling you until you get success

   ****** terraform apply -auto-approve 
success

outputs:
 we hv the 2 private and public subnets 
vpc cidr and vpc id that we returned using output 


2:36:25
  ***********  NOW WE WNAT TO READ THIS STATEFILE
again we are going to use this particular data source to read the statefile that is inside of this dir 
once we read that it wil return the firts subnet [0] thats what we are intersted in which is public 
its the first one on the list , its a list of subnet


59..00

so with that now ill cd out to my parent dir and right there ill run terafrm plan 
have already initialize, hv already downloaded the pluggin so i can just run terfrm plan
bc init only initializes the backend and the provider, so i can run trfrm plan 
    2. tf-file-provisioner terraform plan

when i do that it willrefresh the state  and while thats happening somethings will be replaced
1) the EIP has been tainted

       INTERVIEW QUESTION

What is tainting in terfrm

   ANS:
this is marking a resource for destruction
when a resource needs to be destroyed in terrafrm, you taint it 
when its tainted, during the next apply , this particular resource will be destroyed

 ************ so our EIP has been tainted therefore it must be replaced 

 our instance is tainted, it must be replaced bc we are changing the subnet, we are forcing replacement of  the subnet
we are moving it frm the subnet that existed here (the vpc that was destroyed) and now moving it to a totaly didff subnet
this is the public subnet we selected [0] (we indidcated we were interested in )when we created our vpc 
  so thats how we access values on a list , we use index

  ******* ** terraform apply -auto-approve 
   
   once we approve it its goin to destroy that resource first, 
remeber if we wanted w ecould pass a life cycle meta argument
we can say we want to prevent destroy or we want to create before destroy
so what that would hav edone, it would first create our new instance before it destroys the other one
but now its destroying that instance first then create a new instance
 if evrythin has been passed correctly, it shud be able to create the instance 
then it will bootstrap it with a script using a provisioner , ok lets give it a moment
so this is the process of configuration
we created our instance and now we are using terfrm to actually install software
we are doin configuration, its now running the script in there
it has installed logstash
its installig kibana now in our instance 

     ask your questions and ill explain whats goin on 


            ************ QUESTION
1) if there are situations wher i have to go to the console to delete a resource, how does that impact the statefile ???
   ANS: 
   Remember when you run terfrm plan , terfrm wil always compare your configuration, this is your desired state, this is what i desire to have
then it goes to aws and looks at your actuall state
so when my desired state say i need to have an instance here bc i hv it in my configuration but when it goes to aws it doesnt find the instance bc i deleted it 
then terfrm wil see that as a decrepracy , a drift, so it will recreate it so that this always matches what is actual 
so if you run apply once you deleted it as long as its still in your configuration file terfm wil go and recreate it
so if you dnt want whatever u deleted, if dnt want it here then remove the configuration file bc this is your desired state

2) is there a function/argument  or the moment we change mayb an argument, it could just taint the former one 

  ANS:
 thers a terfr command to taint a resource
  terraform taint 
terraform taint aws_instance.my_instance              .... so if youv created such an instance ,its in your statefile ,bt u wnat to terminate it or to dstroy it 
you can taint it, tainting just means u acn just mark it for destruction and when nxt you run terfrm aply, it wil automatically terminate it bc the resource is tainted
 

3) am asking bc when i look at the template, sometimes it awes me, in our working env do we have some template that we can be modifying or we hv to write the code all the 
time

   ANS:     2:49:00
I did say it might look overwhelming but as you keep working at it especially if u rely on documentation
trfrm is very easy to work with bc ther are templates, ther are modules, u go to the documentation you copy and you paste and you just modify
you will be asked to create modules, custom modules , so custom modules you can use the official module to copy examples bt ur customizing them menaing your makiing them
to suit your env, your changing the names , the variables , the regions, changing hw users are interacting with that, bt the base code ur basically reusing it
in terfrm thers smt we call dry , like dnt repeat youself, terfrm supports that concept wherby we use modules so that our code is reuseable
like now how hv created this vpc here, hv create it as a module so i dnt anticipate  that whenever i wnat a vpc anytime that i will go back and rewrite this code, like 
what i did , i copied this code form somewher else bc i had already wirtten that so am just reusing the code and you will see a lot of that
so craete your infrastructure as modules and just reuse them , just make changes as as you deem fit and just reuse them


4) AM still confused as to how the documentation works , like how do we access all the documentations just to pick it back up when we are doing our work in the real world??

5) before WE apply our code is there a way to check our work,like a test run or thers like a backend checkover our work before we do terfrm apply n hv it show up on our 
 instance as for d real world work

  ANS:
   and thats why you see evrytime am runing trfrm plan, what does the do??
if u have configuration , before u create anything u wnat to check the plan, is this what you wnat? this is the test that your doing, you generating a plan
terfm wil generate for u a plan and the plan will ask you, is this what you wnat to do , 
in most of the cases you will jsut be runing trfrm plan, bc you dnt have to everytime run the pipeline bc once you apply it means you are goin to trigger a bill
most time u can do a local plan to see if your configuration is valid, once the plan exists success and u check the plan and its good then u can go ahead and run apply
just rely on terfrm plan to check that what your intending to do is captured in the plan before you run apply 





2:53:37
        ########### explanation contd ...

   so right here we used provisiners thats as far as i can go and using a null resource it did a connection and once it did that, it transferred some files,did ssh connection
it transfered these files, (null resource) kibana, apache then the script then transfering the files, it did a remote exec afyer that we atraed running these scripts thats
now use to install our softwares and start the services
so we ran the null resource, it unpack and u guys will hv this code so u can try it 
it unpacked and basiaclly ran the script , after which it install all those packages 
here am using terfrm to do configuration and how are we doing it, we are using a provioner  to do a provisoning
now hv installed kibana, elestic search
 opened (elasticsearch.yml)        ....2:55:10
now elastic search according to this when we install the pod that ealstic search listems to is 9200
but kibana  ,, opens (kibana.yml) is our display board , it listens on 5601
we knw that kibana is scrapping , we are passing the local host that port
kibana is reading frm elastic search  it getes its metric 
#this is the urls of the Elasticsearch instances to use for all your queries 
elasticsearch.hosts: [https://localhost:9200]

  2:55:49 ******** 
so if i go on my aws console now , i should have an instance t2.medium running
its created in availabilty zone us west 1c, it ha sthat sg
it has the elasticip 
if i copy that publicip, bc now this is the local host
and paste it in google, kibana runs on 5601   : publicip:5601
 
    2:58:30*********** cant connect, cant be reached 
   3:03:30  ......... opened our sg and open all tcp for one sg
   now working 
but its not security wise to open all traffic 
  ########### so thats a project once you have the code work on the security grp and make sure that you only opened the req port
but for dmeo purposes what we are just  rying to proof is that we have used tefrrm to do  configuration and we hv beene able to install elastic searchn kibana and view 
our dashboard 

   *********** 3:06:06
we have metrix space so maybe we can create our dashboard 
you can plac around with the index patterns n create ur dashboard


    QUESTION
1) whats basicaly the function of the dashboard, is it for monitoring the app??  APN, app performance monitoring 

   ANS:  3:09
yes for monitoring it 
right here you can monitor diff ,like right now its me metrics based on the OP that is running on 
so if i wnat host name , type, its giving me that,  am running this on a linux , platform its ubnuntu the sever its giving me that 

so u can cfeate a dashboard that will show u evertin , so frm this metrics you can create a dashboard that wil be able to display those metrics 


###### the point is we hv been able to use terfrm to do configuration and we hv been able to craete our instance
we hv been abl eto use provisoners, we v used nt just file provisoners,  we v also used remote exec provisoners to actually run our script
whne u look at all this we v seen that our terfrm statefile is also in this local env , it jsut mena that we also using a remote backend here

   *********** 3:11:26
what if we wnated to use a remote backend ??
 we wil go to where our provider file (prorvider.tf) and pass a backend that is remote 
when u pass a backend that is remote, this is like an S3 bucket so bc when we run terfrm init the firts tin that normally hapens is it initializes the backend 
bc thats the firts thing that runs, your backend therfore its means that its just needs to exist 
so i can go to my AWs console and i can look for S3

   *******  3:12:43
do i have an S3 bucket in this particular region, the region wher i created .... ? created what

i have a bucket her called landmark automation, now becsuas i have that 
so i wnat to store my statefile inside of this bucket instaed of my local env, for collaboration
so for me to do that therfore , ill need to configure a remote backend 
am using aN S3 just for state storage , just to store my statefile 
so ill find that bucket that exists which is (landmarkautomation) 
bc i have that bucket that exists and its in us west2
i just need to give it the region wher the bucket was created, it is global but yet when i created it, it was created in us west as a region 
am pasing my backend inside of my terfrmblock, this is where w epass our backend 
and the key is judt a path, its a dir , a dir that i want to create , i can call the dir elk, (no longer ec2)
its a dir inside of the bucket wher i wnat to create my statefile which is (terraform.tfstate)
then the region is US west2
so here am using a remote backend which is s3

******************************** 3:13:43
   NO.32

       in provider,tf 

  ( provider.tf     from NO.25 )  not the provider.tf for my-vpc that he subsequently created in NO.30

terraform {
  required_version = "~>  1.0 "
  required_provisioners {
    aws = {
       source = "harshicorp/aws"
       version = "~> 5.0"
    }
 }
                                                          **************   so here am using a remote backend which is s3
 backend "s3" {                                            ************  am pasing my backend inside of my terfrmblock, this is where we pass our backend 
   bucket  = "landmark-automation-kenmak"
   key   =  "elk/terraform.tfstate"            ******* and the key is judt a path, its a dir
   region = "us-west-2"
 }

provider "aws" {
   region = "us-west- 1
}



**********3:14:52
because initially i had this infarstruture created using a local backend now i want to migrate my state to a remote backend so how do i do that???

  we know that when u run terfrm init, the first thing it does is that it always initializes the backend
so if i wnat to migrate this state after hv made changes to my configuration and put the buscket ill run tefrem init but before i do that
 2 tf=file-provisoner terraform init
right now we see that our statefile is right here, its local
 2 tf=file-provisoner terraform init
so once i run terfrm init, its initilizing the backend 

  the first thing it says is
*********** DO YOU  want to copy existing state to the new backend ????
   preexisting state was found while migrating the previous , so it was local 
but now do u wnat to copy this staret to the new s3 bucket 
enter yes to copy and no to empty the state 

   *****  yes
 successfully configured the backend "S3" trgrm wil automatically us ethis bckend unless the backend configuration changes

 terraform has sucessfully initialized 

    *********** 
now i relaod my terfrm
i click the statefile but its empty bc its has been migrateed to s3 bucket
my backup (i blicv a file terrafrom backend ) is here but the current state of the infrastruture is stored now in an S3 bucket
so this module her is using S3 bucket for state storage 
 now if i go to my s3 bucket (i configured it) and open it iil see a new dir called elk that wa just created
if i click on elk ill see my tefrm.tfstate that was jsut created minutes ago 
 i can open it on the bject url, its says access denied but i can open it by clciking on the option open above and i see the conent of my ststefile

   ******* 3:18:29
now this statefile is now in a distributed env so when am working with you w eare able to collaborate bc we all hv acces to the statefile as opposed to if it was\on my 
local env then u wil nt have accse to it , so bc w ewrk in a collaborated env which is distributed 
so we always use S3 bucket for our state storage which is a distributed env

    INTERVIEW QUETSION
How do you work with terfrm in a distributed env , IN A COllaboartive env
 
    ANS:
we sue s3 for stste storage , we use it as our backend , wherby evertin that we do , we run here (our local env), its been captured in the s3 bucket 


    ************ 3:19:36
  e.g 
 if i run terfm state list 
  2 tf=file-provisoner terraform list
 this will jsut list the resources thats captured in the statefile

   now if i go ahesd n destroy this ,lets see what is goin to happen 
   2 tf=file-provisoner terraform destroy .auto.approve

***************** 
   i cant destroy my vpc bc my instance exist in the vpc but i can dsetroy this bcthe statefile is diff 
the statefile for my instance is remote its in my s3 but the ststefile for my vpc am using local 

3:21:33
 ******* lets continue with destroying
 but as its destroying 

  3:21:06
********** now can i migrate this state (vpc) to an s3 bucket ... ???? yes of course 

 
   opens the provider.tf for my-vpc ..... NO.30
  now i can add my remote backend  (copies the backend frm NO.32) And pastes it here   , the same BACKEND but now i can give it a path like my-vpc
i want the statefile to be saved in the same bucket but now it is goin to be in this path .. my-vpc
  for the instance its in the path elk but this is a diff statefile for my my-vpc
  
provider "aws" {
   region = "us-west-1"
}

  
terraform {
  required_providers {
    aws = {
       source = "harshicorp/aws"
       version = "~> 5.0"
}
data "terraform_remote_state" "my-vpc"
 backend "s3" {                                            
   bucket  = "landmark-automation-kenmak"
   key   =  "my-vpc/terraform.tfstate"                        **************  i can give it a path like my-vpc     (changed frm elk)   
   region = "us-west-2"
 }


   ****** after am done destroying, ill migrate this statefile to s3
 how do i do that??
 cd into my-vpc  and once am in ther ill run tefrm init
my-vpc terfrm init
terfrm init initializes the backend , secondly, it downloads the module
so if i want to migrate my state to a diff backend , what command can i use 
  its terraform init 
it senses i want to migrate bc now i hav a diff backend configured in the provider file 
  yes 
successful and it will transfer the statefile
now we see that statefile is gone


  ********* 3:24:13
 in AWS,i click on the elk path statefile for my instance and everyting in the statefile has been destroyed 
so the statefile is empty, it doenst have any reosurces, any outputs bc i just ran terfrm destroy for my instance

  ********* back in the bucket i can see i now hv my my-vpc dir created 
and in it i have a statefile, if i open it i see that i hv my vpc with public subnet, vpc cidr (the content)
so hv migrtaed my statefile for my-vpc here


  *******3:24:56

     HOW CAN I READ A STATEFILE THAT IS IN AN s3 bucket ???????

i wnat to create my resource , my instance in my (local env) but this statefile (my-vpc) is in s3 bucket
 so how do you read that
 .... again we use the same principle , we use the smae data source (data.tf)
but then our backend is not local 

 3:25:23  ************** 
   

  data.tf

   data "terraform_remote_state" "vpc"
   backend "s3" {                                ******** am saying that the bucket is s3
   config = {                                                ****** the config
     bucket  = "landmark-automation-kenmak"                 ****** am passing the bucket name 
     key   =  "my-vpc/terraform.tfstate"                            *********** and the ksy, this is dir , the path  to wher the statefile is 
     region = "us-west-2"                             ********* then the region
   }
 }

*/
#  data "terraform_remote_state" "vpc" {         ********** we comment this bc our backend is nolonger local 
  backend = "local"                               
  config = {                                       
       path    = "../my-vpc/terraform.tfstate" 



  ************ 3:26: 36
 so once uv configured this as your backend, i can now go into wher my instance is an i can run terfm plan
       2 tf=file-provisoner terraform plan
  the first thing is it is goin to read the terfrm state frm the s3 bucket
once it reads it frm the bucket it gets the info and it returns that same subnet id frm my statefile 

so this is how we use backends in  our infrastructure 

  ********** so just remember that befoer u initialize terfrm u will need to have that backend 

   ###########  some time  you might think why cant i create an S3 bucket using terfrm  ???
yes you can do that 
however, if it is in the same module ( wit the illustration he was giving wit the cursor, i bliv we wil run the command to create s3 bucket in provider.tf file) what will 
happens is that bc once u run init ,initilizing needs to initialize the backednd first before you create it, so what u may want to do is you may want to create your instance
but first in the (provider.tf file) comment out your backend as s3 (ie the s3 bucket backend) then run the code using a local backend so that your s3 bucket is created once 
the bucket is created then you can uncomment and comment your backend with the name of the bucket you just created , with the path , then your terfrm will initilize inside
of the bucket you ve created but as long as the bucket exists when you are running terfrm init then it is goin to initilize that bucket as your bucket
so thats how we do 

3:28:57
 ******* then there is what we call state looking
thats one concept that nxt time we meet, ill be taking abt that 
where by in the bucket trfrm prevent sif 2engrs are working on the statefile at the sametime
mayb bc its  an s3 bucket , i mayb be running apply and you are runing destroy
so we dnt want to corrupt our ststefile, so what tfrfm does is that it, it places a lock, you can lock the statefile so that only one engr can work on the statefile at
a time so that if you have 2 people issing terfrm command at the same time, you dnt want to corrupt your statefile
so we always use what is called dynamond db table to lock our ststefile in the s3 bucket

********** so this is a concept that we wll look at when nxt we meet ...


       QUESTION 
1) IN the event that i start working in a cop and the have their statefile in the backend, an di wnat to start working in collaboration with th eother team, 
how do i migrate it to my own local env so that i can work with the statefile that is already in existence

   ANS:
that will not be the case because once you pull it , we only have 1 ststefile  
normally they prevent you form pulling unless you are pulling it and pushing it back
but theres a command for that :
 terraform state pull

 ********  it pulled our stsefile and you can tell by the lineage number , it doesnt change 

******** so if i wanted to inspect the ststefile, like what is there , then i can easily do a state pull
 then look at it, work on it then i can do a state push to push it back but i think its asking for a path  or u can use -force

  ##### but basically you can use terfform state commands  
      terrafrm state -h
it shows you the sub commands, how to manipulate terfrrm state
 the state commands are only used when you are working with a statefile, but the statefile is very sensitive so you need to know what your doing bc if u mess up/corrupt 
the staefile then your entire infrastructure is corrupted bc the ststefile can no longer be used to compare what is desired and what is present
therefore thats why if you want to do anything on the statefile use the state commands , you dnt go on the statefile itself and try to edit/change it
if i need to remove a data source instaed of goin on the statefile to delete it ill use a state remove

subcommands;
list
mv
pull
push
replace-provider
rm
show




  ###### its a lot but when you start working with terfrm you will find that its pretty fun to work with , bc its pretty easy to work with n once u understand how it
works u wil really enjoy working with it more than goin on the console bc u can see excatly what your doing , and the process is always repeatable, if tomao i get up i
can recereate the same thing without thinking what have did, so i do the hard work once to see how its done but once i hv it in code then i can easily go back and repeat 
the process



      MY QUESTION

  what do you mean by wE are migrating subnets ??? 
IS it bc  of NO.3 above? where the variable file has that key (eks-instance") that cant be found we changed region in video3 ???

   ANS: 
 (the vpc that was destroyed) it was destroyed bc it was a basic vpc , we got and EIP error when he run trfrm apply(i guess he cudnt upgarde the vpc probably bc there may hv
been some things the vpc should have been configured with while it was been created so he had to create a new one)
I think he used the word migrtating bc the subnet id will change given that we created a new vpc



#################################################################################
    ANNOUNCEMENT
2:04:30 
prof said python will be covered during bootcamp

someone asked about promethus n grafana n he said its deployed with kubernetes 
someone asked for newrelic and he said we already hv a resource to that effect, its even on our youtube channel so that resource will be deplyed accordingly, once u look
at promethus and grafana, newrelic will just be anoda deployment in that category so you will not have any problem

we have people who have testified in the past who finished the course and its like they hv not had any mastery but in the bootcamp everything changed , 

once you fill the form we get it instantly 
 its cloud computing that we are leveraging on 

 
*****************    jly 2nd  bootcamp 
i dnt wnat you to think that u dnt have live experience 
u were working in landmark , whatever we did we didnt do it from the classroom, we did it frm the workplace
what has helped landmark students over the years is confident
the entire programme is project base
 someone told me no prof i dnt wnat any experince on my resume, i just want to go in for interview n tell dem i just completed internship at landmark
and when she spoke at the interview she was told she was speaking from a professional perspsective and she was hired 
        ### this is what ill first try 

if u dnt knw that frm day 1 this programme has been project based then thats a serious problem 





















diff btw output and datasource
https://stackoverflow.com/questions/75246774/difference-between-data-source-and-output-block-in-terraform

data essentially represents a dependency on an object that isn't managed by the current Terraform configuration but the current Terraform configuration still needs to make
use of it. Mechanically that typically means making a Get or Read request to a specific API endpoint and then exporting the data from that API response in the resulting 
attributes.

output represents is one of the two ways that data can flow from one module into another. variable blocks represent data moving from the parent module into the child module,
and output blocks represent data moving from the child module out to the parent.

There is no strong relationship between these two concepts bt one way they sometimes connect is if you use the tfe_outputs data source belonging to the hashicorp/tfe provider,
or if you use the terraform_remote_state data source from the terraform.io/builtin/terraform provider. Both of those data sources treat the output values from the root
module of some other Terraform configuration as the external object to fetch, and so you can use these as one way to use the results from one configuration
as part of another configuration, as long as the second configuration will be run in a context that has access to the state of the first.





   
        




             




















































