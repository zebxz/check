 REVISION;KUBERNETES ARCHITECTURE, PROBLEMS FACED IN YOUR ENV, TROUBLESHOOTING, IN K8 appl IS EXPOSED USING SERVICE DISCOVERY, COMMON ERROR IN KUBERNETES, VPC LIMIT, 
NODE NOT READY (MG8 CLUSTER), ERROR CONVERTING YAML TO JSON, COMMUNICATION ERROR, PROMETHEUS AND GRAFANA, How they work together for APM, IMP OF MONITORING,  
THREE ASPECTS OF HELM, DEPLOY PROMETHUS & GRAFANA USING HELM, PREREQUISITE,  STEPS TO DEPLOY PROMETHUS & GRAFANA;  Install helm, PROMETHUS : HELM TEMPLATE promethus/promethus,
ALERT MANAGER, WHEN DO WE SEND ALERT, NOW DEPLOY PROMETHUS, DEPLOY GRAFANA USING HELM, GRAFANA HAS NEW ADMIN PASSWD ANYTIME ITS REDEPLOYED, DEPLOY NGINX, 
CREATING INGRESS RULE FOR HOSTBASED ROUTING  for appl in default ns , CREATING INGRESS RULE FOR HOSTBASED ROUTING for appl in the dev namespace,
ONE LOADBALANCER (NGINX INGRESS)IS ROUTING TRAFFIC TO MULTIPLE APPLICATIONS IN DIFFERENT NS, TROUBLESHOOTING, CREATE A RECORD FOR ALERT MANAGER, GRAFANA DASHBOARD IS PREFERRED,
SETTING UP GRAFANA, SAMPLE DASHBOARDS TO IMPORT, SERVICES/ITEMS PROMETHUS DEployed, EFK, RANCHER, INSTALL RANCHER/NEED DOCKER , CAN IMPORT ANY AMOUNT OF CL8 INTO RANCHER, 
RANCHER DIDNT WORK/ WILL ilustrate again in bootcamp





NICE explantion for :  .. 2) Explain your experience in kubernetes?? .......  18:00
my question: when is hosted zones created

4:00     ................... watch it
you would be ask what you have been able to use k8 to solve in your environment
ITS A Common experience when it comes to k8, either they want u to explain your experience or they just want to discus some problems u may v encountered using k8 and hw you
were able to fix those problem. also when u apply k8 thersa lot of xperience u gather n der a lot of problem uve been able to solve, 
so the xpectation is for you to be able to state clearly the kind of problems uv been able to fix applying k8 in ur env

also at times , they ask to xplain  k8 architecture
this is a very imp aspect in k8                              



helm to install and deploy:
  P/G  
  EFK  
  Rancher    

6:00
this is a very imp aspect in k8 n u shud be able to xpln dem                        


          REVISION;KUBERNETES ARCHITECTURE
1) Explain the kubernetes architecture??  
   masterNodes/controlPlane:
made up of 
      apiServer  : wich is the main adminstartor, the entry point into our cl8
/etcd/scheduler/controllerManagers
   WorkerNodes:
      kubelet/kube-proxy/containerRuntime-Container-d/  
these are the component
we also have the
   kubernetesClient|: 
      kubectl / UI- Kuberenets dashboard
********thes are what we can use to mg8 task in k8 when it comes to architecture

                                            PROBLEMS FACED IN YOUR ENV

2) Explain your experience in kubernetes??

3)What problems have you encountered applying kubernetes in environment?
we kn the k8 mg8 containerized appl, if ders a problem in the app 
How can you troubleshoot application related issues in kubernetes??
our appl are running in pods so if ders a problem, we v a few things we can do , we also saw dat when it comes to docker 
now if u look at k8 and docker, if ders a problem in doker in our container we can run the below , also for k8 we run the command below
kubernetes manages containerised applications:
  kubernetes                                                    docker    :
  kubectl get pods                                          docker ps or docker ps -a      
  kubectl describe pods podName                             docker inspect containerName/ID  
  kubectl get svc/service  
  kubectl get endpoint/ep  
  kubectl logs podName                                        docker logs containerName/ID 
  kubectl exec podName  
exec to execute command inside the pod ..... 24:00                       docker exec containerName/id 
  kubectl top podName                                            docker stats/top   
we can also pipe n grep for errors
kubectl logs app-(app name) | gre errors



                              TROUBLESHOOTING
#############more xplanation  .................13:05
kubectl get po 
he tried to access his kops cl8 bt it didnt wrk so  he said 
when we v dis rror msg   to get it fixed we go to our kops cl8 setup n export the kube config file that is the file we used for kops inatallation
kops export kubecfg $NAME  --admin
then run 
kubectl get pod 
so if ders a problmr in our appl we can run that and we can also describe the pod 
kubectl describe po (pod name)



                         IN K8 appl IS EXPOSED USING SERVICE DISCOVERY
 service discovery:
    webAppservice ---> 7webAppPods
lets assume i hv 7 webApppods
THEREfore i will create a webapp service wich will lb traffic to the 7pods
############for me to knw my webappservice is routing traffic to all of this pod i can run
kubectl get svc
impotrantly we can get the endpoints/ ep

#############   also  .................................................NICE .........................################################18.00
we run kubectl get deploy  to see the app we deployed
kubectl get ep 
WE SEE THE CURRENT EP is 2 
kubectl scale deply (app name) ----- replica 4
we run kubectl get ep again 
now we see Ep is 4
############### there fore we can rightly say that the service is routing traffic as expected
so i can be abl rto xpalin this in real time bc it is working as expected
#########we can aslo check kubestl get svc 
to see if we can accesss the app and also check online

********** we can also get logs for the appl
kubectl logs app-(app name) | gre errors
if ders an error the error msg is able to tell us 

########################################we cud also have these errors ............27:00       our images are found in dockerhub or amazonECR
WE can run kubectl get events
in the process of pulling the image creating the container and
starting the container there could be a problem in btw this proceses
its either we are pulling the wrong image
we cud hv the right image but wrong authentication with the image pull sercert when pulling frm a private repo


                        COMMON ERROR IN KUBERNETES
common errors in kubernetes:  
1)  pods are in pending  state 
  pullimage error / imagepulloff   
   WRONG  image: mylandmarktech/hello:20     
          mylandmarktech/hello:22   
2) authenticationerror:
  mylandmarktech/nodejs-fe-app:2  
  imagePullSecrets:
  - dockerhubcred
3)KOPS CLUSTER DEPLOYMENT ISSUES: like
  IAM user not authorise     ............. we nid this to create a kops cl8 in aws
    create an IAM role with required permissions/policy  
       VPCFULLACCESS/EC2FULLACCESS/S3FULLACCESS and attached
       to the kops control server 
    Attached required IAM policies to the IAM user/group and   
    run aws configure  


                                     VPC LIMIT
  #vpc limit reached / :............ by default when u run a vpc tier account, the max numbe rof vpc u can crtae in one regionis 5, so when u reach this u can request#
    5 vpc running in production  
    Request additional vpc quota from aws  
  deploying a kops cluster [ a new vpc,s3, ec2-instances, ELB, ASG ]


                          NODE NOT READY (MG8 CLUSTER)

node notReady: when u deploy self mg8 cl8, u hv to deploy ur node , u hv to mk sure that ur nodes are ready bt  mg8 cl8 like  EKS we dnt worry abt d node nt being ready
  kops/kubeadm = kubernetes CNI [flannel, weave, calico] is not deployed our node will nt be ready
NODES not ready cud be bc of k8 cli, k8 netwrk interfaces nt installed 
  ensure that the kubelet service is running [systemctl start kubelet]   if not node wil nt be ready    
  node is stopped/terminated in aws , in this case restart it 
 OR  Place the nodes behind an ASG with a ELB for health check  
 *** use kops/eks over kubeadm    to manage such problem
#crashloopbackoff 
  # pod repeatedly crashing
#ReadinessProbeFailed or 
#LivenessProbeFailed
#Insufficient resources
#persistent volume mount failures
#InvalidNamespace
#cni plugin error

                                 ERROR CONVERTING YAML TO JSON
 
# error converting YAML to json
=============================

k8 manifest file can be written in json or yaml
bt when u run kubectl apply, it has to use the file 
e.g   ............. this is an error bc we dnt hv any kind as devops
kind: Devops
apiVersion: PODS    
metadata  
spec

kubectl api-resources | grep devops  
but no result
 bt grep for pod n we v result
kubectl api-resources | grep i pod


                                        COMMUNICATION ERROR
# what about issues when all pods are running but the applications are unable to communicate?
this cud be as a result of misconfigurations.
for example: for external access we use a nodeport service type
pod.yml   
-------
kind: Pod    
apiVersion: v1   
metadata:  
   name: myapp  
   labels:
      tier: be  
spec:
   containers:
   - name: myapp   
     image: mylandmarktech/hello   
     ports:
     - containerPort: 80

svc.yml  
-------
kind: Service
apiVersion: v1
metadata:
  name: websvc
  namespace: dev
  labels:
    app: web
spec:
  type: NodePort # ClusterIP
  selector:
    app: myapp # labels= tier:be
  ports:
  - port: 80
  - targetPort: 8080  # containerPort80/8080
  - nodePort: 40200   # 30000 - 32676
we dnt hv port range of 40200, its out of our service range

################################ so if u go for an interview and u are asked what is wrong with this script, why cant we route traffic what cud be the problem
 its just misconfiguration   , also our target port has to be the same with container port, also my selector has to match to my labels

*********** so if am given a scenario like this i identify problems and fixed them

so if u are asked did u encounter any problem using k8 ????
the answer is yes
did u suceesd in fixing them ?
most of the time yes






PROMETHEUS AND GRAFANA               *********************47:40  .............########### check powerpoint
if we want to ensure our server is function well we hv to install a software that will monitor the server and give us info abt it 
=====================
its an open source monitoring and alerting toolkit. prometheus consists of serveral components some of which are listed below:
***online***
Prometheus and Grafana are a popular combination used for application performance monitoring (APM). Prometheus collects and stores time-series data like metrics from 
applications, while Grafana visualizes this data in dashboards to provide real-time insights into performance, health, and potential issues. 

How they work together for APM
Prometheus: Gathers metrics from your applications. You can instrument your code using Prometheus client libraries to collect custom metrics, such as request latencies,
error rates, and business-specific KPIs. It then stores this data in a time-series database. 
Grafana: Connects to Prometheus as a data source to create customizable dashboards. These dashboards allow you to visualize the metrics collected by Prometheus, helping you 
track trends, identify bottlenecks, and set up alerts for performance anomalies. 



                              IMP OF MONITORING
importance os monitoring:
  •  avoid reactive panics  
  •  Increase uptime. webapp pod    in k8, automatically it wil restart the pod or if the pod goes down it will recreate a new pod replace it. 
  •  Improve hardware and software performance.
  •  Plan for the future by making the best use of your resources.
  •  get an almost immediate return on investment

************if ur server is abt to go down or is running short of resources, it may get to a point wher the appl can no longer run in the server 
e.g 
server [mem4GB ] --- maven[2BG], at all  time i need 2gb for my maven service to run                                             to avoid reactive panic
  OTHER SERVICES/processes are using 3G , then maven will nt run , therefore no bill can be done appl can run n dats a server so to avoid this we can monitor the server
  BOA.COM  / aa.com / cnn.com /     to avoid reactive panic  ,, imagine this server goes down, so we can monitor the env so that even b4 cx notice it we ar already aware 
  facebook.com  / 3B active users                                                                                        n fix it b4 they even notice 
  whatsapp.com /  3B active users 

  accessBank = 25B Naira as at Jan 1, 2023 
    Jan 1, 2023 - assets = 25,000,000,000/400 = $62.5m
  November = 62.5 * 1400 = 87.5B Naira 

WITH monitoring:
 Date       assets  NAIARA   USD   
 Jan 2023            25b    62.5m   
 Dec 2023            87.5b  62.5m  

WITHOUT monitoring:
 Date       RATES  NAIARA   USD-Asset   
 Jan 2023   400      25b    62.5m   
 Dec 2023   1400     25B    17.9m  

=====================================
Prometheus/Grafana 1:09   ........................... ************************ powercycle ......... prometheus configuration
=====================================
application performance monitoring = APM   
monitoring (micro-service) applications with  
Prometheus and Grafana
======================================

                                                   DEPLOY PROMETHUS & GRAFANA USING HELM
********************************deploy prometheus using helm
helm repo add prometheus https://prometheus-community.github.io/helm-charts
helm install prometheus prometheus/prometheus
helm show values prometheus/prometheus
helm template prometheus/prometheus

helm install prometheus prometheus/prometheus
helm upgrade prometheus prometheus/prometheus -f prometheus-values.yml -n dev

we acn still deploy promethus cloning this , bc its in our github repository ............ and following the instruction but thats not what we ar using
git clone https://github.com/myLandmakTechnology/prometheus-grafana-ELK-EFK



PREREQUISITE
*********************************************** we need a k8 server with dynamic storage class  to deploy prometheus n grafana 
2) Kubernetes nodes iwt mini 4GB RAM with 2 core processor
3) server which has kubectl $ helm configured


deploy grafana using helm   
==========================
...... 1:16:15          *so we are deployin promtheus n grafana in our cl8 and we wil be using k8 package manager whcich is helm   
  so if i dnt hv helm installed i can install helm3

                                    
                                      THREE ASPECTS OF HELM

deploy prometheus using helm:
helm comes with the :
  helm: CLI/charts/repository     .....these are the 3aspects of helm
 helm-charts      : predefined manifest files we can use to deploy both custon and 3rd partyapplications
helm-cli           : the command line utility
helm-repository     : where the charts are found

******as far as helm is concerned helm charts for:
nginx-ingress   https://helm.nginx.com/stable
grafana         https://grafana.github.io/helm-charts
prometheus      https://prometheus-community.github.io/helm-charts

******************SO WE want to deploy
nginx-ingress
grafana
prometheus

######we saw nginx ingress b4, bt lets see hw we can deploy nginx ingress using helm


        
                              STEPS  TO DEPLOY PROMETHUS & GRAFANA
first thing to do is to ensure that helm is installed

1)  Install helm

****************************make sure that helm3 is installed   
================================
helm installation:copy this 3line command and execute in my cl8:
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
chmod 700 get_helm.sh
 ./get_helm.sh

*********************   helm ls ........... to see we v helm installed 
*****************  helm repo ls ........... 

1a).........ADD Helm chart FOR  ngninx, promrtheus, grafana
deploy33.sh    (he didndt use dis, jst copy and paste the 3links)
helm repo add nginx-ingress https://helm.nginx.com/stable
helm repo add prometheus https://prometheus-community.github.io/helm-charts 
helm repo add grafana https://grafana.github.io/helm-charts 

helm repo ls >>>>>>> we hv the repositories
we can also upadte the repositort: ........helm update repo

helm install prometheus  prometheus/prometheus 

2) PROMETHUS : HELM TEMPLATE promethus/promethus  
==========================
this promethus has some chart, so we wnt to deploy this using the chart called promrtheus/promtheus
we can search this repository
e.g helm search repo promethus and we can see so many charts 
but we are using chart prometheus/prometheus
the chart has some value files  so we can run:
helm show values prometheus/prometheus
now we can see values for my prometheus configuration, these are default values
value files are default values
everytin has been configured
############  we also have templates              *************1:25:00
from this values file what is going to be installed
so we can run helm template promethus/promethus
lets see the diff k8 objs that will be 
we hv a long list of obj
we are deployin promethus as a statefulm set, 
the number of replca is 1
we hv are also cretain a deployment and this is happening in the default namespace
we hv her promethus non exporter , its a daemon set
we hv the service of promethus
***************i can also cahnge some info on my file


                                     ALERT MANAGER
there is a values file here, i hv dis values file  ...: promethus-file.yml, hv done some configurations   (hv shared it in the class)
in my deployment here
vi promethus-file.yml                  *******************************1:27:00
am just changin some info on my values file for promethus
in this info whats applicable in the changes
the alert manger file is goin to be changing, we are goin to be sending alert via email notification to my landmarkech. gmail.com
and this is the interval, gmial wil be notified   .......... the grp interval is goi to be 2minutes

                                         WHEN DO WE SEND ALERT
so when do we send alert?
serverfails:
  alerting_rules.yml:
      groups:
      name:NodeDown
when our node is down for 2mins we will send alert 
when memory is less than 85%
when our pods nt ready
when persisten vol error
etc
*************************************************pls make sure you copy this guy and create a file for it ( i bliv the alert file) ?????????????????

2a)  NOW TO DEPLOY PROMETHUS
we could decide to create a namespcae
kubectl get ns
kubectl create ns dev
ill use d dev ns for this deployment

helm repo add prometheus https://prometheus-community.github.io/helm-charts        **************this is goin to deploy base on the default value file
we can still deploy
but now i hv a custom value file which is 
prometheus prometheus/prometheus -f prometheus-values.yml -n dev
deployin promethus in the dev ns
kubectl get po -n dev
nothing in the dev ns 
so lets deploy promethus in the dev ns
helm install prometheus prometheus/prometheus -f prometheus-values.yml -n dev
kubectl get po -n dev
now we see its being deployed in the dev ns
its creating promethus alert manger
promethus node exporter
nod eexporter need to scrap metrix     ***************************1:31:22 ................ i dnt understand   ????????????????????

kubectl get svc -n dev
its creating service like alert mAnager -headless
it has this headless service bc this is deployed as a stateful set
all our services a cl8 ip


3)   lets deploy grafana               ****  take note ...2.18.25 initially he tried to use clusterip wich is more secured bt  gateway eror,so changed it back to loadbalancer 
 DEPLOY GRAFANA USING HELM ................. 1:32

grafana   https://grafana.github.io/helm-charts             ********************we only hv this helm chart
  helm repo add grafana https://grafana.github.io/helm-charts
  helm install grafana grafana/grafana -n dev                    *******************IN THE DEV NS

#########ONCe installed get admin user by runnin this command  (to get admin user psswd)
kubectl get secret --namespace dev grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo

  username = admin  
  password = 5ibxy9DOfkwuTjyWaW34T5wKAmA5sQ7Dp7aBkvmh

#################### what do we use to access grafana externally , its a service right?
we need the service
lets check the dev ns 
kubectl get svc -n dev
we see grafana service is clusterIP 
we cant use clusterIP for external service so we can edith the grafana service 
kubectl edit svc grafana -n dev        *** to vi & edit the deployment file
since am using kops cl8 i can change it to service type loadbalancer and its going to create a loadbalancer in aws 
but loadbalancer are not free 
kubectl get svc -n default   (ie default ns)
i have this loadbalancer i created before
ls
i see the previous deployments  ...... e.g app.yml , javaweb , mongo
vi into app.yml  ,,, which has the springapp svc
i see the service type is lb so i change it to clusterip
kubectl apply -f app.yml
service type has been changed
so now we hv the loadbalancer service in the dev ns bt its nt free
so i wnat to maintain my clusterIP which is most secured
helm ls -n dev
we see the deployments in the dev ns (grafana & promethus)
lets delete the grafana deployment
helm uninstall grafana -n dev
when i deployed grafana before, it created a lb in aws 
now redeploy grafana    .. he deployed it in the default ns & it now had clusterip ..     2.18.25. gateway eror, so changed it back to loadbalancer 
** 1.51.15: he had to reinstall it in the dev ns  **mee* (bc hes trying to illutrate that ngnix is in default ns but it can lb promethus & grafana in dev ns)
                                                                                        and also  spring app & java web  which are in the default ns.
i want to show you smt that is very important /...............................1:40.30
why do i wnat to use only clusterIP services bc its more secured

                                                        
                                                     GRAFANA HAS NEW ADMIN PASSWD ANYTIME ITS REDEPLOYED
whenevr you deploy grafana make sure you get the admin PW
helm I
The pW wil be diff from what we got before

4)                                       DEPLOY NGINX
lets also deploy ngnix ingress
i can search the repository and get the charts that are there
so lets search this repository and get the right helm chart to use 
ngnix ingress will enable us get a layer7 lb in our cluster which we can use to route traffic to multiple applications

helm charts for:
nginx-ingress   https://helm.nginx.com/stable   ....................THIS is our online repository
so lets search this repository and get the right helm chart to use 
helm search repo  nginx-ingress    
 nginx  nginx-ingress/nginx-ingress       >>>>>>>>>>>>>>>>>>>>>>>  ... i v this helm chart
helm install nginx  nginx-ingress/nginx-ingress -n dev      . .... in the dev ns
#########now the nginx ingress controller has been installed, 
kubectl get svc -n dev
it has created a lb in aws
we check the lb in aws and we see the hosted zone    ... ie ( landmarkapp.net, grafana.landmarkapp.net)
i bliv hosted zones were creayted earlier
let me show u guys smt

4a)                                               A record for (promethus & grafana )2.02.55
since we hv this lb in aws i can  go to aws and create  Arecords ................1:46:00
mk sure to cretae the right loadbalancer while making the Arecord
now we create a file (the ingress file)


  4b)                                                         CREATING INGRESS RULE FOR HOSTBASED ROUTING (for appl) in default ns
'.....................########## ingress rule using the host based routing for springapp.landmarkapp.net & java.landmarkapp.net
vi ingress33.yml
 

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress33  
  namespace: default  
spec:
  ingressClassName: nginx
  rules:
  - host: springapp.landmarkapp.net
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: springapp                   ************* routin to our springapp svc
            port:
              number: 80
  - host: java.landmarkapp.net
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: javawebappsvc         *********************** routing to javawebapp svc
            port:
              number: 80


                             ONE LOADBALANCER (NGINX INGRESS)IS ROUTING TRAFFIC TO MULTIPLE APPLICATIONS
                                     (springapp.landmarkapp.net,java.landmarkapp.net)

kubectl apply -f ingress33.yml
deployed
kubestl get ingress
kubectl describe ingress  
we see its in the dev ns
we see the backend
springapp.landmarkapp.net
java.landmarkapp.net
and its routing traffic when we check online 
one lb is routing traffic to multiple applications



                                               CREATING INGRESS RULE FOR HOSTBASED ROUTING (for appl ) in the dev namespace
                                       (grafana.landmarkapp.net, prometheus.landmarkapp.net, alerts.landmarkapp.net)

####lets create another file 
cp ingress33.yml ingress3.yml
vi ingress3.yml
             
 1:5700
---                                           Prometus and grafana are running in the dev ns
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingresspg
  namespace: dev
spec:
  ingressClassName: nginx
  rules:
  - host: grafana.landmarkapp.net
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: grafana  ***************************  routing to grafana
            port:
              number: 80
  - host: prometheus.landmarkapp.net
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: prometheus-server
            port:
              number: 80
  - host: alerts.landmarkapp.net
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: prometheus-alertmanager     **************   service name for alert manager
            port:
              number: 9093

********************************************* kubectl apply -f ingress3.yml
we check online: prometheus.landmarkapp.net
we can see the alerts that have been configured
************ we tried to access prometheus.landmarkapp.net  ... i think alerts.landmarkapp.net


   **mee***                    ONE LOADBALANCER (NGINX INGRESS)IS ROUTING TRAFFIC TO MULTIPLE APPLICATIONS IN DIFFERENT NS
           (springapp.landmarkapp.net,java.landmarkapp.net in default ns & grafana.landmarkapp.net, prometheus.landmarkapp.net, alerts.landmarkapp.net in dev ns)





2.05.00 to 2.16.55          TROUBLESHOOTING
     error.......       bad gateway
now we can  alerts.landmarkapp.net

                                                  CREATE A RECORD FOR ALERT MANAGER
but couldnt bc we havent created Arecords for alert manager service
after the ARecord created we can access it online 
we could also access grafana.landmarkapp.net 

                                                 2.08.36              GRAFANA DASHBOARD IS PREFERRED  
he said the promethus UI isnt the best option to use so we hv to use the grafana dashboard
but we still couldnt access it
with this error am unable to xplain hw all of this guy will give us the expected result
he said we couldnt acces it yet bc it takes a while 
finally, we can now acceess it , the grafana dashboard pops up
2.21.00   so we hve deployed our cluster


                                                   SETTING UP GRAFANA
then we entered the username and PW we generated earlier when we installed grafana 
 the grafana dashboard has a data source and the source is promethus   ...................**************2:22:21
name : promethus server
save and test
connection: http://promethus server
successfully connected to promethus
so its building a dashboard 
we can see the grafana dashboard wich is communucarting with the prometus server
we can import a dashboard
in the dashboard he typed 7249
connection: promethus server
now we see the dashboard ,n whatever is happening in your cl8 can be visualized in your promethus dashboard
so whtaever issues we ar hv  e.g running low on memmory ,on our promethus UI we have alert


                                                             SAMPLE DASHBOARDS TO IMPORT
when u configure promethus as we hv done, there are some sample dashboard to watch out for 
e.g
Sample Dash Board IDS: 
  3119, 7249,  8919, 6417 ,11074
we can import any of these dashboards
it gives us info on memeory usage and 
all of these inf can also be understood by non IT GUYS
these are dashboards that have been designed by other developer
***************so promenthus is monitorng my entire cl8
the mamory uasge the pods that are ruuning their metris, whatever that is happenin
there are diff dashboard that we can check
all my k8 objects, frm deployment, the pods,replicaset , node, controller
ther is an alert manager wich we hv created certain alert: like when memory is low,
POd not ready, pod crashing, persistent vol error, these are the alerts
there are congifuration for the different issues for which we will receive alert
so this promethus


2.18.25... grafana to lb

                                                   SERVICES/ITEMS PROMETHUS DEployed
********************when we deployed promethus , some items were deployed 
kubectl get po -n dev
didnt get exactly what he wanted so 
sudo su - kops
kubectl get po -n dev
we can see the items 
remember we deployed garafana independently
                                             
when we deployed prometus, it deployed 
1) promrthus alert manager
2) promethus kube state metrics
3) promethus node exporter
4)promtheus push gateway
5) promethus server 
etc
2.28.15 but how did it deploy my alert manager
kubectl get deploy -n dev
NAME
grafana
nginx-nginx ingress controller    ******** deployed as a deployment
promethus kube state metrics                ** deployed as a deployment
promethu promthus push gateway              ** deployed as a deployment
promethus seevrer     ************************  all these as a deployment

how about staeful staeful
kubectl get sts -n dev 
NAME                              READY
promethus -alertmanger             1/1
******************** PROMETHUS ALERT manger was deployed as a statefulset with one replica

lets check daemon set
kubectl get ds -ndaemonset 
promthus promthus node exporter       *************** this was deployed as a daemonset 




what we shud tk note of is that 
helm instal promethus came with all these micro services

1. alertmanager-0       -- statefulset
2. kube-state-metrics   -- deployment     ********** 
3. node-exporter        -- DaemonSet     ********* collects metrics from all the nodes so we nid each pod to be runnin in each node thats why its deployed as a daemonset
4. pushgateway           - deployment
5. prometheus-server   ( deployed along side vol concept)  - statefulset/database     ***** this is deployed as a statefulset bc it comes wit a database




                   EFK

                                

EFK =  
 Ther is elastic file , fibric and kunberner whic i will be sharing the videO with with you guys thats the final stuff to master, but before we look at that 
LET me xplain smt else, thers what we call rancher


                                    RANCHER

Rancher and kubernetes:
PROVIDE A Kubernetes platform  
With Rancher we can import and manage multiple kubernetes clusters    
We can create clusters like EKS / AKS / GKE, etc.   
Rancher provide a dasborad for kubernetes
Rancher is deployed using docker  


                                          INSTALL RANCHER/NEED DOCKER 
we will deploy rancher using docker  ................................ 2.33.00
checking if he has docker server to deploy rancher
didnt hv , so creating docker server using ubuntu
open all ports

COMMAND TO INSTALL RANCHER
sudo docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher


hes using a user data to install rancher, so paste the script 
#!/bin/sh  
sudo apt install docker.io -y  
sudo docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher
            ###### then launch the server

but it didnt work bc he realised that initially he didnt add -y 

******************so in mobaxterm he vi dk.sh  and paste
# Add Docker's official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl gnupg
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
********************************************************************************

ubuntu@ip-10-0-0-56:~ docker 
docker not installed 
********************************* so he ran 
sudo apt install docker.io 
successfuly installed

######################################### now deploy rancher
sudo docker run --privileged -d --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher


                                        CAN IMPORT ANY AMOUNT OF CL8 INTO RANCHER
************once rancher is deployed, we can import our cl8 into rancher
i can import any number of cl8 into rancher
rancher is being deployed, its a massive deployment
successfully deployed

ubuntu@ip-10-0-0-56:~   sudo docker ps 
CONTAINER ID        IMAGE 
b3E4C558C646C      rancher/rancher  
we hv this container
he then went to aws, this the container running
copied the ip , to acces it online , i wnt  to access the capplication, its a container
cant be reached
checked his security 
then tried the ip online again and it worked
warning potential risk ahead, thats fine
ACCEPT and continue
its asking for password 
then copy the command on rancher website on the screen 
sudo docker logs  container-id  2>&1 | grep "Bootstrap Password:"
copy the container id  : b3E4C558C646C 
sudo docker logs  b3e4c58c646c  2>&1 | grep "Bootstrap Password:"
in mobaxtern run :
sudo docker logs  b3e4c58c646c  2>&1 | grep "Bootstrap Password:"
 ************now we can see the passwd
copy it and paste on the website on the screen, am connecting to rancher now, it wil provide a dashboard
rancher is goin to give me a k8 dashboard 
now we can customise our own PASSWD OR selct a specific one 
allow
allow 
copy the sever ip adress
continue
but couldnt connect

                     RANCHER DIDNT WORK/ WILL ilustrate again in bootcamp

############## he said he wanted us to connect so he can show us how to connect to our cl8 from rancher
but bc of the error he wil hv to reillustrate in booth camp
so pls this is k8, ther are a few things that are pending which u will get them in videos 
like security and efk


##################### today is our final class #################
nxt wk we will search with boot camp

QUESTIONS:
ofcourse pod scaling will be revisited as well



sudo docker logs  container-id  2>&1 | grep "Bootstrap Password:"
sudo docker logs  b3e4c58c646c  2>&1 | grep "Bootstrap Password:"
b3e4c58c646c



  https://18.119.126.168

  security and EFK  

kops@master:~$ kubectl get po -n dev
NAME                                                READY   STATUS    RESTARTS   AGE
grafana-948dd5956-xcn5q                             1/1     Running   0          35m
nginx-nginx-ingress-controller-7f48d57b4c-7tghw     1/1     Running   0          43m

