5:49
theres an example when it comes to eks cl8 which u v to be able to discuss with relative ease , called probes
ders 1 aspect abt k8 cl8 wich is abt probes which is generally used to manage health check in our cl8

we wil see hw probes are used n why u may deploy ur appl n choose stuffs like probe n when that shud happen

SO DATS ONE TIN U SHUD BE ABLE TO DISCUS THE USE OF PROBES WHEN IT COMES TO DEPLOYING UR CL8 

probes:  Health check   
=======
TYPES OF PROBES
Liveness Probe:
Suppose that a Pod is running our application inside a container, 
but due to some reason letâ€™s say memory leak, cpu usage, application deadlock etc the
application is not responding to our requests, and stuck in error state.
Liveness probe checks the container health as defined (we tell it do), 
and if for some reason the liveness probe fails, it restarts the container.
internally , when i deploy my appl i can check if the appl is runnin by runnin the curl command
  curl localhost:8080       ..even for normal tomcat when I DEPLOY A tomcat appl, i can check if the appl is running or nt, i can curl
  curl localhost:8080/java-web-app     ...... also, i can the path to the appl
  curl localhost:8080/maven-web-app 
  curl localhost:8080/tesla       this is an apl that we v deployed, internally this appl can mk sure that its able to start this container, it must ensure that the 
                                         path is healthy, if its nt healthy our liveness probe is goin to fail n the container wil be restarted via d kubelet service..19.00
$? = 0  exit code !=0                  if it runs this command curl n the exit code is equal to $=0, then it means that it wil be routing trafic to that particular pod bt 
  curl localhost:80/tesla                 if nt equals to 0, it can route trafic to the pod, thats hw liveness probe is expected


2)Readiness Probe:
This type of probe is used to detect if a container is ready to accept traffic.
You can use this probe to manage which pods are used as
backends for load balancing services. If a pod is not ready, 
it can then be removed from the list of load balancers.

#############EXPLANATION: 15:00
COS services act as lb, n if a pod is nt ready , it wil nt add the pod to the pool, so if a pod is ready dats the only time the job is goin to be performed, if its nt ready
then it means our health check has failed, so thats what readiness probe is going to achieve

                                            webapp001
                                         image:mavenwebapp
                                          curl localhost:8080
   (routing traffic to the pods)
      service                                 webapp003
      webappSVC                             image:mavenwebapp
                                           curl localhost:8080

                                               webapp007
                                           image:mavenwebapp
                                           curl localhost:8080

                        
  so if for e.g pod007 is nt ready, it will remove it frm our lb agorithm, it wnt be der anymore, the service wil nt be routing traffic to an unhealthy pod, until when the 
pod is active thats when it will route traffic


3)startup Probe:
This is use for slow starting containers  
60 secs  
for containers that are taking a long time to start, we deploy the container using using startup probe as the health check

18:00
WE SOME INF ONLINE THAT U CHECK ABT PROBES ASwell, hw it works, it has a gud documentaction we can look at

https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ 
https://kubernetes.io/

https://github.com/LandmakTechnology/kubernetes-manifests/blob/main/liveness_readiness_probes_example.yml
https://github.com/LandmakTechnology/kubernetes-manifests/

*************DEPLOYING APPL INCORPORATING LIVENESS PROBES...........
               LETs deploy a replica set
in the ubuntu server he sitched to sudo - su kops
but got error when he ran
kubectl get all     ... 24:24
he was unable to access his kops cl8 so 
when that happens we nid to export the kube config file again(i guess thats 10b in the inatallation file we used in kops installation): run kops export kubecfg $NAME --admin
now run
kubectl get all and we hv our cl8 running


app-probes.yml  
==============
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: javawebapprs
spec:
  replicas: 3
  selector:
    matchLabels:
       app: javawebapp
  template:
    metadata:
      name: javawebapppod
      labels:
        app: javawebapp
    spec:
      containers:
      - name: javawebappcontainer
        image: mylandmarktech/java-web-app
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /java-web-app
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 1
          periodSeconds: 15
        livenessProbe:
          httpGet:
            path: /java-web-app
            port: 8080
          initialDelaySeconds: 15
          timeoutSeconds: 1
          periodSeconds: 15 

---
apiVersion: v1
kind: Service
metadata:
 name: javawebappsvc
spec:
  type: NodePort
  selector:
    app: javawebapp
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 32000 

http://:8080/java-web-app

  curl localhost:8080  

http://34.215.29.198:32000/java-web-app 

Configure a Metrics Server on your kops Cluster4??
=================================================
https://github.com/LandmakTechnology/metric-server

kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

kops@master:~$ kubectl apply -f metric-server/metrics-server-deploy.yml
serviceaccount/metrics-server created


serviceaccount/metrics-server unchanged
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader unchanged
clusterrole.rbac.authorization.k8s.io/system:metrics-server unchanged
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader unchanged
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator unchanged
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server unchanged
service/metrics-server unchanged
deployment.apps/metrics-server configured
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io unchanged

rbac:
serviceaccount:
   ServiceAccounts Subject =  Users/Groups/otherApplications  
role :
    objects = pods/deployments/services/rs/ds/rc [ * ]
    verbs   = edit/delete/view/read  [*] 
    apiGroups = [ v1 ]  [ * ]
    namespace = role are applicable to a particular namespace

RoleBinding: 
   This is assigning/association the role to a subject 
   ServiceAccounts/Users/Groups/otherApplications\

  ClusterRole:    
    objects = pods/deployments/services/rs/ds/rc [ * ]
    verbs   = edit/delete/view/read  [*] 
    apiGroups = [ v1 ]  [ * ]
    ClusterRole are applicable to THE ENTIRE Cluster and all namespaces   
  ClusterRoleBinding: This is assigning/association the role to a subject 
    ServiceAccounts/Users/Groups/otherApplications

rolebinding.rbac
clusterrole.rbac
clusterrolebinding

