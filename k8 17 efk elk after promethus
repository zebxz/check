
 TO ACCESS APPLICATION THAT IS RUNNING, DIFF BETWEEN PROMETHUS/GRAFANA & ELASTIC STACK, INSTALL EFK/ELK stack K8's cluster USING HELM, WHAT IS ELASTIC STACK, 
WE ARE GOING TO SEE HOW TO USE ELASTICSEARCH, KIBANA AND BEATS, ELK Stack Architecture, PREREQUISITE TO DEPLOY ELastic Stask, USE HELM TO DEPLOY Elk, STEPS TO DEPLOY EFK, 
NEED TO ACCESS KIBANA EXTERNALLY, CREATE INGRESS RULE FOR KIBANA,  HOST BASE ROUTING INGRESS RULE FOR KIBANA, CREATE A RECORD, CAN RETAIN LOGS IN ELASTIC SEARCH, LOG, TOPS, 
K8 TALKING POINT, S3 BUCKET, VS CODE /TERRAFORM



check the EFK/ ELK powerpoint
WE ARE GOING TO SEE HOW TO USE ELASTICSEARCH, KIBANA AND BEATS

we are talking about elastic stack , lock stack and kuberna or elastic stack,  filebeat and kuberna
we are goin to be deploying this stack in our k8 cl8 using helm


REASON WE NID TO DO THIS:        **************** powerpoint
monitoring, alerting and log aggregationare essential for the smooth functioning of a production grade k8 cl8



                                       TO ACCESS APPLICATION THAT IS RUNNING 
e.g 
1) WHEN WE DONT HAVE MANY APPLICATION:
when u want to access an applictaion that is running, in k8 we can check the logs
kubectl get pod
we see the pods that are running in the defaulf dev space
NAME                        READY     STATUS
hello-app-6hztm              1/1         running
tdapp-759f7c6687              1/1         running

we can check the logs of each of the appl
we can describe the pod
kubectl describe  tdapp-759f7c6687     
OR check/get the logs  of the pod
kubectl logs tdapp-759f7c6687     
we can see the appl was deployed using the spring booth frame work
if u look at the appl, you can visualize the logs
********************if ders anytin wrong with your appl you can check the logs it will tell whats happening

############## kubectl get deploy
NAME                      READY         
ngninx-nginx-ingress        1/1
tdapp                       2/2 

**************** kubectl logs tdapp
Error from server (Not found): pods tdapp not found 
####################### ie it has to be in pods to check the logs , ( run kubectl get pod, first before running kubectl get logs tdapp)

* 9:10

2) WHEN WE HAVE MULTIPLE APPLICATIONS:     kubectl logs cluster-autoscaler-6cf74d69f7 -n kube-system    (kubectl logs app-(app name) | gre errors)
  but when u have multiple appl and ther are issues,
kubectl get pod -A        * TO get all the pods in all the name spaces
we see all the pods running
NAMESPACE            NAME    
apm                 grafana
apm                 promethus
kube-system         cluster-autoscaler-6cf74d69f7
grafana pod is running

the alertmanger   , incharge of sending alert, if smt is going wrong 
kubestate -maetrics ttaht gathr all the data, whatever is happening within our k8 objects eg pods, deplyment, rs, rc,  even nodes
node exporter is also gathering whatererv is happening in the nodes, the metrics  node exporter is gathering them 
promrthus sererv pod , it will scrap all of this data into the promethus  time series databasea nd once the dta has been scrapped we can visualise what is hapening using 
grafana

**************** so if we hv 100 pods we cant be running kubectl for all of the pods, that will be a tidious task
kubectl logs cluster-autoscaler-6cf74d69f7
ERROR form server not found clustr-autoscaler      ************************** this is bc we didnt check in the correct namespace (we nid to pass the namespace)
kubectl logs cluster-autoscaler-6cf74d69f7 -n kube-system
the logs are successfully generated                     8:25





****online..
No, the Elastic Stack is a collection of tools that includes Elastic APM, which is an application performance monitoring system. 
The Elastic Stack is the foundation that Elastic APM is built upon, consisting of other components like Elasticsearch, Logstash, and Kibana. 

DIFF BETWEEN PROMETHUS/GRAFANA & ELASTIC STACK
Prometheus, Grafana, and the Elastic Stack serve different primary functions in the realm of observability, though their roles can overlap. 
Prometheus is for metrics monitoring, the Elastic Stack (ELK) is for log management and analysis, and Grafana is a versatile visualization tool that can sit on top of both



         INSTALL EFK/ELK stack K8's cluster  USING HELM


OBJECTIVE:

Monitoring, alerting & log aggregation are essential for the smooth functioning of a productive grade kubernetes cluster.

Appplication & system logs help us to understand what is happening inside a cluster, the logs are particularly useful for debugging problems and monitoring cluster activity.

In microservices architecture, a single business operation might trigger a chain of downstream microservices calls, which acn be pretty challenging to debug.
Things, however can be easier when the logs of all microservices are centralized and each log event contains details that allow us trace thr interaction between the application.
we can use Elastic stack along with Docker to collect, process, store, index, and visualize logs of microservices.

HOW ARE WE ABLE TO CENTRALIZE OUR LOG MGT  ************************ powerpoint 12:40
we can use Elastic stack along with Docker to collect, process, store and visualize logs of microservices



WHAT IS ELASTIC STACK  .........................the vendor is ELASTIC
its an open source app from elastic designed to take data From any source and in any format and then search, analyze,and visualize that data in real time.
it was formely known as ELK stack, in wich the letters in the name stood for the appl in the grp. Elasticsearch, logstack,and kibana. A fourth app, Beats, was 
subsquently added to the stack.

ELASTIC search: is a real time, distributed storage, JSON-based search, and analytics engine designed for horizontal scalability, maximum reliability & easy management. it can
be used for many purposes, but one context where it excels is indexing streams of semi-structured data, suscha s logs or decoded ntwrk packets.

ie now we can use elastic search to store data and to index the data, indexing streamas of semi-structured data, e.g logs or decoded ntwrk packets.
************* so elastic search will act as a container, it will store all the logs  ...... this i sthe use of elastic stack, it also does indexing.
it will index the logs , so when the logs are inside elastic search or the semi structure data n some ntwrk packages are inside elasticsearch waht happens is 
elasticsearch can do indexing, it will index the logs , it will perform indexing ,indexing is just like a pattern , lets assume it wants to cfreate an index which is goin
to like be displaying logs on a timestamp basis so that whatever is happening in the cl8 , u can use ur timestamp to know aht is goin on, so that can tk place, therfore
elastic search is goin to act as a store n a kind of an analyzer.. thats what elastic search is goin to do.
once the logs are stored, the next thing is that we want to visuaize these logs and we can use kibana  .... (THE SAMe way we saw grafana)

KIBANA: is an open source analytics and visualization platform designed to work with elasticsearch. it can be used to search,view n interact wit data stored in elasticsearc
indices, allowing sadvanced dtaa analysis n visualing dta in a variety of charts, tables n maps

the nxt componant is called beats
BEATS: open source data shipprs that can be installed as agents on servers to send operational data directly to Elasticsearch or via logstash, wher it can be further
processed n enhanced.
Therea re a number of beats for diff purposes.
FILEbaet    , logfile
metricbeat  , metrics   e.g metric data like cpu, memory
packetbeta , netwrk data
heartbeat: uptime monitoring

WE ARE GOING TO SEE HOW TO USE ELASTICSEARCH, KIBANA AND BEATS

in few words
filebeats collects data from the log files and sends it to logstash
logstssh enhances the data and sends it to elasticsearch
elasticsearch stores and indexe the data
kibana displays the data stored in elasticsearch




                         ELK Stack Architecture
           2) data processing        3)storage           4)Visualize
1)Log

            logstash                 elatsicsearch      kibana

however one more component is needed or data collection called beats. this led Elastic to rename ELK as the Elastic stack.

            Data collection          data processing        3)storage           4)Visualize
1)Log

              beats                       logstash                 elatsicsearch         kibana (can also be used to share)




PREREQUISITE TO DEPLOY ELastic Stask     .....23:00
we need a k8 cl8 with storage class configured



                           USE HELM TO DEPLOY Elk

                         **** . (since we ar not changin any values we wont use -f) bc we wont use any customized file for the deployemt, we did nt chnGE D 

kubectl get pv 
to check for our persist volume we created initially
kubectl get sc
to see we hv a dynamic storage class 
************* also if we have deploy k8 using kops, it would have as well cm with dynamic storgae class


                            STEPS TO DEPLOY EFK

1)create namespace
kubectl create ns efk
add
we are using helm to deploy efk

B)  ADD efk repo
helm repo add elastic https://helm.elastic.co
we can go to this site to search the chart that are der   :https://helm.elastic.co
helm repo update
helm repo ls 
we wnt to deploy elasticsearch
searching the repo we just added
helm search repo elastic **************** searching the repo we just added
we can see all the charts  in the repo
this is the chart we want to use :   elastic/elasticsearch
helm show values elastic/elasticsearch
lets see the values in the chart
lets createa anew file 

c) mkdir efk
cd efk
ls                                                  (redirectin it into this custom file)
helm show values elastic/elasticsearch >> elasticsearch.values  
ls 
elasticsearch.values
vi elasticsearch.values                   ................ 31:00
we see the valuesfile that will be used to deploy elastic search
he changed replicas from 3 to 1 and master node from 3 to 1 ,, just for this practical

2)
helm install elasticsearch elastic/elasticsearch     ........(using the default values file)
   It will deploy elasticsearch in the default namespace 
   It will deploy elasticsearch using default value file 
helm install elasticsearch elastic/elasticsearch \
  -n efk -f elasticsearch.values              .....(using our custom file)  ..... dnt use the default file use the custom file
it will be deployed in the efk ns we created earlier

kubectl get all -n efk 
nothing available

helm install elasticsearch elastic/elasticsearch -n efk -f elasticsearch.values    
kubectl get all -n efk 
*********now we see the elasticsearch pod coming up and its being deployed as a stateful pod ....1
kubectl get pvc -n efk
it has pvc for the elasticsearch pod with 30Gi
this is bc we hv dynamic class storage configured in our env so data can be stored bc elastic search wil store the data
kubectl get pod -n efk 
we can see the pod and the service created

2) ##############     WE WANT to deploy kibana    .... (helm install elastic/kibana)

helm show values elastic/kibana
we see the default values for kibana
one replica is being deployed and dats what we need
we hv some cinfigs maps that wil be deployed
config maps are additional configuration
we see the service type is CLusterIP and we can work with that bc we have nginx already deployed
*********** but if nginx wasnt deployed, we would have changed the service type to nodeport or lb ....2
all the default values are ok
the chart we are using for thais deployment ias elastic/kibana
################### deploying kibana
(its deployed in the efk name space)  
helm install  elastic/kibana -n efk      ... (since we ar not changin any values we wont use -f) bc we wont use any customized file for the deployemt, we did nt chnGE D 
VALUES file           
kubectl get po -n efk
  ***********we see kibana was deployed using deployment, k8 object........3
kubctl get statefulset -n efk
we see elasticsearch was deployed using stateful set
bc elastic search comes with a database, its the database wher all the logs are shipped into
*********************and logs that hv been shipped into elasticsearch,we can visualize the log using kibana
filebeats will ship the log into elasticsearch
***********************we now install filebeat
helm install filebeat elastic/filebeat -n efk        (in the efk namesapce)
also we are nt changing any values for filebeat 
we nid filebeat to be running in all the nodes, so it can be collecting the logs and shipping it 
therefore, it will be deployed as a daemonset

3) ##########
helm install filebeat elastic/filebeat -n efk
now deployed
kubectl get ds -n efk
filebeat-filebeat
kubectl get svc -n efk
we hv 3services
NAME                                  TYPE
elasticsearch-master                 CLusterip
elasticsearch-master-headless        Clusterip
kibana-kibana                        Clusterip


                             NEED TO ACCESS KIBANA EXTERNALLY
we need to access kibana service externally 
right now we hv just the clusterip we hv used to deploy kibana
but see we already hv ingress controller deployed in our cl8, we dnt ned to worry abt nodeport


                      CREATE INGRESS RULE FOR KIBANA
4)  ***************************so we are goin to create ingress rule for kibana
we created one yesterday for alertmanger, promethus server and grafana
so hw do we wnat to access kibana, we are creating an ingress rule
############# host base routing


                                                      HOST BASE ROUTING INGRESS RULE FOR KIBANA
# ingress2.yml
apiVersion: extensions/v1beta1  # networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-rule-1
  namespace: efk
spec:
  ingressClassName: nginx
  rules:
  - host: kibana.dominionsystem.net         (hostname), to accses the backend services, type: kibana.dominionsystem.net 
    http:
      paths:
      - backend:
          serviceName: kibana-kibana       (gotten by running kubectl get svc )
          servicePort: 5601                 (gotten by running kubectl get svc ) at work u also maintain the right service port


this is host basse routing
to create thsi ingress resource
vi  ingress-rule.yml   ,  and paste
kubectl apply -f ingress-rule.yml
ingress.extensions/ingress-rule-1 created
kubectl describe ingress ingress-rule-1 -n efk
 we can see kibana.dominionsystem.net
and its routing trafic to the backendport on the port number   kibana-kibana:5601(10.0.1.124:5601)
its port forwarding taking place here     ................ 52:00
bc when u type : kibana.dominionsystem.net
ur traffic is routed via the lbalancer service
&once it comes into d cl8,it talks 2 dis service kibana-kibana:on d service port number 5601 & kibana service route d service to dcontainer on dis port no.(10.0.1.124:5601)
this is what is happens

                                                   CREATE A RECORD 
in aws we hv to go to route 53 n create dns record    ....... 52:16
now we can access it online by typing kibana.dominionsystem.net

now we hv some pod that we hv deployed
kubectl get ingress -o wide
we can see the address
lets get ingress in the right namespace
kubectl get ingress -n efk -o wide 
we can see ths adress , its the address of our lb
nslookup kibana.dominionsystem.net 
we see its resolving to the adress , (i bliv its the lb address)  .......56:45
so when u ttype kibana.dominionsystem.net , in the background, its runing dns lookup, it wants to find out, what is the address, bc every server has a name 
when endusers type          ## it talks to    >>   this talks to   >> talks to >>  
******************* kibana.dominionsystem.net >> Ingress controller >> kibanaSVC >> Kibanapods
the ingress controller is using the rules defined in this ingress resource and to talk to the ingress controller pod, it goes tru the lb and the lb we hv created Arecords
so if you want to talk to the lb just type kibana.dominionsystem.net, traffic is routed, it gets to the ingress controller, ic routes  to the kibanapods tru the kibanaSVC



NOW THAT WE HAVE ALL THESE ESTABLISHED 
now hes logged into elastic search engine online   >> (i guess)
he said now this is my kibana dashboard, now i wnt to knw what is goin on in my cl8
this is just abt log mgt, elastic search, filebead and kibana
elastic seacrh is der, filebead collects the data an ships the data into elastic search, elastic search will index the data n kibana wil help us visualize to explore n to
even share the info that is shared in elastic seqrach
lets try to explore we hv overview: u can add some integration and stuff, discover , dashboard: u can create  a dashboard or instal some smaple data
in discover: let screate index pattern ................ 1:02:22
name:  filebeat     ... am tying filebeat bc its the shipper
what can we use, we can use timestamp then we can just create index, we can add other fields if we need to 
very soon we wil be seeing whta is happening insid ethe cl8, it will be doing some data streaming
we hv a lot of option we can do, index policy, index mgt , roll back ..... etc 
now we hv 6,268hits
now its goin gto be capturing evrytin that is happeing in the cl8 based on the timestamp , whatevr is happening u can see evrytin
we can add a field  ...... 1:04:50
so dis is jst u creating tryin to creste some data tracing pattern like when u hv elastic serach u can create like hw long do u reatin logs  in elastic search,

                                          
                                              CAN RETAIN LOGS IN ELASTIC SEARCH 
we can decide to retain logs in elastic search for like a month  cos u dnt wnat to keep logs ther for so long. bc logs are  goin to be generated constantly so our retainment
policy can just be for a month  ,,, bt it also depends , it can be decided by mgt ,,, so in INTERVIEW, we can say that logs are kept for one month afetr wich we can decide to 
move the logs from elastic search n save them in my aws s3 bucket
i just went to data stream, we can visualize data in a format wher even non IT folks can underatnd, 


                           LOG
A log is a variable file, which means whatsoevr is hapening, they keep on changing, they are goin to be constantly changing
its just like when u run  : top

                                       TOPS
tops tells u whats happening in ur system , hw many users, the processes that are runing , the values keep changing
the events that are taking place are variable events 




####################################  1:11 35
###################### PLS THERs one ASPECT  OF K8  WHich I HVAE NT COVERD WITH YOU GUYS n some other aspects WHICH I WILL COVER IN BOOT CAMP , bt the aspect i was supose 
 to cover i decided to share the video with u guys bc i want to shed some light in the 2nd half of the class, on certain terraform concepts...... k8 security n rbac




****************************** K8 TALKING POINT



Summary 
   Talking points:
1. Container Orchestra     ... ie
      kubernetes Orchestrate containerise applications 
      kubernetes Orchestrate containerise micro-service applications 

******* we choose k8 bc

2. Self healing capacity, scalability, Disaster recovery, LB,  
   
3. Automated rollouts and rollbacks

we use k8 objects to deploy workload in our cl8
imp objs to tk note of 
services for service discovery
pods
rc , rs, 
satefulset  ,, which we just saw today
daemonset
deployment
volumes
configmaps
secrets
namespace concept
ResourceQuota
nodeAffinity
podAffinity
security

*********these are very imp concepts for  to b able to xplain them
before the objects, we spoke abt the architecture
in the architecture, we saw the controlpaane, wich is also called the master node
apiserver
etcd
scheduler
controller-managers
kubernetes-cni  :
kube-proxy
kube-dns
workernodes

Kubernetes architecture:
  control plane /master-node:
    apiServer        ************ this receives the api call  eg         kubectl get  ,        helm install   
       kubectl get  
       helm install   
       ui       *********************  using the k8 dashboard with  some ui commands , u wil be communiatoin wit the APiserver, once it receives it it persist it in etcd
    etcd [db]   (db or keyvalue store)   ,,,, then whatevar info has been stored here the scheduler is notified that some wrk nids to be done
    scheduler         ************ then transfer d inf to d workernode ,kublet receives d info n immediaterly, d scheduler schedules d pod in d worker node n d resorce  
    controller-managers                                                                                              is created
    kubernetes-cni:
      kube-proxy 
      kube-dns 
  worker-nodes:                                                                                    1:21:10
    container-runtime[docker, containerD]  ,, cud be docker in the past or today cntainerD ...i pu docker in d past bc docker was depreciated jst last yr n ur xperience 
    kubelet                                                                                           is nt  docker was only 
    kubernetes-cni:        the cni is also ruuning in the worker node like in the master node
      kube-proxy 
      kube-dns 
Kubernetes objects:
  services = service discovery :   our app are running inside a pod so to acces them u must go tru a service
    users/admin/app--->SVC--->pod[APPcontainer]          user,admin , app can access the pod tru a service e.g spring talk to the db pod tru the service 
    service-TYPES:                                                                               the pod to d service is an endpoint n d service performs this task does lb
      ClusterIP :              the default service in k8                                                                          that is service discovery
        spec:         
          type       1:33:00 ...  how will a service identify that it nids to route service to a po dvia service discovery, under service we hv api version, metadata, spec, 
          selector:                   under spec seletor, in the selector we pass smt like this ,app: myapp bc d selector must match
            app: myapp 
      NodePort         *********** external
      LoadBalancer      ****************** external
      ExternalName       **************** we didnt demo this, used to cmm with app outised ur cls8
      ingress            ***has  alot of advantages .in security its xplained as additional layer of security, we can use it to deploy app using clusterIP wich increses 
         ClusterIP                                  security in our cl8
  pods---> houses containers       ************* wher appl are run
     SingleContainerPods         *********** truout this course we hadjst had singlecontainer pod bt we can v some  pod that act as ultilty  pod to the main pod 
     MultiContainerPods       ************    and in that case we can hv multi container pod
  PodTemplate:
    metadata:
      labels:
        app: myapp 
    spec:
      containers
  ReplicationControllers:   (rs,staefulset, daemonset,deployment) therfore, we dnt deploy d containers directly inside the pod dirctly bc  d pod lifecycle is very short
    spec:                                      so we depoy them using controllers, so wit the help of controlers we cqn deploy our appl
      selector:                                 bt how will d controller knw that they are incaharge of a pod,  remeber that in d pod ther is a template file,
        app: myapp                                 and in the pod ders a template ders a metadata wher we pass some labels like app: myapp
                                  for replication controllers we hv selector is looking for app:myapp, so selector knw that app=myapp, so d controllers ar able to identify
                                                      their pod usin labels n selectors, so this  replicationcontroller knows that ist responsible for this guy app:myapp
ReplicaSet:                                       similarly,if it was for rs, daemonset , deployment , these are the new generation or the big guys bc 
    selector:                                         they jst dnt use equalitybased selector
      matchExpressions [set-based]                under selector they add anoda layer : e.g match label for equality base
      matchLabels: [eqaulity based]            1:32:00           this is the same pattern for rs, stateful set , daemonset, deployment
        app: myapp 
  StatefulSet    .... used to deploy db appl
  DaemonSet          ... to deploy appl that wil run in all your node like exporter we deployed yesterday n filebead  we deployed today
  Deployment        ... recommended k8  obj to deploy in k8 bc it has rollback , rollout n it also maintains all d replicaset for all d deployment, it kips a recrd of it
  volumes                   imp  for deploying db appl ,we lokked at  PV wher a piece of storage can b created withinin our cl8 n managed by cl8 by d kube service                                                                    dats y u can rolbk n rollout
  configMaps      .......  since we create portable containerise applications , wich means our Developers don't hard code , since they dnt hardcode 
  if they are tryin to pass
    db-username: ${username}     ie in the dockerfile this info are not hardcoded, so we use config maps to pass this inf whiledeploying our contaainerized appl in k8
    db-password: ${password}            so config is goin 2 be used for us to pass additional config that were not nt passed in d docker file or any additonal config
    dockerHub-credentials , ssh-keys                that the container need to run effectively . its used to pass non-confidntial data e.g we v smt like db-password: ${password}
                                               this is a secret info, hw do we pass dis info in our cls8 or we wnt to dpeloy appl n our images ar in private repositories
                                we nid our dockerhub credentials, hw ar we goin to pass d credentials for dockerhub, or we wnt to pass some ssh keys all of dis will be passe
  secrets                                                                            using secrets
  namespace :       *********** helps us to create virtual cl8 within our k8 cl8, e.g u can create ns for dev env, prod , stagin  n ts also gud for security, e.g u can decide
    virtual clusters within our k8s.                                           that developers wil only wrk in the dev ns, they v nothing to do with prod ns

ResourceQuota    ********* also a gud k8 obj, since we v ns we can restrict hw much resources each ns can create to uptimize our resource usage in k8 ,,, using resourcequota
  node   ********* very imp, bc dis is wher all d workload are deployed, thay are dpeloyed in worker node , we v nodeselector, nodeaffinity, node affinity 
  what happens with node is that:  by default the scheduler assigns pods to be 
    created in node with sufficient resources 
  nodeSelector   : if we wnt a pod to be created only in a particular node (nodeselector or nodeaffinity)
    use to restrict the node(s) where pods should be created     
  nodeAffinity 
    use to restrict the node(s) where pods should be created 
  podAffinity  : this is used to prioritize     ,, use pod affinity to assign high priority to the pod   ..    (INTERVIEW QUESTION)
    PriorityClasses     ...................    lets assume u ar deployin multiplse appl, with d help of affinity ur prod app wil tk priority, 
      prod-app [high priority]  
      dev-app  [low priority]
      test-app [meduim priority]
    dev-app is running   , you are deployin prod app bt its 
    prod-app is pending due to insufficient resources  bc u hv deploy dev app already
    If podAffinity is enforced dev-app pods will be evicted and    
    prod-app pods will be running  ,, bc my server doesnt hv suficient resources

1:46:45             ,, if the pod is nt ready liveness will restart it to make sure it comes up 
Health Checkers:
  liveness probes/
  readiness probes/   lets assume we v 4containers running, b4 traffic is routed to a pod a health check is performed using readiness probe, it mksure trafic is nt routed
     users/apps/admins --->appSVC---> 4pods[appsContainer]                                                      to a pod thats nt ready to receive trafic
     http:
        /java-web-app
    curl -v localhost:8080/java-web-app    e.g to run the curl command on ur command line, to run a test if ur appl is running, so this is like a health test as well
                                        so readiness probe wil mk sure that b4 trafic is routed to a pod by a service, the pod is ready to receive trafic
                                       so if a pod is nt ready, readines probe wil remove the pod frm the loadservice algorithm bc the appSVC cant route trafic to the pod
   
startup probes        ..... for containers that are slow to start if we dnt wnt the container to fail, we can use startup probes

###################### this are imp stuff to tk note of    #############




you;ll go tru the security video 
security


portable containerise applications  
our Developers don't hard code 
    db-username: ${username}
    db-password: ${password} 
    dockerHub-credentials , ssh-keys 




QUESTION
why do we store logs in s3 bucket????????  frm my knowlwddge its like space consuming , usually  for some time we jst get rid of it
ANS:
that wil depend on mgt


                                                  
                                                  
                                                  S3 BUCKET
1:54:40
moving logs  into s3 bucket

simon@k8:~/efk$ aws s3 ls                 ********* now we see the buckets we already hv 
class27adevops
landmark27
simon@k8:~/efk$          ******* so if  simon@k8:~/efk$   is our elastic search server and i v some log files, we can copy the logs into any of the buckets i v chosen to
                                                             move my log file into , so that implies we can store our log files in elastic search for 1month after 1mth
                                                                  we can move the logs to s3 bucket.


elastic-search:
  storage for log files 
    nodes 
    pods   

s3:
  objects  storage ,
objects like
    log files  
    audio files   
    video files 
    archive files 
  storage classes 



##################### HE SAID AFTER THE break THEY WILL COVER THE BELOW , bt the video didnt continue again #####################


                                               VS CODE /TERRAFORM

Terraform:

Terraform introduction:
  Terraform is an Infrastructure as a code [IaC] tool 
  Creating VMs using hypervisor:
    virtualBox  
    vmWare 
    cetrix Zen
    hyperV  
      Console  
      commands   
      IaC = VAGRANT  
  Cloud providers / platforms:
    AWS   
    GCP  
    AZURE  
    IBM  / REDHAT 
  We can create resources in cloud platforms using:

    Console = GUI 
    CLI = 
    IaC:
      Terraform [WORKS FOR ALL PROVIDERS]  
      cloudFormation [AWS]
      ansible 
      python-sdk 


                                             
Use VS CODE to easily write and modify files, scripts and codes. 
Terraform codes are written in:
    Hashicorp configuration language = HCL 
===========================
1. Create a dbServer in aws using terraform
1. create a vpc in aws using terraform



resource "aws_instance" "web" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = "t3.micro"

  tags = {
    Name = "HelloWorld"
  }
}

Authenticated and authorised using:
  IAM  

provider "aws" {
  region = "us-west-1"
}
resource "aws_instance" "db"{
  ami             = 
  instance_type   = "t2.micro"
  key_pair        = "class27"
  security_groups = ['sg1', 'sg2', 'sg10' ]
  tags = {
    Name = "db-server"
    Environment = "production"
  }
}

terraform:
  init  = initialises a terraform dir and 
          download providers plugins 
  validate = validate tf files 
  plan     = 
  apply 
  apply --auto-approve 
  destroy  
  destroy --auto-approve 
  format
 = records all Infrastructures created  
  terraform show or cat terraform.tfstate
  import  --bring resources under the mgt of terraform  
            which were not created by terraform
  modules  
  variables  
  outputs 
  workspaces
  terraform.tfstate mgt     
     locally [5 Engineers]
        Paul
           terraform apply  
        Esther
           terraform apply 
     remotely 
        s3 backend
        dynamoDB table locks  























