check the EFK/ ELK powerpoint
WE ARE GOING TO SEE HOW TO USE ELASTICSEARCH, KIBANA AND BEATS

we are talking bout elastic stack , lock stack and kuberna or elastic stack fibric and kuberna
we are goin to be deploying this stack in our k8 cl8 using helm


REASON WE NID TO DO THIS:        **************** powerpoint
monitoring, alertingand log aggregationare essential for the smooth functioning of a production grade k8 cl8

e.g 
when u want to access an applictaion that is running, in k8 we can check the logs
kubectl get pod
we see the pods that are running in the defaulf dev space
NAME                        READY     STATUS
hello-app-6hztm              1/1         running
tdapp-759f7c6687              1/1         running

we can check the logs of ecah of the appl
we can describe the pod
kubectl describe  tdapp-759f7c6687     
OR check/get the logs  of the pod
kubectl logs tdapp-759f7c6687     
we can see the appl was deployed using the spring booth frame work
if u look at the appl, you can visualize the logs
********************if ders anytin wrong with your appl you can check the logs it will tell whats happening

############## kubectl get deploy
NAME                      READY         
ngninx-nginx-ingress        1/1
tdapp                       2/2 

**************** kubectl logs tdapp
Error from server (Not found): pods tdapp not found 
####################### ie it has to be in pods to check the logs , ( run kubectl get pod, first before running kubectl get logs tdapp)

*****************************  9:10
##############  but when u have multiple appl and ther are issues,
kubectl get pod -A        * TO get all the pods in all the name spaces
we see all the pods running
NAMESPACE            NAME    
apm                 grafana
apm                 promethus
kube-system         cluster-autoscaler-6cf74d69f7
grafana pod is running
the alertmanger   , incharge of sending alert, if smt is going wrong 
kubestate -maetrics ttaht gathr all the data, whatever is happening within our k8 objects eg pods, deplyment, rs, rc,  even nodes
node exporter is also gathering whatererv is happening in the nodes, the metrics  node exporter is gathering them 
promrthus sererv pod , it will scrap all of this data into the promethus  time series databasea nd once the dta has been scrapped we can visualise what is hapening using 
grafana

**************** so if we hv 100 pods we cant be running kubectl for all of the pods, that will be a tidious task
kubectl logs cluster-autoscaler-6cf74d69f7
ERROR form server not found clustr-autoscaler      ************************** this is bc we didnt check in the correct namespace (we nid to pass the namespace)
kubectl logs cluster-autoscaler-6cf74d69f7 -n kube-system
the logs are successfully generated                     8:25

HOW ARE WE ABLE TO CENTRALIZE OUR LOG MGT  ************************ powerpoint 12:40
we can use Elastic stack along with Docker to collect, process, store and visualize logs of microservices

WHAT IS ELASTIC STACK  .........................the vendor is ELASTIC
its an open source app from elastic designed to take data From any source and in any format and then search, analyze,and visualize that data in real time.
it was formely known as ELK stack, in wich the letters in the name stood for the appl in the grp. Elasticsearch, lahstack,and kibana. A fourth app, Beats, was 
subsquently added to the stack.

ELATICT search: ie now we can use elastic stack to staore data and to index the data, indexing streamas of semi-structured data, e.g logs or decoded ntwrk packets.
************* so elastic search will act as a container, it will store all the logs  ...... this i sthe use of elastic stack, it also does indexing.
it will index the logs , so when the logs are inside elastic search or the semi structure data n some ntwrk packages are inside elasticsearch waht happens is 
elasticsearch can do indexing, it will index the logs , it will perform indexing ,indexing is just like a pattern , lets assume it wants to cfreate an index which is goin
to like be displaying logs on a timestamp basis so that whatever is happening in the cl8 , u can use ur timestamp to know aht is goin on, so that can tk place, therfore
elastic search is goin to act as a store n a kind of an analyzer.. thats what elastic search is goin to do.
once the logs are stored, the next thing is that we want to visuaize these logs and we can use kibana  .... (THE SAMe way we saw grafana)

KIBANA: is an open source analytics and visualizationplatform designed to work with elasticsearch. it can be used to search,view n interact wit data stored in elasticsearc
indices, allowing sadvanced dtaa analysis n visualing dta in a variety of charts, tables n maps

the nxt componant is called beats
BEATS: open source data shipprs that can be installed as agents on servers to send operational data directly to Elasticsearch or via logstash, wher it can be further
processed n enhanced.
Therea re a number of beats for diff purposes.
FILEbaet    , logfile
metricbeat  , metrics   e.g metric data like cpu, memory
packetbeta , netwrk data
heartbeat: uptime monitoring

WE ARE GOING TO SEE HOW TO USE ELASTICSEARCH, KIBANA AND BEATS

in few words
filebeats collects data from the log files and sends it to logstash
logstssh enhances the data and sends it to elasticsearch
elasticsearch stores and indexe the data
kibana displays the data stored in elasticsearch

ELK Stack Architecture
           2) data processing        3)storage           4)Visualize
1)Log

            logstash                 elatsicsearch      kibana

however one more component is needed or data collection called beats. this led Elatic to rename ELK as the Elastic stack.

            Data collection          data processing        3)storage           4)Visualize
1)Log

              beats                       logstash                 elatsicsearch         kibana (can also be used to share)


PREREQUISITE TO DEPLOY ELastic Stask     .....23:00

we need a k8 cl8 with storage calss configured

kubectl get pv 
to check for our persist volume we created initially
kubectl get sc
to see we hv a dynamic storage class 
************* also if we have deploy k8 using kops, it would have as well cm with dynamic storgae class


create namespace
kubectl create ns efk
add
we are using helm to deploy efk
helm repo add elastic https://helm.elastic.co
we can go to this site to search the chart that are der   :https://helm.elastic.co
helm repo update
helm repo ls 
we wnt to deploy elasticsearch
searching the repo we just added
helm search repo elastic **************** searching the repo we just added
we can see all the charts  in the repo
this is the chart we want to use :   elastic/elasticsearch
helm show values elastic/elasticsearch
lets see the values in the chart
lets createa anew file 
mkdir efk
cd efk
ls                                                  (redirectin it into this custom file)
helm show values elastic/elasticsearch >> elasticsearch.values  
ls 
elasticsearch.values
vi elasticsearch.values                   ................ 31:00
we see the valuesfile that will be used to deploy elastic search
he changed replicas from 3 to 1 and master node from 3 to 1 ,, just for this practical

helm install elasticsearch elastic/elasticsearch     ........(using the default values file)
   It will deploy elasticsearch in the default namespace 
   It will deploy elasticsearch using default value file 
helm install elasticsearch elastic/elasticsearch \
  -n efk -f elasticsearch.values              .....(using our custom file)  ..... dnt use the default file use the custom file
it will be deployed in the efk ns we created earlier

kubectl get all -n efk 
nothing available
1) ######################## 
helm install elasticsearch elastic/elasticsearch -n efk -f elasticsearch.values    
kubectl get all -n efk 
now we see the elasticsearch pod coming up and its being deployed as a stateful pod 
kubectl get pvc -n efk
it has pvc for the elasticsearch pod with 30Gi
this is bc we hv dynamic class storage configured in our env so data can be stored bc elastic search wil stpre the data
kubectl get pod -n efk 
we can see the pod and the service created

2) ##############     WE WANT to deploy kbana    .... (helm install elastic/kibana)

helm show values elastic/kibana
we see the default values for kibana
one replica is being deployed and dats what we need
we hv some cinfigs maps that wil be deployed
config maps are additional configuration
we see the service type is CLusterIP and we can work with that bc we have nginx already deployed
but if nginx wasnt deployed, we would have changed the service type to nodeport or lb 
all the default values are ok
the chart we are using for thais deployment ias elastic/kibana
################### deploying kibana
(its deployed in the efk name space)  
helm install  elastic/kibana -n efk      ... (since we ar not changin any values we wont use -f) bc we wont use any customized file for the deployemt, we did nt chnGE D 
VALUES file           
kubectl get po -n efk
we see kibana was deployed using deployment, k8 object
kubctl get statefulset -n efk
we see elasticsearch was deployed using stateful set
bc elastic search comes with a database, its the database wher all the logs are shipped into
and logs that hv been shipped into elasticsearch,we can visualize the log using kibana
filebeats will ship the log into elasticsearch
***********************we now install filebeat
helm install filebeat elastic/filebeat -n efk        (in the efk namesapce)
also we are nt changing any values for filebeat 
we nid filebeat to be running in all the nodes, so it can be collecting the logs and shipping it 
therefore, it will be deployed as a daemonset
3) ##########
helm install filebeat elastic/filebeat -n efk
now deployed
kubectl get ds -n efk
filebeat-filebeat
kubectl get svc -n efk
we hv 3services
NAME                                  TYPE
elasticsearch-master                 CLusterip
elasticsearch-master-headless        Clusterip
kibana-kibana                        Clusterip

we need to access kibana service externally 
right now we hv just the clusterip we hv used to deploy kibana
but see we already hv ingress controller deployed in our cl8, we dnt ned to worry abt nodeport
4)  ***************************so we are goin to create ingress rule for kibana
we created one yesterday for alertmanger, promethus server and grafana
so hw do we wnat to access kibana, we are creating an ingress rule
############# host base routing

# ingress2.yml
apiVersion: extensions/v1beta1  # networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-rule-1
  namespace: efk
spec:
  ingressClassName: nginx
  rules:
  - host: kibana.dominionsystem.net         (hostname), to accses the backend services, type: kibana.dominionsystem.net 
    http:
      paths:
      - backend:
          serviceName: kibana-kibana       (gotten by running kubectl get svc )
          servicePort: 5601                 (gotten by running kubectl get svc ) at work u also maintain the right service port

this is host basse routing
to create thsi ingress resource
vi  ingress-rule.yml   ,  and paste
kubectl apply -f ingress-rule.yml
ingress.extensions/ingress-rule-1 created
kubectl describe ingress ingress-rule-1 -n efk
 we can see kibana.dominionsystem.net
and its routing trafic to the backendport on the port number   kibana-kibana:5601(10.0.1.124:5601)
its port forwarding taking place her     ................ 52:00
bc when u type : kibana.dominionsystem.net
ur traffic is routed via the lbalancer service
&once it comes into d cl8,it talks 2 dis service kibana-kibana:on d service port number 5601 & kibana service route d service to dcontainer on dis port no.(10.0.1.124:5601)
this is what is happens
in aws we hv to go to route 53 n create dns record

52:16



Summary 
   Talking points:
1. Container Orchestra 
      kubernetes Orchestrate containerise applications 
      kubernetes Orchestrate containerise micro-service applications 

2. Self healing capacity, scalability, Disaster recovery, LB,  
   
3. Automated rollouts and rollbacks

Kubernetes architecture:
  control plane /master-node:
    apiServer 
       kubectl get  
       helm install   
       ui    
    etcd [db] 
    scheduler 
    controller-managers 
    kubernetes-cni:
      kube-proxy 
      kube-dns 
  worker-nodes: 
    container-runtime[docker, containerD]
    kubelet
    kubernetes-cni:
      kube-proxy 
      kube-dns 
Kubernetes objects:
  services = service discovery :
    users/admin/app--->SVC--->pod[APPcontainer]
    service-TYPES:
      ClusterIP :
        spec:
          type  
          selector:
            app: myapp 
      NodePort   
      LoadBalancer  
      ExternalName 
      ingress 
         ClusterIP 
  pods---> houses containers  
     SingleContainerPods  
     MultiContainerPods 
  PodTemplate:
    metadata:
      labels:
        app: myapp 
    spec:
      containers
  ReplicationControllers:
    spec:
      selector:
        app: myapp 

  ReplicaSet:
    selector:
      matchExpressions [set-based]
      matchLabels: [eqaulity based]
        app: myapp 
  StatefulSet 
  DaemonSet 
  Deployment 
  volumes 
  configMaps  
  secrets
  namespace :
    virtual clusters within our k8s. 
  ResourceQuota 
  node 
    by default the scheduler assigns pods to be 
    created in node with sufficient resources 
  nodeSelector
    use to restrict the node(s) where pods should be created     
  nodeAffinity 
    use to restrict the node(s) where pods should be created 
  podAffinity
    PriorityClasses   
      prod-app [high priority]  
      dev-app  [low priority]
      test-app [meduim priority]
    dev-app is running   
    prod-app is pending due to insufficient resources 
    If podAffinity is enforced dev-app pods will be evicted and    
    prod-app pods will be running 

  Health Checkers:
  liveness probes/
  readiness probes/
     users/apps/admins --->appSVC---> 4pods[appsContainer] 
     http:
        /java-web-app
    curl -v localhost:8080/java-web-app 
  startup probes
  security


portable containerise applications  
our Developers don't hard code 
    db-username: ${username}
    db-password: ${password} 
    dockerHub-credentials , ssh-keys 

Terraform:

elastic-search:
  storage for log files 
    nodes 
    pods   

s3:
  objects 
    log files  
    audio files   
    video files 
    archive files 
  storage classes 

Terraform introduction:
  Terraform is an Infrastructure as a code [IaC] tool 
  Creating VMs using hypervisor:
    virtualBox  
    vmWare 
    cetrix Zen
    hyperV  
      Console  
      commands   
      IaC = VAGRANT  
  Cloud providers / platforms:
    AWS   
    GCP  
    AZURE  
    IBM  / REDHAT 
  We can create resources in cloud platforms using:

    Console = GUI 
    CLI = 
    IaC:
      Terraform [WORKS FOR ALL PROVIDERS]  
      cloudFormation [AWS]
      ansible 
      python-sdk 
Use VS CODE to easily write and modify files, scripts and codes. 
Terraform codes are written in:
    Hashicorp configuration language = HCL 
===========================
1. Create a dbServer in aws using terraform
1. create a vpc in aws using terraform



resource "aws_instance" "web" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = "t3.micro"

  tags = {
    Name = "HelloWorld"
  }
}

Authenticated and authorised using:
  IAM  

provider "aws" {
  region = "us-west-1"
}
resource "aws_instance" "db"{
  ami             = 
  instance_type   = "t2.micro"
  key_pair        = "class27"
  security_groups = ['sg1', 'sg2', 'sg10' ]
  tags = {
    Name = "db-server"
    Environment = "production"
  }
}

terraform:
  init  = initialises a terraform dir and 
          download providers plugins 
  validate = validate tf files 
  plan     = 
  apply 
  apply --auto-approve 
  destroy  
  destroy --auto-approve 
  format
 = records all Infrastructures created  
  terraform show or cat terraform.tfstate
  import  --bring resources under the mgt of terraform  
            which were not created by terraform
  modules  
  variables  
  outputs 
  workspaces
  terraform.tfstate mgt     
     locally [5 Engineers]
        Paul
           terraform apply  
        Esther
           terraform apply 
     remotely 
        s3 backend
        dynamoDB table locks  























