check the EFK/ ELK powerpoint
WE ARE GOING TO SEE HOW TO USE ELASTICSEARCH, KIBANA AND BEATS

we are talking bout elastic stack , lock stack and kuberna or elastic stack fibric and kuberna
we are goin to be deploying this stack in our k8 cl8 using helm


REASON WE NID TO DO THIS:        **************** powerpoint
monitoring, alertingand log aggregationare essential for the smooth functioning of a production grade k8 cl8

e.g 
when u want to access an applictaion that is running, in k8 we can check the logs
kubectl get pod
we see the pods that are running in the defaulf dev space
NAME                        READY     STATUS
hello-app-6hztm              1/1         running
tdapp-759f7c6687              1/1         running

we can check the logs of ecah of the appl
we can describe the pod
kubectl describe  tdapp-759f7c6687     
OR check/get the logs  of the pod
kubectl logs tdapp-759f7c6687     
we can see the appl was deployed using the spring booth frame work
if u look at the appl, you can visualize the logs
********************if ders anytin wrong with your appl you can check the logs it will tell whats happening

############## kubectl get deploy
NAME                      READY         
ngninx-nginx-ingress        1/1
tdapp                       2/2 

**************** kubectl logs tdapp
Error from server (Not found): pods tdapp not found 
####################### ie it has to be in pods to check the logs , ( run kubectl get pod, first before running kubectl get logs tdapp)

*****************************  9:10
##############  but when u have multiple appl and ther are issues,
kubectl get pod -A        * TO get all the pods in all the name spaces
we see all the pods running
NAMESPACE            NAME    
apm                 grafana
apm                 promethus
kube-system         cluster-autoscaler-6cf74d69f7
grafana pod is running
the alertmanger   , incharge of sending alert, if smt is going wrong 
kubestate -maetrics ttaht gathr all the data, whatever is happening within our k8 objects eg pods, deplyment, rs, rc,  even nodes
node exporter is also gathering whatererv is happening in the nodes, the metrics  node exporter is gathering them 
promrthus sererv pod , it will scrap all of this data into the promethus  time series databasea nd once the dta has been scrapped we can visualise what is hapening using 
grafana

**************** so if we hv 100 pods we cant be running kubectl for all of the pods, that will be a tidious task
kubectl logs cluster-autoscaler-6cf74d69f7
ERROR form server not found clustr-autoscaler      ************************** this is bc we didnt check in the correct namespace (we nid to pass the namespace)
kubectl logs cluster-autoscaler-6cf74d69f7 -n kube-system
the logs are successfully generated                     8:25

HOW ARE WE ABLE TO CENTRALIZE OUR LOG MGT  ************************ powerpoint 12:40
we can use Elastic stack along with Docker to collect, process, store and visualize logs of microservices

WHAT IS ELASTIC STACK  .........................the vendor is ELASTIC
its an open source app from elastic designed to take data From any source and in any format and then search, analyze,and visualize that data in real time.
it was formely known as ELK stack, in wich the letters in the name stood for the appl in the grp. Elasticsearch, lahstack,and kibana. A fourth app, Beats, was 
subsquently added to the stack.

ELATICT search: ie now we can use elastic stack to staore data and to index the data, indexing streamas of semi-structured data, e.g logs or decoded ntwrk packets.
************* so elastic search will act as a container, it will store all the logs  ...... this i sthe use of elastic stack, it also does indexing.
it will index the logs , so when the logs are inside elastic search or the semi structure data n some ntwrk packages are inside elasticsearch waht happens is 
elasticsearch can do indexing, it will index the logs , it will perform indexing ,indexing is just like a pattern , lets assume it wants to cfreate an index which is goin
to like be displaying logs on a timestamp basis so that whatever is happening in the cl8 , u can use ur timestamp to know aht is goin on, so that can tk place, therfore
elastic search is goin to act as a store n a kind of an analyzer.. thats what elastic search is goin to do.
once the logs are stored, the next thing is that we want to visuaize these logs and we can use kibana  .... (THE SAMe way we saw grafana)

KIBANA: is an open source analytics and visualizationplatform designed to work with elasticsearch. it can be used to search,view n interact wit data stored in elasticsearc
indices, allowing sadvanced dtaa analysis n visualing dta in a variety of charts, tables n maps

the nxt componant is called beats
BEATS: open source data shipprs that can be installed as agents on servers to send operational data directly to Elasticsearch or via logstash, wher it can be further
processed n enhanced.
Therea re a number of beats for diff purposes.
FILEbaet    , logfile
metricbeat  , metrics   e.g metric data like cpu, memory
packetbeta , netwrk data
heartbeat: uptime monitoring

WE ARE GOING TO SEE HOW TO USE ELASTICSEARCH, KIBANA AND BEATS

in few words
filebeats collects data from the log files and sends it to logstash
logstssh enhances the data and sends it to elasticsearch
elasticsearch stores and indexe the data
kibana displays the data stored in elasticsearch

ELK Stack Architecture
           2) data processing        3)storage           4)Visualize
1)Log

            logstash                 elatsicsearch      kibana

however one more component is needed or data collection called beats. this led Elatic to rename ELK as the Elastic stack.

            Data collection          data processing        3)storage           4)Visualize
1)Log

              beats                       logstash                 elatsicsearch         kibana (can also be used to share)


PREREQUISITE TO DEPLOY ELastic Stask     .....23:00

we need a k8 cl8 with storage calss configured

kubectl get pv 
to check for our persist volume we created initially
kubectl get sc
to see we hv a dynamic storage class 
************* also if we have deploy k8 using kops, it would have as well cm with dynamic storgae class


create namespace
kubectl create ns efk
add
we are using helm to deploy efk
helm repo add elastic https://helm.elastic.co
we can go to this site to search the chart that are der   :https://helm.elastic.co
helm repo update
helm repo ls 
we wnt to deploy elasticsearch
searching the repo we just added
helm search repo elastic **************** searching the repo we just added
we can see all the charts  in the repo
this is the chart we want to use :   elastic/elasticsearch
helm show values elastic/elasticsearch
lets see the values in the chart
lets createa anew file 
mkdir efk
cd efk
ls                                                  (redirectin it into this custom file)
helm show values elastic/elasticsearch >> elasticsearch.values  
ls 
elasticsearch.values
vi elasticsearch.values                   ................ 31:00
we see the valuesfile that will be used to deploy elastic search
he changed replicas from 3 to 1 and master node from 3 to 1 ,, just for this practical

helm install elasticsearch elastic/elasticsearch     ........(using the default values file)
   It will deploy elasticsearch in the default namespace 
   It will deploy elasticsearch using default value file 
helm install elasticsearch elastic/elasticsearch \
  -n efk -f elasticsearch.values              .....(using our custom file)  ..... dnt use the default file use the custom file
it will be deployed in the efk ns we created earlier

kubectl get all -n efk 
nothing available
1) ######################## 
helm install elasticsearch elastic/elasticsearch -n efk -f elasticsearch.values    
kubectl get all -n efk 
now we see the elasticsearch pod coming up and its being deployed as a stateful pod 
kubectl get pvc -n efk
it has pvc for the elasticsearch pod with 30Gi
this is bc we hv dynamic class storage configured in our env so data can be stored bc elastic search wil stpre the data
kubectl get pod -n efk 
we can see the pod and the service created

2) ##############     WE WANT to deploy kbana    .... (helm install elastic/kibana)

helm show values elastic/kibana
we see the default values for kibana
one replica is being deployed and dats what we need
we hv some cinfigs maps that wil be deployed
config maps are additional configuration
we see the service type is CLusterIP and we can work with that bc we have nginx already deployed
but if nginx wasnt deployed, we would have changed the service type to nodeport or lb 
all the default values are ok
the chart we are using for thais deployment ias elastic/kibana
################### deploying kibana
(its deployed in the efk name space)  
helm install  elastic/kibana -n efk      ... (since we ar not changin any values we wont use -f) bc we wont use any customized file for the deployemt, we did nt chnGE D 
VALUES file           
kubectl get po -n efk
we see kibana was deployed using deployment, k8 object
kubctl get statefulset -n efk
we see elasticsearch was deployed using stateful set
bc elastic search comes with a database, its the database wher all the logs are shipped into
and logs that hv been shipped into elasticsearch,we can visualize the log using kibana
filebeats will ship the log into elasticsearch
***********************we now install filebeat
helm install filebeat elastic/filebeat -n efk        (in the efk namesapce)
also we are nt changing any values for filebeat 
we nid filebeat to be running in all the nodes, so it can be collecting the logs and shipping it 
therefore, it will be deployed as a daemonset
3) ##########
helm install filebeat elastic/filebeat -n efk
now deployed
kubectl get ds -n efk
filebeat-filebeat
kubectl get svc -n efk
we hv 3services
NAME                                  TYPE
elasticsearch-master                 CLusterip
elasticsearch-master-headless        Clusterip
kibana-kibana                        Clusterip

we need to access kibana service externally 
right now we hv just the clusterip we hv used to deploy kibana
but see we already hv ingress controller deployed in our cl8, we dnt ned to worry abt nodeport
4)  ***************************so we are goin to create ingress rule for kibana
we created one yesterday for alertmanger, promethus server and grafana
so hw do we wnat to access kibana, we are creating an ingress rule
############# host base routing

# ingress2.yml
apiVersion: extensions/v1beta1  # networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-rule-1
  namespace: efk
spec:
  ingressClassName: nginx
  rules:
  - host: kibana.dominionsystem.net         (hostname), to accses the backend services, type: kibana.dominionsystem.net 
    http:
      paths:
      - backend:
          serviceName: kibana-kibana       (gotten by running kubectl get svc )
          servicePort: 5601                 (gotten by running kubectl get svc ) at work u also maintain the right service port

this is host basse routing
to create thsi ingress resource
vi  ingress-rule.yml   ,  and paste
kubectl apply -f ingress-rule.yml
ingress.extensions/ingress-rule-1 created
kubectl describe ingress ingress-rule-1 -n efk
 we can see kibana.dominionsystem.net
and its routing trafic to the backendport on the port number   kibana-kibana:5601(10.0.1.124:5601)
its port forwarding taking place her     ................ 52:00
bc when u type : kibana.dominionsystem.net
ur traffic is routed via the lbalancer service
&once it comes into d cl8,it talks 2 dis service kibana-kibana:on d service port number 5601 & kibana service route d service to dcontainer on dis port no.(10.0.1.124:5601)
this is what is happens
in aws we hv to go to route 53 n create dns record    ....... 52:16
now we can access it online by typing kibana.dominionsystem.net

now we hv some pod taht we hv deployed
kubectl get infress -o wide
we can see the address
lets get ingress in the right namespace
kubectl get ingress -n efk -o wide 
we can see ths adress , its the address of our lb
nslookup kibana.dominionsystem.net 
we see its resolving to the adress , (i bliv its the lb address)  .......56:45
so when u ttype kibana.dominionsystem.net , in the background, its runing dns lookup, it wants to find out, what is the address, bc every server has a name 
when endusers type          ## it talks to    >>   this talks to   >> talks to >>  
******************* kibana.dominionsystem.net >> Ingress controller >> kibanaSVC >> Kibanapods
the ingress controller is using the rules defined in this ingress resource and to talk to the ingress controller pod, it goes tru the lb and the lb we hv created Arecords
so if you want to talk to the lb just type kibana.dominionsystem.net, traffic is routed, it gets to the ingress controller, ic routes  to the kibanapods tru the kibanaSVC

NOW THAT WE HAVE ALL THESE ESTABLISHED 
now hes logged into elastic search engine online   >> (i guess)
he said now this is my kibana dashboard, now i wnt to knw what is goin on in my cl8
this is just abt log mgt, elastic search, filebead and kibana
elastic seacrh is der, filebead collects the data an ships the data into elastic search, elastic search will index the data n kibana wil help us visualize to explore n to
even share the info that is shared in elastic seqrach
lets try to explore we hv overview: u can add some integration and stuff, discover , dashboard: u can create  a dashboard or instal some smaple data
in discover: let screate index pattern ................ 1:02:22
name:  filebeat     ... am tying filebeat bc its the shipper
what can we use, we can use timestamp then we can just create index, we can add other fields if we need to 
very soon we wil be seeing whta is happening insid ethe cl8, it will be doing some data streaming
we hv a lot of option we can do, index policy, index mgt , roll back ..... etc 
now we hv 6,268hits
now its goin gto be capturing evrytin that is happeing in the cl8 based on the timestamp , whatevr is happening u can see evrytin
we can add a field  ...... 1:04:50
so dis is jst u creating tryin to creste some data tracing pattern like when u hv elastic serach u can create like hw long do u reatin logs  in elastic search, we can
decide to retain logs in elastic serach for lke a month  cos u dnt wnat to keep logs ther for so long. bc logs are  goin to be generated constantlyso our retainment policy
can just be for a month  ,,, bt it also depends , it can be decided by mgt  ,,, so in INTERVIEW, we can say that logs are kept for one month afetr wich we can decide to 
move the logs from elastic search n save them in myb aws s3 bucket
i just went to data stream, we can visyalize data in a format wher even non IT folks can underatnd, 

A log is a variable file, which means whatso eevr is hapening, they keep on changing, they are goin to be constantly changing
its just like when u run  : top
tops stells u whats happening in r system , hw many users, the processes that are runing , the values keep cahanging
the events that are taking place are vaarible events 

#############################

####################################  1:11 35
###################### PLS THERs one ASPECT  OF K8  WHich I HVAE NT COVERD WITH YOU GUYS n some other aspects WHICH I WILL COVER IN BOOT CAMP , bt the aspect i was supose 
 to cover i decided to share the video with u guys bc i wnat to shed some light in the 2nd half of the class, on certain terraform concepts...... k8 security n rbac


****************************** K8 TALKING POINT



Summary 
   Talking points:
1. Container Orchestra     ... ie
      kubernetes Orchestrate containerise applications 
      kubernetes Orchestrate containerise micro-service applications 

******* we choose k8 bc

2. Self healing capacity, scalability, Disaster recovery, LB,  
   
3. Automated rollouts and rollbacks

we use k8 objects to deploy workload in our cl8
imp objs to tk note of 
services for  service discovery
pods
rc , rs, 
satefulset  ,, which we just saw today
daemonset
deployment
volumes
configmaps
secrets
namespace concept
ResourceQuota
nodeAffinity
podAffinity
security

*********these are very imp concepts for  to b able to xplain them
before the objects, we spoke abt the architecture
in the architecture, we saw the controlpaane, wich is also called the master node
apiserver
etcd
scheduler
controller-managers
kubernetes-cni  :
kube-proxy
kube-dns
workernodes

Kubernetes architecture:
  control plane /master-node:
    apiServer        ************ this receives the api call  eg         kubectl get  ,        helm install   
       kubectl get  
       helm install   
       ui       *********************  using the k8 dashboard with  some ui commands , u wil be communiatoin wit the APiserver, once it receives it it persist it in etcd
    etcd [db]   (db or keyvalue store)   ,,,, then whatevar info has been stored here the scheduler is notified that some wrk nids to be done
    scheduler         ************ then transfer d inf to d workernode ,kublet receives d info n immediaterly, d scheduler schedules d pod in d worker node n d resorce  
    controller-managers                                                                                              is created
    kubernetes-cni:
      kube-proxy 
      kube-dns 
  worker-nodes:                                                                                    1:21:10
    container-runtime[docker, containerD]  ,, cud be docker in the past or today cntainerD ...i pu docker in d past bc docker was depreciated jst last yr n ur xperience 
    kubelet                                                                                           is nt  docker was only 
    kubernetes-cni:        the cni is also ruuning in the worker node like in the master node
      kube-proxy 
      kube-dns 
Kubernetes objects:
  services = service discovery :   our app are running inside a pod so to acces them u must go tru a service
    users/admin/app--->SVC--->pod[APPcontainer]          user,admin , app can access the pod tru a service e.g spring talk to the db pod tru the service 
    service-TYPES:                                                                               the pod to d service is an endpoint n d service performs this task does lb
      ClusterIP :              the default service in k8                                                                          that is service discovery
        spec:
          type  
          selector:
            app: myapp 
      NodePort         *********** external
      LoadBalancer      ****************** external
      ExternalName       **************** we didnt demo this, used to cmm with app outised ur cls8
      ingress            ***has  alot of advantages .in security its xplained as additional layer of security, we can use it to deploy app using clusterIP wich increses 
         ClusterIP                                  security in our cl8
  pods---> houses containers       ************* wher appl are run
     SingleContainerPods         *********** truout this course we hadjst had singlecontainer pod bt we can v some  pod that act as ultilty  pod to the main pod 
     MultiContainerPods       ************    and in that case we can hv multi container pod
  PodTemplate:
    metadata:
      labels:
        app: myapp 
    spec:
      containers
  ReplicationControllers:
    spec:
      selector:
        app: myapp 

  ReplicaSet:
    selector:
      matchExpressions [set-based]
      matchLabels: [eqaulity based]
        app: myapp 
  StatefulSet 
  DaemonSet 
  Deployment 
  volumes 
  configMaps  
  secrets
  namespace :
    virtual clusters within our k8s. 
  ResourceQuota 
  node 
    by default the scheduler assigns pods to be 
    created in node with sufficient resources 
  nodeSelector
    use to restrict the node(s) where pods should be created     
  nodeAffinity 
    use to restrict the node(s) where pods should be created 
  podAffinity
    PriorityClasses   
      prod-app [high priority]  
      dev-app  [low priority]
      test-app [meduim priority]
    dev-app is running   
    prod-app is pending due to insufficient resources 
    If podAffinity is enforced dev-app pods will be evicted and    
    prod-app pods will be running 

  Health Checkers:
  liveness probes/
  readiness probes/
     users/apps/admins --->appSVC---> 4pods[appsContainer] 
     http:
        /java-web-app
    curl -v localhost:8080/java-web-app 
  startup probes
  security


portable containerise applications  
our Developers don't hard code 
    db-username: ${username}
    db-password: ${password} 
    dockerHub-credentials , ssh-keys 

Terraform:

elastic-search:
  storage for log files 
    nodes 
    pods   

s3:
  objects 
    log files  
    audio files   
    video files 
    archive files 
  storage classes 

Terraform introduction:
  Terraform is an Infrastructure as a code [IaC] tool 
  Creating VMs using hypervisor:
    virtualBox  
    vmWare 
    cetrix Zen
    hyperV  
      Console  
      commands   
      IaC = VAGRANT  
  Cloud providers / platforms:
    AWS   
    GCP  
    AZURE  
    IBM  / REDHAT 
  We can create resources in cloud platforms using:

    Console = GUI 
    CLI = 
    IaC:
      Terraform [WORKS FOR ALL PROVIDERS]  
      cloudFormation [AWS]
      ansible 
      python-sdk 
Use VS CODE to easily write and modify files, scripts and codes. 
Terraform codes are written in:
    Hashicorp configuration language = HCL 
===========================
1. Create a dbServer in aws using terraform
1. create a vpc in aws using terraform



resource "aws_instance" "web" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = "t3.micro"

  tags = {
    Name = "HelloWorld"
  }
}

Authenticated and authorised using:
  IAM  

provider "aws" {
  region = "us-west-1"
}
resource "aws_instance" "db"{
  ami             = 
  instance_type   = "t2.micro"
  key_pair        = "class27"
  security_groups = ['sg1', 'sg2', 'sg10' ]
  tags = {
    Name = "db-server"
    Environment = "production"
  }
}

terraform:
  init  = initialises a terraform dir and 
          download providers plugins 
  validate = validate tf files 
  plan     = 
  apply 
  apply --auto-approve 
  destroy  
  destroy --auto-approve 
  format
 = records all Infrastructures created  
  terraform show or cat terraform.tfstate
  import  --bring resources under the mgt of terraform  
            which were not created by terraform
  modules  
  variables  
  outputs 
  workspaces
  terraform.tfstate mgt     
     locally [5 Engineers]
        Paul
           terraform apply  
        Esther
           terraform apply 
     remotely 
        s3 backend
        dynamoDB table locks  























