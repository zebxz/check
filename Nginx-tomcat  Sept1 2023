INSTALLING NGINX AND CONFIGURING DNS/ROUTE 53


disable salinex but its not working
tried to setup ngnix but it says not suported


39:44
for you to explain in an interview that you have experience in ngnix webservers is also important 





                 START
 WHAT IS CONTAINERIZED SERVVER 
i know we have seen a situation where our applications are deployed and endusers need to access the application
so we will configure an ngnix webserver
***1.27so ill launch an ngnix webserver
we will install it on a redhat
we are launching a virtual machine, you can now see the problem with vmware
you can see how long it is taking a virtual machine to come up
when we were creating a containerized machine it took like how long, ?
containers take seconds but virtuals takes minutes
so containers are light weight but virtual machines are heavy weight
** so any company you take containerization to it’s a big deal


                                
3:13, now we hv this application, in the appl, we hv 3tomcat servers and in each one we hv their public ip address,
4.40 now we hv this project ngnix
n this project i have maven web app, hv cloned the application.
cd maven web app
ls
mvn clean
mvn package to create our packages
now it has created the package maven web app.war
so am going to becopying this war files into all my tomcat servers
so am doing deployment in the webapp dir in tomcat
8;50 ..... deployed to each of the tomcat server ( one 

It is not  a good practice that when you want to access your application, you access them directly where endusers traffic goes directly to the application server.

Sow we are going to create a proxy ngnix server, so its going to be acting a sour frontend proxy such that end users cannot talk directly, they must go through the ngnix webserver
12;45 ... now we go to the ngnix webserver we launched in aws
mow we access it in mobaxterm

Install ngnix
1)	We want to install ngnix to act as a proxy which implies that enduser traffic will be routed by the ngnix webserver so ngnix is acting as a load balancer, he receives the traffic from clients/endusers and route it to our backend 
application and our application will talk to our data bases  ***tomcat1&2* when users try to access their account they are entering their userName/ Password all these information needs to be stored in this database, very sensitive 
information   *** meee*** our backend application refers to tomcat app..
 *** tomcat1&2***    Backend tiers:
comes with high security = this is running in militarized zone = MZ
this includes application tier and database tier

    frontend tier  : Medium security = D-Militarised zone = DMZ
  Like the webservers, loadbalancers, jumpservers
38:57
eg we hv 60m users;
We dont allow traffic from app users to get to the app servers directly because if it does, it will be a security bridge which is highly risky.
therefore  , inbetween the appservers & users we place the webservsrs/LB
therefore, if u want to access these appservers, it is routed via the LB or webservers .. & this is very secured 
60mUsers ------> webserver/LB ----->appServers 

*** 15:15  now enable and start ngnix
  systemctl enable ngnix
to acces it ip address:80
on the server we can see the config file /etc/ngnix/ngnix.conf
copy the file  
and in maven we create a backup for the webserver
2)	We back up  the conf file by configuring the config file
move the file = sudo mv /etc/ngnix.conf  /etc/ngnix/ngnix
we recreate the file with our own configuration   ** i bliv , its in package management but i cant find the link yet
*** 17.55*** we copy our configuration iformation (includes our tomcat ip address) and paste it into our new config file 
 we could have 100 tomcat servers but for now we hv only 3
Vi /etc/ngnix/ngnix.conf   (which is now our new config file). Therefor I have this for my backend server
19:45, in the backend, I have my ngnix server any traffic that comes it talks to the ngnix server and its route it to any of the tomcat servers

3)	We ensure the configuration is valid by executing;  ngnix –t 
4)	Sudo ngnix –s reload but since I am already sudo ill run nginx –s 
5)	Sudo system ctl restart ngnix
6)	We check our browser if we can access ngnix server
It says 502 bad gateway, its because thers a security in linux called selinux which is enforced at this stage 
Therefore we will disable the security  by running:
Setsebool  –P httpd can_netwrk_connect true,,, we run this command to allow the proxy server to route traffic
 22:10, now when we access the ngnix server we see it routing traffic to ecah of the tomcat servers simulteanously
*********** we will be using other load balance like:
 ELSB: elastic load balancer


23:10
when we look at AWS web services we are going to study a very important concept called virtual private cloud where you will realise that in this architecture our frontend
load balancer will be in our public subnet while our application servers and databases will be running in the private subnet, so this is what is happening, ngnix is acting
as a load balancer


24:00
The Question is :
             HOW  DOES IT LOAD BAL OUR TRAFFIC 

The default option: to uses to bal traffic is called:
Round robin:  Request/traffic is distributed evenly accross the servers , with server weight taken into consideration, ngnix will be routing traffic to this backend servers 
depending on the load traffic will be distributed equally.
this method is used by default



26:00
  OTHER METHODS INCLUDE

Least connections:  A request is sent to the server with the least no of active connectors , also server weight is considered
iP hash: eg, we can decide that Ip address from Africa let it go to appserver1, Ip address of America let it go to server2 etc
the server to which a request is sent is determined from the client IP address. eg it can decide that any address that has a specific octcts, that is it comes from a specific
destination should go to appserver one etc. thats is what is called ip hash.


                        
                                        HOW WE SUSTAIN THE MAINTENANCE PERIOD
26:59
if a server is down or undergoing maintenance/ upgrading it, vi the config file and write : "down" beside the server ip address
28.15.....   afterwhich we run   
nginx -t to ensure the configuration is valid  then
nginx -s reload
system ctl restart ngnix
we reload the server and we see traffic is nt routed to it again bc it doesnt come up with the other ip address
we can put aserver down when we are upgrading or patching

Also for the server we want to allocate more traffic we will write "weight =4", 
eg
we have 5m requests/users, we can assign  1m to the other server with 8GB  and 4M to that server with 64GB, ie 4x the number of users
then reloaad

35.00, so in real time we will nt be exposing our application directly for endusers traffic, we are going to use a proxy 


QUESTION
i know we hv lB in AWS , THAT means if we are nt using nginx, we can use AWS LB
yes
and rhats what we will see when we get to k8 and AWS 
but its ver imp that you get the picture first
bc why do we need a webserver or Load balancer
load balancers helps to 
1)load balancers
                                                                    users >>> LB >>>> APP >>>> DB
2)security: Acts like a layer of security ie if anyone is trying to hack into our app it can be stopped at this stage which is very important 
3)health check: if its routing traffic/load balancing to about 30servers it ensures that it does not route traffic to a server that is not healthy, so it runs health check in 
the backend 
4)patching upgrading of our servers


 DO WE CONSIDER THE TYPE OF LOAD BALANCER OR IT DOESNT MATTER AT THIS TIME
39.45 at this point we are not considering the type of lb , bt for you to explain in an interview that you hv experience in ngnic webserv is very imp
bc we are going to be doing other managed loadbalancer like ELB
once u understand this aspect of it that too will be easy for you to understand


40.20you werw spekaing and said the nginx will be in the publuc subnet 
my question is if there are 2 available zones, does that mean there wil be 2 nginx in the 2 availabilty zones??

********* ANS 
if you have a VPC : virtual private cloud ie your private netwrk, in this private network you can have about 6subnets depending on the region 
lets assume you have 6subnets
so now we have 3private subnets and 3public subnets
take note this is smt we will stilll look at
now in aws global resources you have:
   global >>> Regions >>> Availability zone 
inside AZ you ahve data centers                               ****** or 18 servers 3 in each Azs
when you create a vpc where you want to launch your resources, assuming you want to create your VPC in US East1 ie us virginia, it has 6 diff AZs meaning that i can create
my 6subnets/servers in all this availabity zones to ensure high availability
if i have my ngnix webserver running in the public subnet, Take note that all these severs are in the same vpcs so they are in the same netwrk thefore nginx webserver can route traffic to all the servers/ec2s since all the servers are
in the same vpc they are in the same netwrk,
45:39  so my ngnix can laod bal traffic to all servers or ec2s running in both private and public subnet

43:07 But if we have vpc1 and vpc2 , to ensure that connection happens we are goin to do vpc pairing so that even though they are in diff netwrk the pairing puts them  in the 
same netwrk that way you can still use a webserver to load bal traffic accross all your servers and this is the adv of using aws


45.43  SO WE BASICALLY DO THIS CONFIGURATION & CREATE A DOCKER IMAGE WHICH WE CAN USE AND DEPLOY AND LINK WITH TOMCAT IMAGE AS WELL
ANS;
Dnt worry you are going to see the amplified version of what have explained here in k8
its just you may work in an org that is not yet implementing microservices as expected 
bc the app we dploy, we deploy in virtual machines 
so if you work in such coy or u hv task that relate to monolitic appl running in virtual machines , you shud understand how that traffic will can be routed
understanding this wil help you easily understand ingress when wwe get to k8



47:00 ... if a user wants to access any of those applications, are they going to be using their individual ips or virtual ip of the ngnix 

ANS:
Before when users want to access their application they have to access individual servers ip but now we have to put all of them under one ngnix webserver, so all the traffic 
via the ngnix webserver .
they dont have to type https://3.090.240/app/ but now, mylandmark/register


48:20
A service in aws we are going to do called DNS
Route 53 = DNS  ( Domian Name Service )
                                                                                                                                                               alias name
if u have dns, we go to our route 53 &create hosted zone, in this zone we will be routing the traffic, instaed of users to type 3.40.21, they can type www.landmark.org/app
traffic will be routed to to this backend application server bc we have configured Route53 with a public domain name or alias name for our ngnix address, so if you want to 
access this application its either you type the ngnix ip address directly or the public domain name we created

49.32  CONFIGURING ROUTE 53

 
52:00
IQ       WHAT HAPPENS WHEN YOU TYPE  app.com or google.com    (domain name service)
54:30
  once you type google.com it going to query global DNS searching for the webservers that are associated to this hostname and once it locates the server, immediately traffic
will be routed to that server


for example, why you type www.landmark.org, what happens is that in the backend you are going to query global DNS(global Domain name service) and there is a command is the 
backend called NS look up, it is going to be searching for the server.

53:15
eg    if i run nslookup www.landmark.org shows the address of our ngnix webserver

                                                
                                          WHAT CAN MAKE A SERVER UNHEALTHY/ TO FAIL
56:00

1) There could be a spike in the CPM, lets assume that a lot of processes are running in the server or the server is receiving a lot of traffic, it may not be haelthy to 
recieve additional traffic . and thats one of the reason that can make a server to be unhealthy

2)if for some reason the java process is not running in the tomact server, if java is not running, tomcat not to start, if you deploy appl you cant access it so if java 
process fail, it can make a server to be unhealthy and as such the webserver will not route to  traffic to an unhealthy target n thats why  when it comes to loadbalancing 
 its  very important with ngnix bc it has all this advantages


   QUESTION
1)the same way we installed ngnix on Redhat, can we do it on ubuntu

ANS: YES we can

2) why will you always prefer we install those app on Redhat, why nt using ubuntu

ANS: its justbc i wnat you guys to understand both redhat and ubuntu, when we inastlled doecker, we installed it on ubuntu and tomorrow we ill configure k8 on ubuntu
so but its good for you to have both exposure Redhat and ubuntu



                                      

