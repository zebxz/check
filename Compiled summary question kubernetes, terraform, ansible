******** explanation on how traffic is routed is in nginx tomcat & kub 10&11 & KUB 12; NGINX

KUB 14 HElm
**meeee** WHY do we have more than one cluster in kubernetes  ????
why is helm characterized with using just one cluster?????    **** Helm manages one cluster while Rancher manages more than one cluster
**online
Helm is not restricted to a single cluster, but it is designed to manage one specific Kubernetes cluster at a time through a direct client-API server interaction. 
The perception of it being single-cluster-only stems from its fundamental architecture as a package manager rather than a multi-cluster orchestration tool





kubernetes 1&2


SOMEONE ASKED how many pods can we create in one node ???????????????



lets look at k8 BEST PRACTICES FOR LARGE CLUSTERS     *********** so if u have a very large cluster you can click on this link, u wil get more info
https://kubernetes.io/docs/setup/best-practices/cluster-large/
For very large clusters NB: dnt create :
   No more than 5,000 nodes
   No more than 110 pods per node
   No more than 150,000 total pods
   No more than 300,000 total containers


IQ: what is Static Pods ?
    Static Pods are controlled by the kubelet service  




KUB 6  .. Class33
what is the default deployment strategy?
  RollingUpdates  


  QUESTIONS
QUESTION......1:24:35
1) can a file be use to taint a node or must it be only commands in the CLI
ANS : anyting can be done with a file ,,,, i will have to check it up and you can check it up too

2) WHAT IS THE MOST used type of deployment
in k8 we can deploy with our containerized appl directly in a pod or we can use controler Managers like RS,RC or daemonSet bt most importantly we can use deployment and we 
use deployment it gives us the option to use diff deployment stragey like recretae, rollingUpdate and part of the deployment strategy we can use deployment techniques like
blue green OR canary deployment and that is you makingyour application available and infact deployment is maeking ur application available
once the applictaion has been made available the nxt quetsion hw wil the appl be discoverable ,hw wil ur cx discover and ultilize the appl , hw wil it be useful to them
there services need to be created, we need to expose the appl using services and der are diff services categories :
cluster IP permits you to access the appl within the cluster ,, so it exposes the apl to objects insid ethe cluster bt if its an appl that end users nid to access clusterIP
wont work therfoer wil use nodePort service or LB service to make it accessible  bc deployment makes your appl available bt  hw to i access it : service discovery


3) CAN deleted replicas be retieved  ,, bc you say we can delete it bt then hw do we roll back if for e.g a junior staff deletes it
we cant retrieve a deleted replica , once its deleted its gone
*the fact i sthat someone doesnt jst cm to ur offic eand u give him access bc in ur wrk env, we will use roll base access control like if u ar new we can permit you to hv 
access only to pod and since our pod are behind a controller managers and when the junior staff delete a pod it will be recreted again so u shu dbe senesitive enough nt 
to grant him mangers access...

4) for best practice what is the recommended number of replicas to keep  ie replicas of diff versions 
we cud keep a max of 5replica sets bc it will mean you have deployed about 5versions and the cx is happy with new verion 
bt its also subjetive to the client , some may decide to keep up to 10 replicas

5) when we rollback wnd then rollout again are new pods always created or the pods are reserved when thay are inactive
when you store those inf k8 will store the data in etcd , so it knows that version6 has the version6 of the image so when you roll back to version6, it goes back to that
particular image that was in version6 , thats what happens 
you can also rollback to a particular version choosing the version you want so when u run kubectl rollout undo deployment/web, u can choose the exact version ur rollin bk to

6) when we deploy w ecan choose for e.g 4replicas  bt when we scale to 2 is there any prefeernce in the pod that are created
d pods are identical in everything,its nt a matter of  no preference ,if you are updatingnscaling frm ur manifest file it will wrk inline wit d latest version ie d number of 
replicas u hv defined in the file bt in terms of if it will chooose which pod to create it wiill bring down all the pod wit the old versionof the imgea n bring up the pod 
wit the new version u now want and in terms of number of replicas its goin to be a function of what is defined in the manifest file 

5) when we taint a node and its undergoin an upgrade when u pass a toloration will pods be schedued on that node?
what is the rationale behind tainting a node , u taint a node bc u wnat to upgrade the node so if the node is already under upgarde ders no nid to be assigning toloration
that wil still create a pod on the node right,, we only do that in rare cases .. so toloration is now that pod is nt schduled on dis node bt i am tolorating that to happen
bt that shud only happen if the node is nt under any maintenance, if its under maintennace u wont want to do that bc what is the rationale

6) hw do we rollback to a specific version???  ..
. go to k8 and check it out so that  in the  nxt class u can give us a briefing on that
kubectl rollout undo deployment hello --to revision=1 OR revision2 n so on   ..... class33 video 5&6


class33 video5&6 (1)
1) when we create toloration, can we create it so it doesnt create pods in the master node??
ans: YES, we can do that, ther are many other options when it comes to toloration, which means we have  diff operators and diff keys and wil look at that when we look at
scheduling, how pods are scheduled on a node.

2) why will we want to schedule pod on a tainted node
e.g we want to  ensure a particular pod is scheduled on all nodes including master bc the pod wil be gathering logs on the master node to tell us if the node is healthy or nt
the master node for e.g is tainted by default but we can use toloration to allow a pod to be scheduled on the master node probbaly bc the pod will be gathering logs on the 
master node

3) what is the actual function of the selector support function of the replica set bc when we were doin the controller managers we had the replication controller n u then
showed us the replica set and u said the replica set has a selector support but when we did both of them the only diff that i notice is in d format we wrot e our API veerison
ie in the replica set and also that match label key that we used
ANS:  

we hv equality based selectors and it is used for what?  
   ReplicationCOntrollers
key:values       ********** eaulity based selectors talk abt equal conditions like key:values
app:myapp
Set base selectors: is used for ;
  Deployments
  Replica Seta
  DaemonSets
  StatefulSets
  NodeSelectors
  NodeAffinity
key: app                    for set base we can have key , value in .. lets assume the key is app , value in:javawebapp could be my app
value in:                            so 1key can hv multiple value , dats why we say setbase selsctor and it basiclaly comes wit stuff like daemonset,RS, deployment etc
   -javawebapp                     its jst that all the examples we hv done so far we v not needed to get to get into set base selector bt as we progress, we are going to
   -myapp                          look at set base selector and if u hv any problem jst wqually go to the k8 official documentation
                                    kubernetes.io abi na oi


                REASONS WHY WE USE STATIC POD
4) is der any scenario that will rEQ us to create stactic pods manged by the kubelete service as opposed to using the controller managers bc static pod itself has healing 
capabilities ACTIVATED 
ANS: YES , BT Static pod cannot scale, so if it created a pod is just a single replica
2ndly, static pod are used to create components that manage k8 like kube proxy, dns, all of those pod that manges your cluster, we use that pod to create them so that they 
will always be recreated even by the kublete service.

5) what det the object we use in deployment, is it coy base ??/ wxcept daemonSeT cos i understand its used for e.g logmgt
so what det the obj w euse for deployment
ANS:
the obj to be used depends on the diff/other components that obj will bring to the table 
deployment is said to be the recommended object for deployment in k8 or for running/deploying workload in k8 bc when u create a deployment, it creates a replica set so
it records whatevr u created such that if anything is wrong it can always rollback, thats why most of the time we use deployment bc deployment will rollout a replica set
u can rollout n rollback ,,, so from a very strong perspective we use deploymentas the recommended obj to deploy workload in k8
****************************************************************************************************


class33 video5&6 ANSWERS(2)

KUB 5&6 class33 answers (2)
Kubernetes objects used to expose/discover applications:
or kubernetes ServiceDicovery objects:
   ClusterIP   
   NodePort  
   LoadBalancer
   ExternalName   
   ingress 
   networkPolicy

                         REASONS FOR TAINTING NODE
1) what are the circumstance for wich we can taint a node and then tolorate the scheduler for pods to be scheduled on the node 
when will a node be tainted?
e.g we v a k8 cluster running with abt 11worker nodes, and it is runnin a particular version of kubectl bc kubectl is a software, remember when we configured 
our k8 cluster we install a lot of packages in the cluster, like kubectl was insatlled kubelete ,API server, etcd, scheduler, controller managers ,all of these
are conrainers that have specific versions or software, now want to recommision this server which means that we wnt to remove d current version of any of the
software that is runnin and replace it with a current versione.g kublete service from version 1.9 to version 1.11, now the process of recommissioning the server
it wil nt b gud to allow pods to be scheduled on that particular worker node so we are going to taint the node so once you taint the node ,it means you are
instructin k8 scheduler nt to scheudel aNY POD DON THE node, you are also instructing k8 to evict ANY POD that is currently runin on the node, so if der were 
any pods currently running on the node they wil be evicted and probably rescheduled on another node.
also, THE NODE COULD BE HAVING MEmory leakage , ders a problem with the node so we need to troubleshoot and resolve the problem so before we start to TS we 
will taint the node so that no pod shud be scheduled on the nodde
also generally, the amster node is tainted by default bt we hv a particular logmt appl that we want to deploy which shud gather logs even from the master node,
so we need to use toloration to allow pods to be schedule on the master node 
ALSO, we can taint a node to keep it on reserve, such that we could have some redundancy bt whenever we decide to assign node to the pod, we now use toloration,
ie torqtaing k8 scheduler to stil schedule a pod or pods on the node
if you are done recommissioning a node a simply remove the taint

2) When we use tolorations to shedule pods we can still acess the pods,, when nodes are tainted they are nt fshut down e.g master node


3) what is the default deployment strategy in kubernetes??
RollingUpdates is the default deployment strategy in kubernetes

                                          WEBHOOK/ PULL SCM
5) INORDER FOR US to update the new versionin on the deployment session on k8, that is done by webhook or pull ACM 
*************SOME ONE SAID ,, with the help of the webhook created in github wich is triggered when thers a new comit, github  then notify jenkins to do a
build ...32:17
 ans : 
another tin u shud knw is that our jobs are trigered frm JENkins, jenkins is our CI/CD pipeline, so frm jenkins if u are runin job1 , we can pass the job 
number to become the default version number of the image that is goin to be pushed to dockerhub such that when k8 is goin to pull, it is goin to dynamically 
get the latest image version that was pushed to dockerhub


                                   ADDING MSG TO ROLLOUT/ROLL BACK VERSION
6) is ther a way to add a msg such that u see what ur rollin back to
il illustrate that particular aspect in our subsequent classes, when u are rollin out a new version u pass an option that says --record so u can like tag a 
particular msg to that such that you can easily xplain what happened in a particular deployment such that if u ar goin to be rollin back to previous verisons,
u can be able to get that msg so u can use theat particular concept and if u want to rollout new versions u can create msgs to ease the processes as well


                             INTEGRATE WORK FLOW WITH POOL REQ
7)is it possible to integrate our workflow with pool REQ so that in our work env at work , someone doesnt just change the deployment witout any approval frm 
say lead engr
ANS:
its very possible,,, and thats what actually happens in our env, dats why we did pool rEQ under git such that if ders a new version of the app, developers
aaare nt comitin dicrectly to d master branch, they are developin to probably the dev branch and from that branch they nid to create a pool for other members 
to be able to review b4 it is commited to the master branch n it is only in the master branch that once der is a new commit github webhook wil b trigered to 
be able to do a buil, do a test, uplosd artifact, run sonarqude code, dockerize, push to regitry and deploy in a k8 cluster.




class33 video7a....1:17:10)
hw is trafic manged wit canry is dis  solely on the k8 side or we are goin to be depending on the cloud provider like aws to manage the taffic
ANS;
for canry we are goin to be use what is called a k8 operator  der are k8 operator like estil that wil enable us to deploy canary so its nt abt cloud side or whatever, u jst 
need to deploy a particular operator or an add on to your cluster that wil permit that to happen.

2) is there a duration for wich blue green deployment can last 
ANS: for blue green, its nt very wide, u cud hva testing period for mayb 7day or 30day bt that may
nt really detect all the problems bt with canary we cud run a canary deployment for asl long as 6months so at a higher level of security we cud want to consider more of 
canary

most critical appl, we use blue green to deploy them e.g u want to test a vaccine , in this type of scenario we use blue green bc we cant be carrying out n operation on a
client and then realise that ther is a fault, the new version is nt as gud as the previous one and you need to roll back , that rollbk cud be diasterous, so w edeploy a 
blue green env and run a test on maybe guinea pig etc , we observer hw itruns and if it is very sucesful, we deploy to production knwing thta it shud nt fail
even wit tha ther cud stil b afailure bc the tasting period is nt like inifinte for blue green, its nt very wide, u cud hva testing period for mayb 7day or 30day bt that may
nt really detect all the problems bt with canary we cud run a canary deployment for asl long as 6months so at a higher level of security we cud want to consider more of 
canary

1:21:30
is it that each time you refresh it takes you to a diff replica set???
ANS:
************here is our cluster ,, whta is manging this pod manifest????
we are using deployment as the object. .. our deployment is hello , this deployment is manging this 2pods , bt it doesnt manges the pod directly, it manges it via ,
bt it doesnt mange the pod directly, it manges it via a replica set so when u rol out this deployment, it creates a replica set, SO FOr each version that is rolled out it
creates a replica set for it and the previous replica set becomes 0  bt they stil exists such that if you want to roll back its done with relative ease.

if am logged in and an upgrade is done will i be logged out such taht i hv to login again into a new replica set
ANS:
if its a 0downtime technique that we are using , you wont be logged out



KUB 8&9
IQ: How can scaling be automated in kubernetes? 
    By installing a guage API [ metric-server ] in our cluster and using 
       1. HorizontalPodAutoscaler[HPA] for pods   
       2. VerticalPodAutoscaler[VPA] for pods 
       3. CUSTER Autoscaler[CAS] for nodes  


CAn we use NFS TO scale inorder to meet the wrk load??
if u are refering to hw to hv multiple read replicas 
we wil be seeing hw to replicate the db so we can hv multiple read replicas

***mee** when we do, i will confirm if we introduce the nFS vol or any type of vol first before we replicate/autoscale or replictate/autoscale first before
introducing NFS 
in Kub7, in the HPA illustration, he autoscaled a diff application but when he started kub vol concept in kub8&9, he didnt continue with the same application
from kub7


KUB 10&11

  KUBERNETES DISCUSSION POINTS
    2:05
 IQ: Explain your experience in kubernetes? 
  **2.15.40.. take note about all these key points, these are discussion points....
I have over 6 years experience in kubernetes performing the following;
- setting up a multi-node self managed kubernetes cluster using kubeadm    ,kubeadm this is self managed
- setting up a multi-node production ready kubernetes cluster using  kops     ,, kops is also self managed bt it comes wit other aws services
- setting up a single-node self managed cluster using minikube and Docker Desktop for testing. (if u want to test certain appl you can deploy them on a single
node cluster
- setting up a multi-node managed production ready k8s cluster
  using amazon eks  ******* may in a video i can show us how to
*****if ther are ant issues in these appl am i able to fix them? 
- troubleshooting issues from k8s setup/configuration or installation. ,, so v been able to trpubleshoot issues like trying to stup my kubeadm n its failing,
or cant add worker node to master node, v had issues like tht n hv been able to fix it by creatin a token in the master runing in the workernode n having them
join the cluster
- maintaining, monitoring and upgrading the cluster components E.G   :
  scheduler, etcd, controllerManagers, kube-proxy, kubectl, kubelet,
  container-D, Kubernetes-cni[weave, flannel], kubectl-csi, apiServer    ***2.08.50,, these are cluster component 
  kops export kubecfg $NAME --admin 
- deploying applications and workloads using kubernetes objects:
    - pods/ReplicationControllers/ReplicaSets/DaemonSets,
      Deployments/StatefulSets/PersistentVolume/ConfigMaps and
      secrets
- using deployment as a choice kubernetes objects for stateless apps  
- using replicasets, volumes with persistenvolumes for Stateful apps  
- using statefulsets to deploy Stateful applications  
- rollouts and rollbacks of Deployments  ********** we can deploy new versions of the appl bc we knw what to do
- deploying applications using controllerManagers [RC/RS/DS/STS/deploy]
- setting up Jenkins-kubernetes integration pipeline for full automation ,,. 2.15.30... very soon we wil also look at this, i think we hv a video for it as well
- deploying both Stateful applications and Stateless applications  
- making use of objects like; PV,PVC and dynamic storage classes to 
  persist data for Stateful applications [mongodb/ES/prometheus/jenkins]
- using configmaps and secrets for a secured application deployment   
- using probes for Health checks configuration in our deployments     
- using RBAC/namespaces/IAM for a secure access in the k8s. 

#### these are some few things i shud be able to talk about ..........



 HOW DOES TRAFFIC GET TO THE PODS/CONTAINERS RUNNING IN KUBERNETES; FROM AN INFRASTRUTURE PERSPECTIVE

 1:36:00  .....   frm an infrastruture perspective
How does traffic get to the PODS/containers running in kubernetes.
we v 50m users whaen they try to access this appl, they are typing the lb dns name , based on our deployment we v deployed a cluster wich has private n public 
subnet in our public subnet der is an Elb , SO endusers are trying to access the appl tru the ELB guy. insid ethe cluster we v node1 and 9 , frm an infrastruture 
perspective, we v the frontend and we hv the backend , now  when ensuders tyep the lb url in the background a command wil be executed, the reQ  frm endusers wil 
query global dns servers searching for the nameserver so in d background nslookup is searching for the nameserver, this is taking mini seconds, some of d 
global dns service providers it wil check is godaddy, google,aws route53, once nslookup is executed it wil immediately route traffic  via the ELS url n then 
traffic gets into our cluster  .........1:56:00 (&inside our cluster we v a service called springSVC
n its routing traffic to backend pod , now any service u create in k8 must create a cluster service ip as well, he ran kubectl get svc n we cud see a clusterip
attached to the loadbalancer service that was created when we deployed the springapp service above,) so frm the elb, traffic gets into the cluster n for it to
get to the cluster it wil hv to get to the node n since its now in the cluster it wil identify which service the traffic is meant for n via the service traffic
gets to the pods, bc traffic cannot go to the pod directly the pods are discovered via the service n as such traffic wil get to the backend pod accordinly and
as part of the spring app  we also deployed a database pod , for the db we hv another service.. .. all our servers are runing in private subnet master n worker
while ELB is in the  public subnet

**mee** enusers traffic (they type lb url, the REQ, queries global dns & in the background, nslookup searches for the nameserver in eg godaddy,google, aws
route53) once nslookup is executed, traffic is routed via the elb (clusterip)into our cluster to the backend pod (appserver/db), inside the cluster the elb 
identifies which pod (app or db) the traffic is meant for & the pod is discovered via service discovery

52.30 Nginx/tomcat
WHAT HAPPENS WHEN YOU TYPE  app.com or google.com    (domain name service)
54:30
  once you type google.com it going to query global DNS searching for the webservers that are associated to this hostname and once it locates the server, immediately traffic
will be routed to that server

for example, why you type www.landmark.org, what happens is that in the backend you are going to query global DNS(global Domain name service) and there is a command is the 
backend called NS look up, it is going to be searching for the server.

eg    if i run nslookup www.landmark.org shows the address of our ngnix webserver




2:36:35      QUESTIONS

                 KOPS SERVER DIFF FROM MASTER NODE
1)FROM THE CL8 WE created using kobs i unsderstood it but, i knw we shh tru d ec2 insatnce, the server wher wecarried out the whole operation is der a way we 
can ssh into the master node using mobaxterm or vscode remote host
****ANS: we did nt nid to sh into the master node bc we created a kops sever bt tk note that the kops server is nt our k8 master node, so frm the kops server
we cud jst remotely mange our kops cl8, we did nt nid to ssh into the kops bt if we had to do that then we can stil do that


    SSH KEY & SECRET
student : cos i knw we did nt create a key pair dasts why am asking, hw do we then do it ?
ANS: WE did nt nid to create a key pair bt we craeted sssh keys while deploying our kops command , when we ran the ssh gen key command and we had exported that
key into the kops as a secret n we can use the key.


                 COMMUNICATION BTW APP & DATABASE
 2) when u tal abt hw the appl comm in the cl8 like d springapp n the dtabase,, hw does dis comm is it tru the programm that mayb the springapp like the  one
we were using like when u put ur data automatically u see it showing in the database,,,,, is it the programmer that do the database that wil create the 
interface?   
ANS:  
the fact is that we are using k8 to orchestrate containerized appl,in our team we v programers who are writing code they hv written d software that wil permit
users to write their name , dob number ,, they hv done that in the backend, we as devops engr we hv been able to build that code, create pipelines n we v been
able to containerize the appl by creatin an image and pushin the image to dockerhub,, in k8 we can pull that image n deploy the appl, onec it is deployed we v
an appl that is running which reQ usr to enter their inf and thos info needs to be captured in a database ..... how can it be captured ina database???? that 
becaomes a serious issue that is why we are now deploying a db appl called mongo in this case,,, 
HW DOES THAT APPL COMM WITH EACH OTHER IN K8 ??? it is done via service discovery and within the cl8 we use the cl8 ip service.

STUDENT: so frm what ur saying, ders a reference in the code that will link the info straight to yhe database ryt
ANS: .. of course


                        CONTAINER-D
3) i nw for k8 our container runtime is containerD, is ther  a particular reason why we using it,, is der a preference??
ANS: 
we learnt docker bc we use docker to containerize appl , once we use docker, we write the docker file, with the help of the file, we build images, the images
are our appl that have been packaged, now these packages we hv shared n distributed them to our image registry like docker hub for example, now der are 2tins
involved here: containerizin and deploying the appl ,, for u to deploy the appl it means that u hv to create the conatiner n start the container.. so the 
containerD in k8 is simply doing 2thing: it create and start the container that is all bc thats why containerD is running in the workernode that create n 
start the container so in the past we used to use docker to achieve that purpose but k8 has deprecated dockcer thats why we no longer use docker, we use 
conatinerD

  ***mee** so does this mean we longer need docker at all?... bc i was thinking that we will use docker to containerize then use k8 to manage the containers.
***meee** NO; I bliv in the past they used docker D or smt like that but now k8 has containerD , it is used to deploy the application ie create containers using images
BUT to create/modify images inorder to package the application, we still need docker.

****online
Docker Desktop and the Docker Engine utilize containerd as their underlying container runtime. While Docker provides a comprehensive platform with tools for building,
managing, and orchestrating containers, containerd is the lower-level component responsible for executing and managing the lifecycle of containers.
Therefore, when you use Docker, you are indirectly using containerd, as it acts as the core engine for running your containers.



                UBUNTU USER PW
4) when u ran kops u put a pW, HW do u knw the PW to use 
ANS:
when i was creatin my kops user , user add kops
when u craete a user with ubuntu, ubuntu wil request for u to assign a PW to the user automatically , so when u ar createin a user, it wil REQ for u to assign
a PW as well

5) whne u use kops to provison ther was an autoscaling grp that was craeted , hw does the autoscaling grp works, how is it dif frm the k8 horizontal pod 
autoscaler
ANS:    
...... very imp question, i thin i missed that when i was explaining.....let me go bk and explain that
how is the autoscalin grp diff from frm what we did When we deployed a self managed cl8 with kubeabdm  that did nt cm with any autoscaler ??????


                  KOPS INSTALLATION CREATES AUTOSCALER/ PRODUCTION READY
now if u look at our ec2 instances lets observe smt  ...... 2:44:41
now am in west virgina, ther are 2worker nodes, if i delete this worker nodes what will happen?
ok , am deleting the 2instances .. obsever what happens when we shut down the instances, now lets check (in the ubuntuserver, he switched to kops user)
unbutu@master:~$ su - kops
;we can see only the master is ready , the 2worker nodes shows not ready bc we are shutting down the other2 nodes 
when we run kubectl get node : we see the 2nodes are now completely offline bc they v beeen terminated
bt now if i check my ec2 dashboard 2nodes are running and a new node is coming up (wit a diff ip address) bc we v an autoscaling grp taht was created wit min
and max and it is what is managing our node lifecycle.
we v an autoscaling grp for the master node that says hw many master node must be ther at al times, so if u delete the master nod ethe auto sacling grp wil 
craete anoda one and we also hv an autoscaling grp for worker node it works the same way .. thats why we call it production ready k8 cluster.


                          UPDATING AUTOSCALING IN KOPS
unbutu@master:~$  kubectl get svc
we hv a (springapp)lb that was created in aws
i can delete it in aws , i  can als run cli command to delete it 
kubectl delete svc springapp
once it is deleted , checking in aws  we see it says (ur resources cud nt be retrieved bc the lbwas only created when we created a type loadbalancer ssrvice
kubectl get service : but we c\nt see the (springapp)lb service again
if we run kubectl egt node, we see the master node n the 2nodes are all running now but if i want to remove the nodes completely, the only thing i can do is
to wrk on my autoscaler  .... he went to autoscaling grp in aws and he changed the desired min to zero, max to 0 and update .... the autoscaler will 
bring/shut down the nodes like ryt now we hv only the master running
*****so if u wnt to delete/stop/reduce your node just go to ur autoscaling grp and update it bring it down to zero
the same applies to the masternode .. u can udate the autoscaling grp for master

### bt am just doing it for e.g bc ,u cant be working for bank of america n just go n bring down their server bt since we ar just practising n i wnt to reduce
my cost i can do that


CONFIGMAPS & SECRET
6) where do we store the configmap and screts
ans; 
we store them in our local env nt in the scm


          EXTERNAL & INTERNAL LB
7) i need clarity for the internal n external lb, wher dey are placed cos frm the diagram u ilustrated above, u said we place the lb in the public subnet pls
xplain more

ANS\;
if u hv an internal lb u can put it in the private subnet that is ok n the external wil be in the public bc the external is internet facing so all those that
wnt to access ur env frm the internet they go tru the external





KUB 12; NGINX INGRESS
 ****NOThing from the video but, i  took some infor form nginx-tomcat & kub 10&11

1)NGINX-TOMCAT
QUESTION             WE CAN USE EITHER AWS ELB OR NGINX  ***mee** NGINX IS self manged manual configuration
i know we hv lB in AWS , THAT means if we are nt using nginx, we can use AWS LB
yes
and rhats what we will see when we get to k8 and AWS 
but its ver imp that you get the picture first
bc why do we need a webserver or Load balancer
load balancers helps to 
1)load balancers
                                                                    users >>> LB >>>> APP >>>> DB
2)security: Acts like a layer of security ie if anyone is trying to hack into our app it can be stopped at this stage which is very important 
3)health check: if its routing traffic/load balancing to about 30servers it ensures that it does not route traffic to a server that is not healthy, so it runs health check in 
the backend 
4)patching upgrading of our servers

2) KUB 10&11
managed or self managed  LB 
if i create a self mangaed lb, wit the self managed LB, i may create a server n in the server, i wil install a software like ngnix afterwich i wil deter hw 
service is routed
self managed =
   NGINX   



KUB 13 EKS INSTALLATION



KUB 14  HELM

...questtion
1) i installed the EKS Ctl tool 
with the EKS ctl tool, i created a 2nd cl8 with the EKS ctl create 
i use the eks ctl delete cl8 to delete the cl8 bt the the one the GUI i use EKS ctl to deletete the cl8 also , i wanst able to delete
it bt it did delete the node grp and i was wondering why it didnt delete the cl8 bt the node

ANS:
EKS has what is called EKS ctl 
EKS CTL Is a comand line utility for EKS, elastic kubernetes service
u can actually create ur cl8 by running eks ctl create cl8 or aws create cl8 , any one
n if ur trying to delete the cl8 using command, most of the time u must first delete the cl8 dependency like node grp
so if a node grp was created in that cl8, u must first delete the node grp , delete the de[pendency before deleting the cl8

2) WHAT IS THE BESTS TOOL WE CAN USE TO MANAGE OUR CL8

ANS:      it all depends 
maybe u want to ask hw we can best deploy our cl8
............u can deploy ur EKS cl8 using terraform scipt, u can use commands to deploy your EKS ie rather than going to the console 
u can jst run on the CLI , aws  EKS create cl8 ( the name of the cl8) that wil create it or eks ctl create cl8 bt u nid to ensure 
that your EKS command line utility has already been installed 
so u can use :
commaNd
console 
terraform
once u do that u can now proceed to deploy appl bc ur main task is appl deployment
most time at wrk u are goinmg to join a coy that already has running cl8 , it cud be EKS cl8, kubeabm cl8, kops cl8
once the cl8 is already existing all u nid is to use the cl8 to deploy appl, so main task now is appl deployment bc u wrk in an env 
with existing cl8 already , so u are goin to be worried abt how  appl are ruuing that are installed and exposed , u hv vol installed and all of that.


3) DIFF bTW helm and aws EKS 
k8 comes wit  a cl8 the cl8 cud be EKS cl8, kubeabm cl8, kops cl8, now once u aleady hv a cl8 u nid to be able to deploy wrk load in the cl8
the question now becomes how can i deploy workload ????  for u to be albe to deploy a work in my cl8, u id a command line utility called kubectl
u cud aslo use the GUI to deploy apkl , tahts what u nid for deployment, now aprt from using kubectl only we hv a package manager  we can use
to deploy called helm and helm can be deployed in any cl8 be it kops, eks etc once its installed u can then use it to deploy Ur appl

DIFF BTW HELM AND EKS 
 helm is a package manager for k8 while EKS is a k8 cl8 that comes with a fully managed controled plane provided by aws
so one is is a cl8 and the other is a command line utility and with the help of helm, u can reder ur manifest file, it can create the 
manifest file for u. 

4)   CAN U CONSIDER KOPS IN THE SAME LINE AS HELM??? .... is it the same as helm

ANS:
kops is a software used to create a k8 cl8
what do u need to deploy ur appl in k8 , u nid running cl8 , a cl8 that is functionig 
a cl8 is a grp of nodes  , that cl8 may v been created using kops or u cud hv created it using aws k8 service or using kubadm
it doesnt amtter what u hv used,all u nid is a cl8,  once u hv  a cl8 u can deploy appl 



kub 16

NICE explantion for :  .. 2) Explain your experience in kubernetes?? .......  18:00
my question: when is hosted zones created

4:00     ................... watch it
you would be ask what you have been able to use k8 to solve in your environment
ITS A Common experience when it comes to k8, either they want u to explain your experience or they just want to discus some problems u may v encountered using k8 and hw you
were able to fix those problem. also when u apply k8 thersa lot of xperience u gather n der a lot of problem uve been able to solve, 
so the xpectation is for you to be able to state clearly the kind of problems uv been able to fix applying k8 in ur env

also at times , they ask to xplain  k8 architecture
this is a very imp aspect in k8      


6:00
this is a very imp aspect in k8 n u shud be able to xpln dem                        

1) Explain the kubernetes architecture??  
   masterNodes/controlPlane:
made up of 
      apiServer  : wich is the main adminstartor, the entry point into our cl8
/etcd/scheduler/controllerManagers
   WorkerNodes:
      kubelet/kube-proxy/containerRuntime-Container-d/  
these are the component
we also have the
   kubernetesClient|: 
      kubectl / UI- Kuberenets dashboard
********thes are what we can use to mg8 task in k8 when it comes to architecture

2) Explain your experience in kubernetes??


  COMMON ERROR IN KUBERNETES
common errors in kubernetes:  
1)  pods are in pending  state 
  pullimage error / imagepulloff   
   WRONG  image: mylandmarktech/hello:20     
          mylandmarktech/hello:22   
2) authenticationerror:
  mylandmarktech/nodejs-fe-app:2  
  imagePullSecrets:
  - dockerhubcred
3)KOPS CLUSTER DEPLOYMENT ISSUES: like
  IAM user not authorise     ............. we nid this to create a kops cl8 in aws
    create an IAM role with required permissions/policy  
       VPCFULLACCESS/EC2FULLACCESS/S3FULLACCESS and attached
       to the kops control server 
    Attached required IAM policies to the IAM user/group and   
    run aws configure  







                                                   TERRAFORM

videao1

interview quetsions
what is IAC
waht are the benefits of iac
hv u evr implamemterd IAC in ur environment.


1)what is a state file
this is a file that manages the stae of ur infrasture, it give u a picture of ur infrasture, what is the actual state of ur infrasture, it is captured as a file
2)does terraform need credentials to interact with cloud provider like an API, like aws
yes terrarfeom needs credentilas 
3)how do we pass credentials to terraform ??.....................44.30
credentials:
firts , if u are trying to create resources in aws, u must hv an aws account, the same applies to GCP , or azure

for AWS, we need to generate security credentials in the console
so to do that, we go to the IAM user n create seceurity credentials wher we create an access key    
  we can create n IAM user............................. 46:00



SOMEBODY QUESTION: 1)
show us how to setup the CLI abi how to run aws configure, saw u are running directly frm ur macbk bt all along we hv been seting up the servers in aWS

ANS REPLIED;
 if u hv gitash/mbaxtem/vs code installed in ur envr run aws configure bc we wnt to config ur credentials, u config ur credntials in ur local envrn for security 
reasson u, if ur using ec2 instances, u dnt configure static credentials in aws, if u are using ec2 instance, u create a role and attach to the instance bt ur nt using a
key in ur instance bc if u set a key that is static, anybody who has access to ur instance that becomes a security issue
so using ec2 instance, we instaed create a role n give permission to the role, we always attach roles to resources. bt in ur local env u hv ur aws cli configure.
am showing that we can run terrform from our local env, we can write code in our local envn n mg8 resources in aws n 2mrw we ill see that we can instal teraform on an ec2
instance n create a role, give permission to the role n the ec2instance wil perform action in aws on ur behalf



QUESTION2                    Diff btw AWS CLI and TERRAFORM CLI
WE V INSTALL the aws cli n now can the aws provision resources in aws? ........... yes 
ok since it can do that n terafrom can also do that so whats the diff btw the 2
  .. ANS..
WE can use aws CLI to provision resources in AWS bt AWS CLI  is native to aws bc i cant use it to provisin resources in azure or gcp , thats bc a cli just for aws
the benefit that terrform brings is that its cloud agnostic , i can use terraform in aws, gcp, azure, k8 ... etc
thats why companies are adopting teraform as an iaac


QUESTION3                          
why do we v the multiple profiles?/

ANSWER:
when u run just aws configure you configure a default profile n each time u set a new default profile, it overrides the previous default profile
but when u run aws configure --profile amaka , this wil create a name profile called amaka , bc you pass an option, we can also vi into the credentials file n add d secret, 
acess keys n create a square bracket n put the nmae of the profile ......... 1:08
                                WE CAN PASS A PROFILE 
so when am runing my teraform, if i dnt want it to use my default credentials , i can pass a profile
e.g i v a developer profile n it only has read ccessn and i wnt to tesT my code wit jst read access, so yes i can v and mg8 multiple accounts, u can send me a secrte key 
frm ur account n i can create a profile here n anythin i do using ur profile, it wil be happening in ur account bc this credentials they also hv a mapping of the account
they v permissions of the prticular account that supply the credential. bt if i switch to my default, its hapening in my account, dats hw we use profiles n we wil see hw we 
can use diff profiles for our teraform runs.
                                                    WE CANT CREATE A PROFILE ON AWS CONSOLE
on the console, we can only create access n secret keys, we cant create profile on the console. bt when we wnt to configure our CLI env then we can then configure a profile
like this to be able to call aws on our behalf. .............. 1:07
just mk sure u hv set ur credential b4 u are able to use terraform.


INTERVIEW QUESTION:
WHAT IS A TERRAFORM WORKING DIR 
this a dir wher u v terraform config files
THATS Wher terraform wil be reading the files n creating a plan ..................1:13


INTERVIEW QUESTION:
what is the purpose of terraform init/ when do we run it?
it is the first command  we run to intialize our working dir and also to download our provider pluggings n also our modules.



INTERVIEW QUESTION:
what is the purpose of terraform init/ when do we run it?
it is the first command  we run to intialize our working dir and also to download our provider pluggings n also our modules.


QUESTION:
can we use the mobxterm we are used to ?  yes we will
u hv any terminal that u hv ur aws configured then u will be able to run terfrm
in ur terminal download terfrm
as long as u hv ur aws cli configured



  DIFF BTW TRM PLAN n TERFRM apply
trfrm gives u a plan without actually executing it , ie it gives the option of accepting or changing or destroy the plan while
apply creates the resources

what cred does trfrm use if we dnt pass it
it uses the default crdentials

HOW DO U sSET DEFAULT CREDENTIALS
we use aws configure

MY QUESTION
if we dnt tell terfm the region we wnt it to provide our resources, it wil create it in the default region set when u run aws config
if u dnt specify d  vpc or the subnet, , bc terfrm wil assume u wnt to create it in the default vpc
so if u dnt hv a default vpc,  terfrm wil return an error, cos it wil b looking for a default vpc to b able to create ur resource
so if u dnt v a default vpc terfrm wil fail
so some of the prerequisite: if u want to create a resource in a default vpc mk sure the region u ar creating ur resource has a default vpc
so how do you create a default vpc ????? ....... i knw 


I SEE THE SCRIPT HAS A COMMAND THAT SAYS FORMAT 
ANS:
terfrm has a lot of commands , we wil seee hw we ll format our code
what we did was a very basic code , we didnt nid to format the code 
when we write more code we see hw we v can use more commands bt the basic work flow, init, validate, plan, apply, destroy 




QUESTION
WHAT ARE THE MANDATORY BLKS THAT ARE COMPULSORY TO USE
IT just depends on the use case
for u to be able to create anytin u must use a resource blk
but if u need an attribute , u hv to use an output blk bc ders no other way of getting it (eg public ip frm an instance) if you are using code, u hv to create an output blk
when it comes to refactoring your code
when u start to write code for prod env , u hv to refactor ur code, u hv to split up ur code  so that it is easily readable, if code is easily readable then its easy to 
maintain. u hv to mk use of variables, split it into multiple files so that anybody using ur code they can easily find what they are looking for.
********if u nid to constrain ur version of trfrm then u hv to use the trfrm settting blk 



QUESTION  ............... 3:22:00
WHY DNT WE USE Mobarxterm
vscode id called an IDE  ...integrated development env , its gives an option to edit and run ur code, it has a tunnel, this is an integrated envr
the disadvantage of mobaxterm, etc
i can use the moboxterm but i cant edit 
in mobaxterm, i dnt hv the flexiblity of the auto complettion like i hv in vscode
when u go into ur companies, in the market u wil be using IDES
there are many IDES
i personally use intellyG IDE
SOME PEOPLE HV the IDE for their code & their terminal in mobaxterm for exmaple to run their code. just like me, as long as i can see my code here n run it frm my termial




  ****VIdeo 2

   1)  INTERVIW QUESTION   ................. 54:44
HOW DO YOU USE terrfrm setting blocks ( U SHUD BE ABLE to discuss this things abt setting ur rEq version & REQ provider version)



2) INTERVIEW  QUETSIOM 
WHats the purpose of terfrm init???
firstly, it initializes the backend then 
it initilizies or downloads the modules
thirdly, the provider

3) QUESTION:  ..... ..........1:58             .... watch 2:06 
1)
       DOES TERFRM DUPLICATE EACH  TIME WE RUN THE TRFRM COMMAND???
if you notice, i only run tfrm plan, i didnt run terfrm apply ,I WAS JUST TRYING TO GENERATE THE PLAN BC THIS IS HW WE TEST whether EVRYTIN IS WRKING 
bt if i created a new resource and run trfrm plan , we can see the plan is to add 1 and destroy 0 
module.ec2.aws_instance.data_test:Refreshing state  .. THIS MEANS its only refreshing the instance resource we already have 
# module.vpc.aws_vpc.vpc will be created   ......... this means  this new resource will be created
terfrm apply 
 we check the stste file we ssee the resource taht was added 

if i run trfrm apply again .. it says 
no changes, your infrastucture matches the configuration
bc the configuration matches what is ur current actual state, so its checking , refreshing what is in ur state file n comparing it wit ur configuratn,so bc it sees everytin 
matches ,ders nothing to change or add


4)  WHAT CRITERIA DO YOU LOOK AT TO KNOW THE VARIABLE TYPE TO USE
TYPES OF VARIABLE


5) INTERVIEW QUESTION
a)have u ever used trfrm to provisoin in diff env  ??????  ............ HOW do u do it
Ans:
1)USING A VARIABLE OF TYPE LIST TO DEPLOY IN DIFF ENV    ***list uses index;  eg 0= dev;t2 micro, 1=dev;t2 medium, 2,3 etc
so u can create a variable of type list of strings , put ur values der, lets says u are creating a module that wil deploy instances both in the stage , dev n prod env n mayb
the req is when ur deploying in the stage env use a t2 micr, in the dev use a t2 medium, in the prod use a t3 large
so u can define a var with a list then based on the env u selsct the index, so ur just declaring on evariable , ur reading it frm one variable bt its a list of variables n 
ur selecting the index based on the env.
***meee..2)
 2)USING A VARIABLE OF TYPE MAP     ***uses key & value   eg; key=dev & the value=t2 micro
so u can create a variable of type map , put ur values der, lets says u are creating a module that wil deploy instances both in the stage , dev n prod env n mayb
the req is when ur deploying in the stage env use a t2 micr, in the dev use a t2 medium, in the prod use a t3 large
so u can define a var of type map then based on the env u selsct the key, so ur just declaring on evariable , ur reading it frm one variable bt its a variable of type map & 
ur selecting the key based on the env.


6)  LOCK.hcl FILE/VERSIONING  (Answer to her question)
    THE FIRST Time u  run terfrm init, terfrm created for us this log file (terraform.lock.hcl) , the log file,it downloaded the versions n it locked them, this is the share
version of our provider aws, it downloaded the latest version , when subsequently u run terfrm init, it checks the log file, if its stil consistent, u hvnt updated any  
provider (to a diff version) it wnt go bk to donwload the plugins again. it wil reuse that dependencies it already downloaded


7)      MANAGING/UPGRADING PROVIDER PLUGGINS/VERSION
s0: run terfrm init -upgrade
now it will download the version which is 3.0 , so it is upgrading the lock version , so terfrn has made some cahges to the provider dependencies 
and in my lock file i can see the version is 3.76.1 bt the constraint is 3.0 , so this now becomes the depencies that terfrm is using , sO if i run terfrm init as long as
v constrain it , it wil nt go bk to download the dependcies again, it will reuse the dependecise
so this hw we manage our provider plugginds in term of version , so if terfrm does a change n it changes that , then u just hv to modify the constrian to that particular
vesrion bt u hv to upgrade ur puggins 


8)            2 DIFF PROVIDER FILES
3) e.g if u hv a provider aws and mayb mistakenly u created anoda provider that has azure or gcp 
how will terfrm read when u hv like 2 provider file wit diff values in them ????
   ANS:

provider = "aws " {
      region = var.region
}

provider "azurerm" {}
   ******************* ther are some other arguments am suppose to pass             .......... 3.24.20

  ****** when u run terfrm init , it will initialize the backend and we can look at our .terrafrm we see it now downloaded azure pluggins bt we already have aws pluggins
***meee** then i bliv its then left to you to decide if u want to create your resources using aws or azure bc u now hv dependencies for both providers



9) PASSING VARIABLES AS A FILE  (the file must end with 'tfvars', pass it on the CLI uing '-var-file= 'the file name')
1) Named files
variable files are files that  starts with any name but ends with 'tf.vars , any file that has this convention u v to pass it on the CLI uing '-var-file=
eg terfrm plan -var-file=prod.tfvars' 
 terafrm apply -var-file=foo.tfvars-var-file=bar.tvars)

****BUT
2) TERRAFORM.tfvars/ AUTO FILE
this type of file is automatically loaded u dnt hv to pass it (the file name)on the command line , ie to deploy it, you
just run 'terfrm plan' without supplying the values.... you dnt have to run; 'terfrm plan -var-file=prod.tfvars' 



 IQ:   how do u mk a variable  REQuired?
by not giving a default value
when we dnt supply a default value, we make it rEQ variable
e.g , now, in the variable file, if i comment on the instance type,(where i passed the value) bt in the main file i stil refernce the variable , then the variable becomes 
a REQ variable




TERRFRM3

1) Have you ever deployed in diff env using the same module
ans: yes
u can use variable files, files that define what your intention is/what you want to pass


1)what do we do when we dnt knw the right attribute to pass???

ANS;
jst depend on documnetation whener u are lost or smt isnt clear, like ur nt sure of what attribute (ie for what you need to return as an output) or argument ur supose to pass
just cm to d dcoumentation n just look at arguments and attributes, you will see what is required & whats optional
eg for a Natgateway, in google type aws_Natgateway and you can see what argument is required


2) we tried to create instance using a file but when we tried to create anoda instance and we changed the resource name to mk it unique, we realise that it destroys the first
instance created, does that mean one file for one instance

***I Think the simple answer he should have given her is, you can use one file but with diff resource blk & diff resource name. or using the same resource name using arguments
                                                                                                                     like count; as we will see subsequently

ANS;
THats why we said in the first class that, you can create as many type of instance as possible but what must be unique is your 'resource name' 
the name must be unique to trfrm bc trfm keeps a state file
eg terfrm state list .. this list the resources i have
SCENARIO 1: 2 THE Same 'resource type' eg 2instance, with the same resource name (mee, eg you delete or change the previous resource blk to use it to create the 2nd instance)
when u try to create a 2nd instance wit the same resource name, when trfrm compare the state file (the file trfm stored frm ur previous configuration) n your new congiguration
file, it doesnt doesnt find the initial configuration file bc u changed/deleted it so it sees that as a drift, my configuration doesnt match my v wall, & so terfrm will try to
destroy what it was managing before but now doesnt exist, so that it creates the new one 
eg , if i run terfrm plan, it refreshes the state file, ie the ec2 instance, the pevious one so always make sure the name is unique, mee,
**mee 
SCENARIO 3:   2 THE SAME RESOURCE TYPE' (eg  2 'ec2Instance') with unique names   ..2.0.7.00 but instead of this we can use 'meta argument; 'count'
               will work.. just have the 2 seperate resource blks
1.51.35 .
 SCENARIO 2:   2 THE Same 'resource type' eg 2instance, with the same resource name  ...(with 2 resource blk) 
 terfrm will say duplicate resource
SCENERIO 4:  2 DIFF RESOURCE TYPE  (eg; ec2instance & vPC), with the same name 
                  will work bc they are diff resources

when we look at meta data shortly frm now, we will see we can create a resource even with the same name 
how we can cerate multiple resources even with the same name  , how terfrm handles that using meta arguments    ............... 1:10:50



3) do we need to create the vpc first if we want to create our instance in the customized vpc?

ANS:
 1) DEFAULT VPC
if ur trying to create your inatnace in a region that does nt hv default vpc then ull neeed to specify which vpc u want to place it but if a region has a default & you dnt pass
a subnet where u want to place the instance, terfm wil place it in the default vpc, any subnet in the default vpc, default vpc wil generally hv public subnet, so it will pick 
up one of the public subnet in the default vpc &  place the instance there
2) BUT FOR CUSTOM VPC
YES you wil need to create the vpc first then place your instance, & we will see hw to do that shortly frm now, create our vpc & use the info frm the vpc to create our instance
but if ur nt using custom the it means ur using default and so u dnt need to pass any argument to ur resourec trfrm wil place it in the defult vpc


INTERVIEW QUESTIONS  , WHat is meta arguments
hv you ever worked with a terfrm argument?
can you explain to me how you've used depends on or count for each provider life cycle meta argument in your env
  ANS:meta arguments are what i pass into my terfrm resource to alter the behaviour of hw the resource behaves


  INTERVIEW QUESTIONS, 
What are implicit dependencies???

ANS:
THis is a situation wherby terfrm internally identifies d dependencies of resource n therfrore its able to identify which resource to create first, so it is abt dependencies
so if u dnt pass any depends on argument but tefrm still organizes which arguments needs to be created first then that becomes implicit depencies, its an inbuilt functionality
of tfrm that identifies which resource nids to be created first 
