******** explanation on how traffic is routed is in nginx tomcat & kub 10&11 & KUB 12; NGINX 

KUB 14 HElm
**meeee** WHY do we have more than one cluster in kubernetes  ????
why is helm characterized with using just one cluster?????    **** Helm manages one cluster while Rancher manages more than one cluster
**online
Helm is not restricted to a single cluster, but it is designed to manage one specific Kubernetes cluster at a time through a direct client-API server interaction. 
The perception of it being single-cluster-only stems from its fundamental architecture as a package manager rather than a multi-cluster orchestration tool





kubernetes 1&2


SOMEONE ASKED how many pods can we create in one node ???????????????



lets look at k8 BEST PRACTICES FOR LARGE CLUSTERS     *********** so if u have a very large cluster you can click on this link, u wil get more info
https://kubernetes.io/docs/setup/best-practices/cluster-large/
For very large clusters NB: dnt create :
   No more than 5,000 nodes
   No more than 110 pods per node
   No more than 150,000 total pods
   No more than 300,000 total containers


IQ: what is Static Pods ?
    Static Pods are controlled by the kubelet service  




KUB 6  .. Class33
what is the default deployment strategy?
  RollingUpdates  


  QUESTIONS
QUESTION......1:24:35
1) can a file be use to taint a node or must it be only commands in the CLI
ANS : anyting can be done with a file ,,,, i will have to check it up and you can check it up too

2) WHAT IS THE MOST used type of deployment
in k8 we can deploy with our containerized appl directly in a pod or we can use controler Managers like RS,RC or daemonSet bt most importantly we can use deployment and we 
use deployment it gives us the option to use diff deployment stragey like recretae, rollingUpdate and part of the deployment strategy we can use deployment techniques like
blue green OR canary deployment and that is you makingyour application available and infact deployment is maeking ur application available
once the applictaion has been made available the nxt quetsion hw wil the appl be discoverable ,hw wil ur cx discover and ultilize the appl , hw wil it be useful to them
there services need to be created, we need to expose the appl using services and der are diff services categories :
cluster IP permits you to access the appl within the cluster ,, so it exposes the apl to objects insid ethe cluster bt if its an appl that end users nid to access clusterIP
wont work therfoer wil use nodePort service or LB service to make it accessible  bc deployment makes your appl available bt  hw to i access it : service discovery


3) CAN deleted replicas be retieved  ,, bc you say we can delete it bt then hw do we roll back if for e.g a junior staff deletes it
we cant retrieve a deleted replica , once its deleted its gone
*the fact i sthat someone doesnt jst cm to ur offic eand u give him access bc in ur wrk env, we will use roll base access control like if u ar new we can permit you to hv 
access only to pod and since our pod are behind a controller managers and when the junior staff delete a pod it will be recreted again so u shu dbe senesitive enough nt 
to grant him mangers access...

4) for best practice what is the recommended number of replicas to keep  ie replicas of diff versions 
we cud keep a max of 5replica sets bc it will mean you have deployed about 5versions and the cx is happy with new verion 
bt its also subjetive to the client , some may decide to keep up to 10 replicas

5) when we rollback wnd then rollout again are new pods always created or the pods are reserved when thay are inactive
when you store those inf k8 will store the data in etcd , so it knows that version6 has the version6 of the image so when you roll back to version6, it goes back to that
particular image that was in version6 , thats what happens 
you can also rollback to a particular version choosing the version you want so when u run kubectl rollout undo deployment/web, u can choose the exact version ur rollin bk to

6) when we deploy w ecan choose for e.g 4replicas  bt when we scale to 2 is there any prefeernce in the pod that are created
d pods are identical in everything,its nt a matter of  no preference ,if you are updatingnscaling frm ur manifest file it will wrk inline wit d latest version ie d number of 
replicas u hv defined in the file bt in terms of if it will chooose which pod to create it wiill bring down all the pod wit the old versionof the imgea n bring up the pod 
wit the new version u now want and in terms of number of replicas its goin to be a function of what is defined in the manifest file 

5) when we taint a node and its undergoin an upgrade when u pass a toloration will pods be schedued on that node?
what is the rationale behind tainting a node , u taint a node bc u wnat to upgrade the node so if the node is already under upgarde ders no nid to be assigning toloration
that wil still create a pod on the node right,, we only do that in rare cases .. so toloration is now that pod is nt schduled on dis node bt i am tolorating that to happen
bt that shud only happen if the node is nt under any maintenance, if its under maintennace u wont want to do that bc what is the rationale

6) hw do we rollback to a specific version???  ..
. go to k8 and check it out so that  in the  nxt class u can give us a briefing on that
kubectl rollout undo deployment hello --to revision=1 OR revision2 n so on   ..... class33 video 5&6


class33 video5&6 (1)
1) when we create toloration, can we create it so it doesnt create pods in the master node??
ans: YES, we can do that, ther are many other options when it comes to toloration, which means we have  diff operators and diff keys and wil look at that when we look at
scheduling, how pods are scheduled on a node.

2) why will we want to schedule pod on a tainted node
e.g we want to  ensure a particular pod is scheduled on all nodes including master bc the pod wil be gathering logs on the master node to tell us if the node is healthy or nt
the master node for e.g is tainted by default but we can use toloration to allow a pod to be scheduled on the master node probbaly bc the pod will be gathering logs on the 
master node

3) what is the actual function of the selector support function of the replica set bc when we were doin the controller managers we had the replication controller n u then
showed us the replica set and u said the replica set has a selector support but when we did both of them the only diff that i notice is in d format we wrot e our API veerison
ie in the replica set and also that match label key that we used
ANS:  

we hv equality based selectors and it is used for what?  
   ReplicationCOntrollers
key:values       ********** eaulity based selectors talk abt equal conditions like key:values
app:myapp
Set base selectors: is used for ;
  Deployments
  Replica Seta
  DaemonSets
  StatefulSets
  NodeSelectors
  NodeAffinity
key: app                    for set base we can have key , value in .. lets assume the key is app , value in:javawebapp could be my app
value in:                            so 1key can hv multiple value , dats why we say setbase selsctor and it basiclaly comes wit stuff like daemonset,RS, deployment etc
   -javawebapp                     its jst that all the examples we hv done so far we v not needed to get to get into set base selector bt as we progress, we are going to
   -myapp                          look at set base selector and if u hv any problem jst wqually go to the k8 official documentation
                                    kubernetes.io abi na oi


                REASONS WHY WE USE STATIC POD
4) is der any scenario that will rEQ us to create stactic pods manged by the kubelete service as opposed to using the controller managers bc static pod itself has healing 
capabilities ACTIVATED 
ANS: YES , BT Static pod cannot scale, so if it created a pod is just a single replica
2ndly, static pod are used to create components that manage k8 like kube proxy, dns, all of those pod that manges your cluster, we use that pod to create them so that they 
will always be recreated even by the kublete service.

5) what det the object we use in deployment, is it coy base ??/ wxcept daemonSeT cos i understand its used for e.g logmgt
so what det the obj w euse for deployment
ANS:
the obj to be used depends on the diff/other components that obj will bring to the table 
deployment is said to be the recommended object for deployment in k8 or for running/deploying workload in k8 bc when u create a deployment, it creates a replica set so
it records whatevr u created such that if anything is wrong it can always rollback, thats why most of the time we use deployment bc deployment will rollout a replica set
u can rollout n rollback ,,, so from a very strong perspective we use deploymentas the recommended obj to deploy workload in k8
****************************************************************************************************


class33 video5&6 ANSWERS(2)

KUB 5&6 class33 answers (2)
Kubernetes objects used to expose/discover applications:
or kubernetes ServiceDicovery objects:
   ClusterIP   
   NodePort  
   LoadBalancer
   ExternalName   
   ingress 
   networkPolicy

                         REASONS FOR TAINTING NODE
1) what are the circumstance for wich we can taint a node and then tolorate the scheduler for pods to be scheduled on the node 
when will a node be tainted?
e.g we v a k8 cluster running with abt 11worker nodes, and it is runnin a particular version of kubectl bc kubectl is a software, remember when we configured 
our k8 cluster we install a lot of packages in the cluster, like kubectl was insatlled kubelete ,API server, etcd, scheduler, controller managers ,all of these
are conrainers that have specific versions or software, now want to recommision this server which means that we wnt to remove d current version of any of the
software that is runnin and replace it with a current versione.g kublete service from version 1.9 to version 1.11, now the process of recommissioning the server
it wil nt b gud to allow pods to be scheduled on that particular worker node so we are going to taint the node so once you taint the node ,it means you are
instructin k8 scheduler nt to scheudel aNY POD DON THE node, you are also instructing k8 to evict ANY POD that is currently runin on the node, so if der were 
any pods currently running on the node they wil be evicted and probably rescheduled on another node.
also, THE NODE COULD BE HAVING MEmory leakage , ders a problem with the node so we need to troubleshoot and resolve the problem so before we start to TS we 
will taint the node so that no pod shud be scheduled on the nodde
also generally, the amster node is tainted by default bt we hv a particular logmt appl that we want to deploy which shud gather logs even from the master node,
so we need to use toloration to allow pods to be schedule on the master node 
ALSO, we can taint a node to keep it on reserve, such that we could have some redundancy bt whenever we decide to assign node to the pod, we now use toloration,
ie torqtaing k8 scheduler to stil schedule a pod or pods on the node
if you are done recommissioning a node a simply remove the taint

2) When we use tolorations to shedule pods we can still acess the pods,, when nodes are tainted they are nt fshut down e.g master node


3) what is the default deployment strategy in kubernetes??
RollingUpdates is the default deployment strategy in kubernetes

                                          WEBHOOK/ PULL SCM
5) INORDER FOR US to update the new versionin on the deployment session on k8, that is done by webhook or pull ACM 
*************SOME ONE SAID ,, with the help of the webhook created in github wich is triggered when thers a new comit, github  then notify jenkins to do a
build ...32:17
 ans : 
another tin u shud knw is that our jobs are trigered frm JENkins, jenkins is our CI/CD pipeline, so frm jenkins if u are runin job1 , we can pass the job 
number to become the default version number of the image that is goin to be pushed to dockerhub such that when k8 is goin to pull, it is goin to dynamically 
get the latest image version that was pushed to dockerhub


                                   ADDING MSG TO ROLLOUT/ROLL BACK VERSION
6) is ther a way to add a msg such that u see what ur rollin back to
il illustrate that particular aspect in our subsequent classes, when u are rollin out a new version u pass an option that says --record so u can like tag a 
particular msg to that such that you can easily xplain what happened in a particular deployment such that if u ar goin to be rollin back to previous verisons,
u can be able to get that msg so u can use theat particular concept and if u want to rollout new versions u can create msgs to ease the processes as well


                             INTEGRATE WORK FLOW WITH POOL REQ
7)is it possible to integrate our workflow with pool REQ so that in our work env at work , someone doesnt just change the deployment witout any approval frm 
say lead engr
ANS:
its very possible,,, and thats what actually happens in our env, dats why we did pool rEQ under git such that if ders a new version of the app, developers
aaare nt comitin dicrectly to d master branch, they are developin to probably the dev branch and from that branch they nid to create a pool for other members 
to be able to review b4 it is commited to the master branch n it is only in the master branch that once der is a new commit github webhook wil b trigered to 
be able to do a buil, do a test, uplosd artifact, run sonarqude code, dockerize, push to regitry and deploy in a k8 cluster.




class33 video7a....1:17:10)
hw is trafic manged wit canry is dis  solely on the k8 side or we are goin to be depending on the cloud provider like aws to manage the taffic
ANS;
for canry we are goin to be use what is called a k8 operator  der are k8 operator like estil that wil enable us to deploy canary so its nt abt cloud side or whatever, u jst 
need to deploy a particular operator or an add on to your cluster that wil permit that to happen.

2) is there a duration for wich blue green deployment can last 
ANS: for blue green, its nt very wide, u cud hva testing period for mayb 7day or 30day bt that may
nt really detect all the problems bt with canary we cud run a canary deployment for asl long as 6months so at a higher level of security we cud want to consider more of 
canary

most critical appl, we use blue green to deploy them e.g u want to test a vaccine , in this type of scenario we use blue green bc we cant be carrying out n operation on a
client and then realise that ther is a fault, the new version is nt as gud as the previous one and you need to roll back , that rollbk cud be diasterous, so w edeploy a 
blue green env and run a test on maybe guinea pig etc , we observer hw itruns and if it is very sucesful, we deploy to production knwing thta it shud nt fail
even wit tha ther cud stil b afailure bc the tasting period is nt like inifinte for blue green, its nt very wide, u cud hva testing period for mayb 7day or 30day bt that may
nt really detect all the problems bt with canary we cud run a canary deployment for asl long as 6months so at a higher level of security we cud want to consider more of 
canary

1:21:30
is it that each time you refresh it takes you to a diff replica set???
ANS:
************here is our cluster ,, whta is manging this pod manifest????
we are using deployment as the object. .. our deployment is hello , this deployment is manging this 2pods , bt it doesnt manges the pod directly, it manges it via ,
bt it doesnt mange the pod directly, it manges it via a replica set so when u rol out this deployment, it creates a replica set, SO FOr each version that is rolled out it
creates a replica set for it and the previous replica set becomes 0  bt they stil exists such that if you want to roll back its done with relative ease.

if am logged in and an upgrade is done will i be logged out such taht i hv to login again into a new replica set
ANS:
if its a 0downtime technique that we are using , you wont be logged out



KUB 8&9
IQ: How can scaling be automated in kubernetes? 
    By installing a guage API [ metric-server ] in our cluster and using 
       1. HorizontalPodAutoscaler[HPA] for pods   
       2. VerticalPodAutoscaler[VPA] for pods 
       3. CUSTER Autoscaler[CAS] for nodes  


CAn we use NFS TO scale inorder to meet the wrk load??
if u are refering to hw to hv multiple read replicas 
we wil be seeing hw to replicate the db so we can hv multiple read replicas

***mee** when we do, i will confirm if we introduce the nFS vol or any type of vol first before we replicate/autoscale or replictate/autoscale first before
introducing NFS 
in Kub7, in the HPA illustration, he autoscaled a diff application but when he started kub vol concept in kub8&9, he didnt continue with the same application
from kub7


KUB 10&11

  KUBERNETES DISCUSSION POINTS
    2:05
 IQ: Explain your experience in kubernetes? 
  **2.15.40.. take note about all these key points, these are discussion points....
I have over 6 years experience in kubernetes performing the following;
- setting up a multi-node self managed kubernetes cluster using kubeadm    ,kubeadm this is self managed
- setting up a multi-node production ready kubernetes cluster using  kops     ,, kops is also self managed bt it comes wit other aws services
- setting up a single-node self managed cluster using minikube and Docker Desktop for testing. (if u want to test certain appl you can deploy them on a single
node cluster
- setting up a multi-node managed production ready k8s cluster
  using amazon eks  ******* may in a video i can show us how to
*****if ther are ant issues in these appl am i able to fix them? 
- troubleshooting issues from k8s setup/configuration or installation. ,, so v been able to trpubleshoot issues like trying to stup my kubeadm n its failing,
or cant add worker node to master node, v had issues like tht n hv been able to fix it by creatin a token in the master runing in the workernode n having them
join the cluster
- maintaining, monitoring and upgrading the cluster components E.G   :
  scheduler, etcd, controllerManagers, kube-proxy, kubectl, kubelet,
  container-D, Kubernetes-cni[weave, flannel], kubectl-csi, apiServer    ***2.08.50,, these are cluster component 
  kops export kubecfg $NAME --admin 
- deploying applications and workloads using kubernetes objects:
    - pods/ReplicationControllers/ReplicaSets/DaemonSets,
      Deployments/StatefulSets/PersistentVolume/ConfigMaps and
      secrets
- using deployment as a choice kubernetes objects for stateless apps  
- using replicasets, volumes with persistenvolumes for Stateful apps  
- using statefulsets to deploy Stateful applications  
- rollouts and rollbacks of Deployments  ********** we can deploy new versions of the appl bc we knw what to do
- deploying applications using controllerManagers [RC/RS/DS/STS/deploy]
- setting up Jenkins-kubernetes integration pipeline for full automation ,,. 2.15.30... very soon we wil also look at this, i think we hv a video for it as well
- deploying both Stateful applications and Stateless applications  
- making use of objects like; PV,PVC and dynamic storage classes to 
  persist data for Stateful applications [mongodb/ES/prometheus/jenkins]
- using configmaps and secrets for a secured application deployment   
- using probes for Health checks configuration in our deployments     
- using RBAC/namespaces/IAM for a secure access in the k8s. 

#### these are some few things i shud be able to talk about ..........



 HOW DOES TRAFFIC GET TO THE PODS/CONTAINERS RUNNING IN KUBERNETES; FROM AN INFRASTRUTURE PERSPECTIVE

 1:36:00  .....   frm an infrastruture perspective
How does traffic get to the PODS/containers running in kubernetes.
we v 50m users whaen they try to access this appl, they are typing the lb dns name , based on our deployment we v deployed a cluster wich has private n public 
subnet in our public subnet der is an Elb , SO endusers are trying to access the appl tru the ELB guy. insid ethe cluster we v node1 and 9 , frm an infrastruture 
perspective, we v the frontend and we hv the backend , now  when ensuders tyep the lb url in the background a command wil be executed, the reQ  frm endusers wil 
query global dns servers searching for the nameserver so in d background nslookup is searching for the nameserver, this is taking mini seconds, some of d 
global dns service providers it wil check is godaddy, google,aws route53, once nslookup is executed it wil immediately route traffic  via the ELS url n then 
traffic gets into our cluster  .........1:56:00 (&inside our cluster we v a service called springSVC
n its routing traffic to backend pod , now any service u create in k8 must create a cluster service ip as well, he ran kubectl get svc n we cud see a clusterip
attached to the loadbalancer service that was created when we deployed the springapp service above,) so frm the elb, traffic gets into the cluster n for it to
get to the cluster it wil hv to get to the node n since its now in the cluster it wil identify which service the traffic is meant for n via the service traffic
gets to the pods, bc traffic cannot go to the pod directly the pods are discovered via the service n as such traffic wil get to the backend pod accordinly and
as part of the spring app  we also deployed a database pod , for the db we hv another service.. .. all our servers are runing in private subnet master n worker
while ELB is in the  public subnet

**mee** enusers traffic (they type lb url, the REQ, queries global dns & in the background, nslookup searches for the nameserver in eg godaddy,google, aws
route53) once nslookup is executed, traffic is routed via the elb (clusterip)into our cluster to the backend pod (appserver/db), inside the cluster the elb 
identifies which pod (app or db) the traffic is meant for & the pod is discovered via service discovery

52.30 Nginx/tomcat
WHAT HAPPENS WHEN YOU TYPE  app.com or google.com    (domain name service)
54:30
  once you type google.com it going to query global DNS searching for the webservers that are associated to this hostname and once it locates the server, immediately traffic
will be routed to that server

for example, why you type www.landmark.org, what happens is that in the backend you are going to query global DNS(global Domain name service) and there is a command is the 
backend called NS look up, it is going to be searching for the server.

eg    if i run nslookup www.landmark.org shows the address of our ngnix webserver




2:36:35      QUESTIONS

                 KOPS SERVER DIFF FROM MASTER NODE
1)FROM THE CL8 WE created using kobs i unsderstood it but, i knw we shh tru d ec2 insatnce, the server wher wecarried out the whole operation is der a way we 
can ssh into the master node using mobaxterm or vscode remote host
****ANS: we did nt nid to sh into the master node bc we created a kops sever bt tk note that the kops server is nt our k8 master node, so frm the kops server
we cud jst remotely mange our kops cl8, we did nt nid to ssh into the kops bt if we had to do that then we can stil do that


    SSH KEY & SECRET
student : cos i knw we did nt create a key pair dasts why am asking, hw do we then do it ?
ANS: WE did nt nid to create a key pair bt we craeted sssh keys while deploying our kops command , when we ran the ssh gen key command and we had exported that
key into the kops as a secret n we can use the key.


                 COMMUNICATION BTW APP & DATABASE
 2) when u tal abt hw the appl comm in the cl8 like d springapp n the dtabase,, hw does dis comm is it tru the programm that mayb the springapp like the  one
we were using like when u put ur data automatically u see it showing in the database,,,,, is it the programmer that do the database that wil create the 
interface?   
ANS:  
the fact is that we are using k8 to orchestrate containerized appl,in our team we v programers who are writing code they hv written d software that wil permit
users to write their name , dob number ,, they hv done that in the backend, we as devops engr we hv been able to build that code, create pipelines n we v been
able to containerize the appl by creatin an image and pushin the image to dockerhub,, in k8 we can pull that image n deploy the appl, onec it is deployed we v
an appl that is running which reQ usr to enter their inf and thos info needs to be captured in a database ..... how can it be captured ina database???? that 
becaomes a serious issue that is why we are now deploying a db appl called mongo in this case,,, 
HW DOES THAT APPL COMM WITH EACH OTHER IN K8 ??? it is done via service discovery and within the cl8 we use the cl8 ip service.

STUDENT: so frm what ur saying, ders a reference in the code that will link the info straight to yhe database ryt
ANS: .. of course


                        CONTAINER-D
3) i nw for k8 our container runtime is containerD, is ther  a particular reason why we using it,, is der a preference??
ANS: 
we learnt docker bc we use docker to containerize appl , once we use docker, we write the docker file, with the help of the file, we build images, the images
are our appl that have been packaged, now these packages we hv shared n distributed them to our image registry like docker hub for example, now der are 2tins
involved here: containerizin and deploying the appl ,, for u to deploy the appl it means that u hv to create the conatiner n start the container.. so the 
containerD in k8 is simply doing 2thing: it create and start the container that is all bc thats why containerD is running in the workernode that create n 
start the container so in the past we used to use docker to achieve that purpose but k8 has deprecated dockcer thats why we no longer use docker, we use 
conatinerD

  ***mee** so does this mean we longer need docker at all?... bc i was thinking that we will use docker to containerize then use k8 to manage the containers.
***meee** NO; I bliv in the past they used docker D or smt like that but now k8 has containerD , it is used to deploy the application ie create containers using images
BUT to create/modify images inorder to package the application, we still need docker.

****online
Docker Desktop and the Docker Engine utilize containerd as their underlying container runtime. While Docker provides a comprehensive platform with tools for building,
managing, and orchestrating containers, containerd is the lower-level component responsible for executing and managing the lifecycle of containers.
Therefore, when you use Docker, you are indirectly using containerd, as it acts as the core engine for running your containers.



                UBUNTU USER PW
4) when u ran kops u put a pW, HW do u knw the PW to use 
ANS:
when i was creatin my kops user , user add kops
when u craete a user with ubuntu, ubuntu wil request for u to assign a PW to the user automatically , so when u ar createin a user, it wil REQ for u to assign
a PW as well

5) whne u use kops to provison ther was an autoscaling grp that was craeted , hw does the autoscaling grp works, how is it dif frm the k8 horizontal pod 
autoscaler
ANS:    
...... very imp question, i thin i missed that when i was explaining.....let me go bk and explain that
how is the autoscalin grp diff from frm what we did When we deployed a self managed cl8 with kubeabdm  that did nt cm with any autoscaler ??????


                  KOPS INSTALLATION CREATES AUTOSCALER/ PRODUCTION READY
now if u look at our ec2 instances lets observe smt  ...... 2:44:41
now am in west virgina, ther are 2worker nodes, if i delete this worker nodes what will happen?
ok , am deleting the 2instances .. obsever what happens when we shut down the instances, now lets check (in the ubuntuserver, he switched to kops user)
unbutu@master:~$ su - kops
;we can see only the master is ready , the 2worker nodes shows not ready bc we are shutting down the other2 nodes 
when we run kubectl get node : we see the 2nodes are now completely offline bc they v beeen terminated
bt now if i check my ec2 dashboard 2nodes are running and a new node is coming up (wit a diff ip address) bc we v an autoscaling grp taht was created wit min
and max and it is what is managing our node lifecycle.
we v an autoscaling grp for the master node that says hw many master node must be ther at al times, so if u delete the master nod ethe auto sacling grp wil 
craete anoda one and we also hv an autoscaling grp for worker node it works the same way .. thats why we call it production ready k8 cluster.


                          UPDATING AUTOSCALING IN KOPS
unbutu@master:~$  kubectl get svc
we hv a (springapp)lb that was created in aws
i can delete it in aws , i  can als run cli command to delete it 
kubectl delete svc springapp
once it is deleted , checking in aws  we see it says (ur resources cud nt be retrieved bc the lbwas only created when we created a type loadbalancer ssrvice
kubectl get service : but we c\nt see the (springapp)lb service again
if we run kubectl egt node, we see the master node n the 2nodes are all running now but if i want to remove the nodes completely, the only thing i can do is
to wrk on my autoscaler  .... he went to autoscaling grp in aws and he changed the desired min to zero, max to 0 and update .... the autoscaler will 
bring/shut down the nodes like ryt now we hv only the master running
*****so if u wnt to delete/stop/reduce your node just go to ur autoscaling grp and update it bring it down to zero
the same applies to the masternode .. u can udate the autoscaling grp for master

### bt am just doing it for e.g bc ,u cant be working for bank of america n just go n bring down their server bt since we ar just practising n i wnt to reduce
my cost i can do that


CONFIGMAPS & SECRET
6) where do we store the configmap and screts
ans; 
we store them in our local env nt in the scm


          EXTERNAL & INTERNAL LB
7) i need clarity for the internal n external lb, wher dey are placed cos frm the diagram u ilustrated above, u said we place the lb in the public subnet pls
xplain more

ANS\;
if u hv an internal lb u can put it in the private subnet that is ok n the external wil be in the public bc the external is internet facing so all those that
wnt to access ur env frm the internet they go tru the external





KUB 12; NGINX INGRESS
 ****NOThing from the video but, i  took some infor form nginx-tomcat & kub 10&11

1)NGINX-TOMCAT
QUESTION             WE CAN USE EITHER AWS ELB OR NGINX  ***mee** NGINX IS self manged manual configuration
i know we hv lB in AWS , THAT means if we are nt using nginx, we can use AWS LB
yes
and rhats what we will see when we get to k8 and AWS 
but its ver imp that you get the picture first
bc why do we need a webserver or Load balancer
load balancers helps to 
1)load balancers
                                                                    users >>> LB >>>> APP >>>> DB
2)security: Acts like a layer of security ie if anyone is trying to hack into our app it can be stopped at this stage which is very important 
3)health check: if its routing traffic/load balancing to about 30servers it ensures that it does not route traffic to a server that is not healthy, so it runs health check in 
the backend 
4)patching upgrading of our servers

2) KUB 10&11
managed or self managed  LB 
if i create a self mangaed lb, wit the self managed LB, i may create a server n in the server, i wil install a software like ngnix afterwich i wil deter hw 
service is routed
self managed =
   NGINX   



KUB 13 EKS INSTALLATION



KUB 14  HELM

...questtion
1) i installed the EKS Ctl tool 
with the EKS ctl tool, i created a 2nd cl8 with the EKS ctl create 
i use the eks ctl delete cl8 to delete the cl8 bt the the one the GUI i use EKS ctl to deletete the cl8 also , i wanst able to delete
it bt it did delete the node grp and i was wondering why it didnt delete the cl8 bt the node

ANS:
EKS has what is called EKS ctl 
EKS CTL Is a comand line utility for EKS, elastic kubernetes service
u can actually create ur cl8 by running eks ctl create cl8 or aws create cl8 , any one
n if ur trying to delete the cl8 using command, most of the time u must first delete the cl8 dependency like node grp
so if a node grp was created in that cl8, u must first delete the node grp , delete the de[pendency before deleting the cl8

2) WHAT IS THE BESTS TOOL WE CAN USE TO MANAGE OUR CL8

ANS:      it all depends 
maybe u want to ask hw we can best deploy our cl8
............u can deploy ur EKS cl8 using terraform scipt, u can use commands to deploy your EKS ie rather than going to the console 
u can jst run on the CLI , aws  EKS create cl8 ( the name of the cl8) that wil create it or eks ctl create cl8 bt u nid to ensure 
that your EKS command line utility has already been installed 
so u can use :
commaNd
console 
terraform
once u do that u can now proceed to deploy appl bc ur main task is appl deployment
most time at wrk u are goinmg to join a coy that already has running cl8 , it cud be EKS cl8, kubeabm cl8, kops cl8
once the cl8 is already existing all u nid is to use the cl8 to deploy appl, so main task now is appl deployment bc u wrk in an env 
with existing cl8 already , so u are goin to be worried abt how  appl are ruuing that are installed and exposed , u hv vol installed and all of that.


3) DIFF bTW helm and aws EKS 
k8 comes wit  a cl8 the cl8 cud be EKS cl8, kubeabm cl8, kops cl8, now once u aleady hv a cl8 u nid to be able to deploy wrk load in the cl8
the question now becomes how can i deploy workload ????  for u to be albe to deploy a work in my cl8, u id a command line utility called kubectl
u cud aslo use the GUI to deploy apkl , tahts what u nid for deployment, now aprt from using kubectl only we hv a package manager  we can use
to deploy called helm and helm can be deployed in any cl8 be it kops, eks etc once its installed u can then use it to deploy Ur appl

DIFF BTW HELM AND EKS 
 helm is a package manager for k8 while EKS is a k8 cl8 that comes with a fully managed controled plane provided by aws
so one is is a cl8 and the other is a command line utility and with the help of helm, u can reder ur manifest file, it can create the 
manifest file for u. 

4)   CAN U CONSIDER KOPS IN THE SAME LINE AS HELM??? .... is it the same as helm

ANS:
kops is a software used to create a k8 cl8
what do u need to deploy ur appl in k8 , u nid running cl8 , a cl8 that is functionig 
a cl8 is a grp of nodes  , that cl8 may v been created using kops or u cud hv created it using aws k8 service or using kubadm
it doesnt amtter what u hv used,all u nid is a cl8,  once u hv  a cl8 u can deploy appl 



kub 16

NICE explantion for :  .. 2) Explain your experience in kubernetes?? .......  18:00
my question: when is hosted zones created

4:00     ................... watch it
you would be ask what you have been able to use k8 to solve in your environment
ITS A Common experience when it comes to k8, either they want u to explain your experience or they just want to discus some problems u may v encountered using k8 and hw you
were able to fix those problem. also when u apply k8 thersa lot of xperience u gather n der a lot of problem uve been able to solve, 
so the xpectation is for you to be able to state clearly the kind of problems uv been able to fix applying k8 in ur env

also at times , they ask to xplain  k8 architecture
this is a very imp aspect in k8      


6:00
this is a very imp aspect in k8 n u shud be able to xpln dem                        

1) Explain the kubernetes architecture??  
   masterNodes/controlPlane:
made up of 
      apiServer  : wich is the main adminstartor, the entry point into our cl8
/etcd/scheduler/controllerManagers
   WorkerNodes:
      kubelet/kube-proxy/containerRuntime-Container-d/  
these are the component
we also have the
   kubernetesClient|: 
      kubectl / UI- Kuberenets dashboard
********thes are what we can use to mg8 task in k8 when it comes to architecture

2) Explain your experience in kubernetes??


  COMMON ERROR IN KUBERNETES
common errors in kubernetes:  
1)  pods are in pending  state 
  pullimage error / imagepulloff   
   WRONG  image: mylandmarktech/hello:20     
          mylandmarktech/hello:22   
2) authenticationerror:
  mylandmarktech/nodejs-fe-app:2  
  imagePullSecrets:
  - dockerhubcred
3)KOPS CLUSTER DEPLOYMENT ISSUES: like
  IAM user not authorise     ............. we nid this to create a kops cl8 in aws
    create an IAM role with required permissions/policy  
       VPCFULLACCESS/EC2FULLACCESS/S3FULLACCESS and attached
       to the kops control server 
    Attached required IAM policies to the IAM user/group and   
    run aws configure  







                                                   TERRAFORM

videao1

interview quetsions
what is IAC
waht are the benefits of iac
hv u evr implamemterd IAC in ur environment.


1)what is a state file
this is a file that manages the stae of ur infrasture, it give u a picture of ur infrasture, what is the actual state of ur infrasture, it is captured as a file
2)does terraform need credentials to interact with cloud provider like an API, like aws
yes terrarfeom needs credentilas 
3)how do we pass credentials to terraform ??.....................44.30
credentials:
firts , if u are trying to create resources in aws, u must hv an aws account, the same applies to GCP , or azure

for AWS, we need to generate security credentials in the console
so to do that, we go to the IAM user n create seceurity credentials wher we create an access key    
  we can create n IAM user............................. 46:00



SOMEBODY QUESTION: 1)
show us how to setup the CLI abi how to run aws configure, saw u are running directly frm ur macbk bt all along we hv been seting up the servers in aWS

ANS REPLIED;
 if u hv gitash/mbaxtem/vs code installed in ur envr run aws configure bc we wnt to config ur credentials, u config ur credntials in ur local envrn for security 
reasson u, if ur using ec2 instances, u dnt configure static credentials in aws, if u are using ec2 instance, u create a role and attach to the instance bt ur nt using a
key in ur instance bc if u set a key that is static, anybody who has access to ur instance that becomes a security issue
so using ec2 instance, we instaed create a role n give permission to the role, we always attach roles to resources. bt in ur local env u hv ur aws cli configure.
am showing that we can run terrform from our local env, we can write code in our local envn n mg8 resources in aws n 2mrw we ill see that we can instal teraform on an ec2
instance n create a role, give permission to the role n the ec2instance wil perform action in aws on ur behalf



QUESTION2                    Diff btw AWS CLI and TERRAFORM CLI
WE V INSTALL the aws cli n now can the aws provision resources in aws? ........... yes 
ok since it can do that n terafrom can also do that so whats the diff btw the 2
  .. ANS..
WE can use aws CLI to provision resources in AWS bt AWS CLI  is native to aws bc i cant use it to provisin resources in azure or gcp , thats bc a cli just for aws
the benefit that terrform brings is that its cloud agnostic , i can use terraform in aws, gcp, azure, k8 ... etc
thats why companies are adopting teraform as an iaac


QUESTION3                          
why do we v the multiple profiles?/

ANSWER:
when u run just aws configure you configure a default profile n each time u set a new default profile, it overrides the previous default profile
but when u run aws configure --profile amaka , this wil create a name profile called amaka , bc you pass an option, we can also vi into the credentials file n add d secret, 
acess keys n create a square bracket n put the nmae of the profile ......... 1:08
                                WE CAN PASS A PROFILE 
so when am runing my teraform, if i dnt want it to use my default credentials , i can pass a profile
e.g i v a developer profile n it only has read ccessn and i wnt to tesT my code wit jst read access, so yes i can v and mg8 multiple accounts, u can send me a secrte key 
frm ur account n i can create a profile here n anythin i do using ur profile, it wil be happening in ur account bc this credentials they also hv a mapping of the account
they v permissions of the prticular account that supply the credential. bt if i switch to my default, its hapening in my account, dats hw we use profiles n we wil see hw we 
can use diff profiles for our teraform runs.
                                                    WE CANT CREATE A PROFILE ON AWS CONSOLE
on the console, we can only create access n secret keys, we cant create profile on the console. bt when we wnt to configure our CLI env then we can then configure a profile
like this to be able to call aws on our behalf. .............. 1:07
just mk sure u hv set ur credential b4 u are able to use terraform.


INTERVIEW QUESTION:
WHAT IS A TERRAFORM WORKING DIR 
this a dir wher u v terraform config files
THATS Wher terraform wil be reading the files n creating a plan ..................1:13


INTERVIEW QUESTION:
what is the purpose of terraform init/ when do we run it?
it is the first command  we run to intialize our working dir and also to download our provider pluggings n also our modules.



INTERVIEW QUESTION:
what is the purpose of terraform init/ when do we run it?
it is the first command  we run to intialize our working dir and also to download our provider pluggings n also our modules.


QUESTION:
can we use the mobxterm we are used to ?  yes we will
u hv any terminal that u hv ur aws configured then u will be able to run terfrm
in ur terminal download terfrm
as long as u hv ur aws cli configured



  DIFF BTW TRM PLAN n TERFRM apply
trfrm gives u a plan without actually executing it , ie it gives the option of accepting or changing or destroy the plan while
apply creates the resources

what cred does trfrm use if we dnt pass it
it uses the default crdentials

HOW DO U sSET DEFAULT CREDENTIALS
we use aws configure

MY QUESTION
if we dnt tell terfm the region we wnt it to provide our resources, it wil create it in the default region set when u run aws config
if u dnt specify d  vpc or the subnet, , bc terfrm wil assume u wnt to create it in the default vpc
so if u dnt hv a default vpc,  terfrm wil return an error, cos it wil b looking for a default vpc to b able to create ur resource
so if u dnt v a default vpc terfrm wil fail
so some of the prerequisite: if u want to create a resource in a default vpc mk sure the region u ar creating ur resource has a default vpc
so how do you create a default vpc ????? ....... i knw 


I SEE THE SCRIPT HAS A COMMAND THAT SAYS FORMAT 
ANS:
terfrm has a lot of commands , we wil seee hw we ll format our code
what we did was a very basic code , we didnt nid to format the code 
when we write more code we see hw we v can use more commands bt the basic work flow, init, validate, plan, apply, destroy 




QUESTION
WHAT ARE THE MANDATORY BLKS THAT ARE COMPULSORY TO USE
IT just depends on the use case
for u to be able to create anytin u must use a resource blk
but if u need an attribute , u hv to use an output blk bc ders no other way of getting it (eg public ip frm an instance) if you are using code, u hv to create an output blk
when it comes to refactoring your code
when u start to write code for prod env , u hv to refactor ur code, u hv to split up ur code  so that it is easily readable, if code is easily readable then its easy to 
maintain. u hv to mk use of variables, split it into multiple files so that anybody using ur code they can easily find what they are looking for.
********if u nid to constrain ur version of trfrm then u hv to use the trfrm settting blk 



QUESTION  ............... 3:22:00
WHY DNT WE USE Mobarxterm
vscode id called an IDE  ...integrated development env , its gives an option to edit and run ur code, it has a tunnel, this is an integrated envr
the disadvantage of mobaxterm, etc
i can use the moboxterm but i cant edit 
in mobaxterm, i dnt hv the flexiblity of the auto complettion like i hv in vscode
when u go into ur companies, in the market u wil be using IDES
there are many IDES
i personally use intellyG IDE
SOME PEOPLE HV the IDE for their code & their terminal in mobaxterm for exmaple to run their code. just like me, as long as i can see my code here n run it frm my termial




  ****VIdeo 2

   1)  INTERVIW QUESTION   ................. 54:44
HOW DO YOU USE terrfrm setting blocks ( U SHUD BE ABLE to discuss this things abt setting ur rEq version & REQ provider version)



2) INTERVIEW  QUETSIOM 
WHats the purpose of terfrm init???
firstly, it initializes the backend then 
it initilizies or downloads the modules
thirdly, the provider

3) QUESTION:  ..... ..........1:58             .... watch 2:06 
1)
       DOES TERFRM DUPLICATE EACH  TIME WE RUN THE TRFRM COMMAND???
if you notice, i only run tfrm plan, i didnt run terfrm apply ,I WAS JUST TRYING TO GENERATE THE PLAN BC THIS IS HW WE TEST whether EVRYTIN IS WRKING 
bt if i created a new resource and run trfrm plan , we can see the plan is to add 1 and destroy 0 
module.ec2.aws_instance.data_test:Refreshing state  .. THIS MEANS its only refreshing the instance resource we already have 
# module.vpc.aws_vpc.vpc will be created   ......... this means  this new resource will be created
terfrm apply 
 we check the stste file we ssee the resource taht was added 

if i run trfrm apply again .. it says 
no changes, your infrastucture matches the configuration
bc the configuration matches what is ur current actual state, so its checking , refreshing what is in ur state file n comparing it wit ur configuratn,so bc it sees everytin 
matches ,ders nothing to change or add


4)  WHAT CRITERIA DO YOU LOOK AT TO KNOW THE VARIABLE TYPE TO USE
TYPES OF VARIABLE


5) INTERVIEW QUESTION
a)have u ever used trfrm to provisoin in diff env  ??????  ............ HOW do u do it
Ans:
1)USING A VARIABLE OF TYPE LIST TO DEPLOY IN DIFF ENV    ***list uses index;  eg 0= dev;t2 micro, 1=dev;t2 medium, 2,3 etc
so u can create a variable of type list of strings , put ur values der, lets says u are creating a module that wil deploy instances both in the stage , dev n prod env n mayb
the req is when ur deploying in the stage env use a t2 micr, in the dev use a t2 medium, in the prod use a t3 large
so u can define a var with a list then based on the env u selsct the index, so ur just declaring on evariable , ur reading it frm one variable bt its a list of variables n 
ur selecting the index based on the env.
***meee..2)
 2)USING A VARIABLE OF TYPE MAP     ***uses key & value   eg; key=dev & the value=t2 micro
so u can create a variable of type map , put ur values der, lets says u are creating a module that wil deploy instances both in the stage , dev n prod env n mayb
the req is when ur deploying in the stage env use a t2 micr, in the dev use a t2 medium, in the prod use a t3 large
so u can define a var of type map then based on the env u selsct the key, so ur just declaring on evariable , ur reading it frm one variable bt its a variable of type map & 
ur selecting the key based on the env.


6)  LOCK.hcl FILE/VERSIONING  (Answer to her question)
    THE FIRST Time u  run terfrm init, terfrm created for us this log file (terraform.lock.hcl) , the log file,it downloaded the versions n it locked them, this is the share
version of our provider aws, it downloaded the latest version , when subsequently u run terfrm init, it checks the log file, if its stil consistent, u hvnt updated any  
provider (to a diff version) it wnt go bk to donwload the plugins again. it wil reuse that dependencies it already downloaded


7)      MANAGING/UPGRADING PROVIDER PLUGGINS/VERSION
s0: run terfrm init -upgrade
now it will download the version which is 3.0 , so it is upgrading the lock version , so terfrn has made some cahges to the provider dependencies 
and in my lock file i can see the version is 3.76.1 bt the constraint is 3.0 , so this now becomes the depencies that terfrm is using , sO if i run terfrm init as long as
v constrain it , it wil nt go bk to download the dependcies again, it will reuse the dependecise
so this hw we manage our provider plugginds in term of version , so if terfrm does a change n it changes that , then u just hv to modify the constrian to that particular
vesrion bt u hv to upgrade ur puggins 


8)            2 DIFF PROVIDER FILES
3) e.g if u hv a provider aws and mayb mistakenly u created anoda provider that has azure or gcp 
how will terfrm read when u hv like 2 provider file wit diff values in them ????
   ANS:

provider = "aws " {
      region = var.region
}

provider "azurerm" {}
   ******************* ther are some other arguments am suppose to pass             .......... 3.24.20

  ****** when u run terfrm init , it will initialize the backend and we can look at our .terrafrm we see it now downloaded azure pluggins bt we already have aws pluggins
***meee** then i bliv its then left to you to decide if u want to create your resources using aws or azure bc u now hv dependencies for both providers



9) PASSING VARIABLES AS A FILE  (the file must end with 'tfvars', pass it on the CLI uing '-var-file= 'the file name')
1) Named files
variable files are files that  starts with any name but ends with 'tf.vars , any file that has this convention u v to pass it on the CLI uing '-var-file=
eg terfrm plan -var-file=prod.tfvars' 
 terafrm apply -var-file=foo.tfvars-var-file=bar.tvars)

****BUT
2) TERRAFORM.tfvars/ AUTO FILE
this type of file is automatically loaded u dnt hv to pass it (the file name)on the command line , ie to deploy it, you
just run 'terfrm plan' without supplying the values.... you dnt have to run; 'terfrm plan -var-file=prod.tfvars' 



 IQ:   how do u mk a variable  REQuired?
by not giving a default value
when we dnt supply a default value, we make it rEQ variable
e.g , now, in the variable file, if i comment on the instance type,(where i passed the value) bt in the main file i stil refernce the variable , then the variable becomes 
a REQ variable





TERRFRM3


HAVE YOU ENCOUNTERED ANY ISSUES WORKING WITH TERFRM

2.10 issue cloning repo using vscode
question mark on directory/s3 bucket (meta argument examples)
so she had to fork/ edith the name

5.30   it clones but its empty
said he will reupdate the repo at the end of class but he has already deleted the question mark
  student confirmed its working now


INTERVIEW QUESTION
1)what is user data????
under whose/which persmission will the instance b boot straping???  ... it wil use root permission
we hv a root user whenevr ur instalin a package it wil use a root credential
2) what is booth straping???
the root user wil create ur default user and add them to d sudoers grp, that is the process of booth straping


1) Have you ever deployed in diff env using the same module
ans: yes
u can use variable files, files that define what your intention is/what you want to pass


1)what do we do when we dnt knw the right attribute to pass???

ANS;
jst depend on documnetation whener u are lost or smt isnt clear, like ur nt sure of what attribute (ie for what you need to return as an output) or argument ur supose to pass
just cm to d dcoumentation n just look at arguments and attributes, you will see what is required & whats optional
eg for a Natgateway, in google type aws_Natgateway and you can see what argument is required


2) we tried to create instance using a file but when we tried to create anoda instance and we changed the resource name to mk it unique, we realise that it destroys the first
instance created, does that mean one file for one instance

***I Think the simple answer he should have given her is, you can use one file but with diff resource blk & diff resource name. or using the same resource name using arguments
                                                                                                                     like count; as we will see subsequently

ANS;
THats why we said in the first class that, you can create as many type of instance as possible but what must be unique is your 'resource name' 
the name must be unique to trfrm bc trfm keeps a state file
eg terfrm state list .. this list the resources i have
SCENARIO 1: 2 THE Same 'resource type' eg 2instance, with the same resource name (mee, eg you delete or change the previous resource blk to use it to create the 2nd instance)
when u try to create a 2nd instance wit the same resource name, when trfrm compare the state file (the file trfm stored frm ur previous configuration) n your new congiguration
file, it doesnt doesnt find the initial configuration file bc u changed/deleted it so it sees that as a drift, my configuration doesnt match my v wall, & so terfrm will try to
destroy what it was managing before but now doesnt exist, so that it creates the new one 
eg , if i run terfrm plan, it refreshes the state file, ie the ec2 instance, the pevious one so always make sure the name is unique, mee,
**mee 
SCENARIO 3:   2 THE SAME RESOURCE TYPE' (eg  2 'ec2Instance') with unique names   ..2.0.7.00 but instead of this we can use 'meta argument; 'count'
               will work.. just have the 2 seperate resource blks
1.51.35 .
 SCENARIO 2:   2 THE Same 'resource type' eg 2instance, with the same resource name  ...(with 2 resource blk) 
 terfrm will say duplicate resource
SCENERIO 4:  2 DIFF RESOURCE TYPE  (eg; ec2instance & vPC), with the same name 
                  will work bc they are diff resources

when we look at meta data shortly frm now, we will see we can create a resource even with the same name 
how we can cerate multiple resources even with the same name  , how terfrm handles that using meta arguments    ............... 1:10:50



3) do we need to create the vpc first if we want to create our instance in the customized vpc?

ANS:
 1) DEFAULT VPC
if ur trying to create your inatnace in a region that does nt hv default vpc then ull neeed to specify which vpc u want to place it but if a region has a default & you dnt pass
a subnet where u want to place the instance, terfm wil place it in the default vpc, any subnet in the default vpc, default vpc wil generally hv public subnet, so it will pick 
up one of the public subnet in the default vpc &  place the instance there
2) BUT FOR CUSTOM VPC
YES you wil need to create the vpc first then place your instance, & we will see hw to do that shortly frm now, create our vpc & use the info frm the vpc to create our instance
but if ur nt using custom the it means ur using default and so u dnt need to pass any argument to ur resourec trfrm wil place it in the defult vpc


INTERVIEW QUESTIONS  , WHat is meta arguments
hv you ever worked with a terfrm argument?
can you explain to me how you've used depends on or count for each provider life cycle meta argument in your env
  ANS:meta arguments are what i pass into my terfrm resource to alter the behaviour of hw the resource behaves


  INTERVIEW QUESTIONS, 
What are implicit dependencies???

ANS:
THis is a situation wherby terfrm internally identifies d dependencies of resource n therfrore its able to identify which resource to create first, so it is abt dependencies
so if u dnt pass any depends on argument but tefrm still organizes which arguments needs to be created first then that becomes implicit depencies, its an inbuilt functionality
of tfrm that identifies which resource nids to be created first 



 QUETSION 
1) The diff btw count meta n for each,i noticed that thers an index in the count bt aside frm that is der any othr diff?? Cos it seems they do d same tin, multiple resources
 ANS:
once we start doing loops which i think i can show u guys on monday, u will see the major dif f
whenu use count, it wrks on alist bc list, if u knw what is called data type, u can v a list n u can hv a map, a set , all these are data types
 so if u are using count, if its reading frm a list, what will happening is it sequences values frm a lsit whic is 0index , so frm 0, 1,2, 3,4,5
bt the problem with count i sthat if u cm an remove a valaue frm the middle of the list , which according to count as it sequence, it was suppose 2 b at index1, bt if i 
remove that value frm d middle of the lsit , what happens is that is going to shift the resource at index2 it moves it to bcome index 1, trfrm does that by destroying d 
resource that was in index2 so that it shifs it to 1 bc list cnat hv a gap inbetween (it has to run frm 0 to whatever)so thats the downside of count  bt if u remove a value
 frm the top of the list the last eelemt, it wil stil wrk fine bc ur nt modifying anytin frm 0 , d problem is if anytin is taken frm d middle 

thats why for each now becomes more superior bc for each uses a set or a map , so lng as u can convert ur list into a set , then a set is nt index, values in a set are nt
indexed like frm 0123, values ina set as a data type are refrernced by the name , so if i remove the name fmr the midlde of the list ist only gon tp affect the resource 
taht has that name bc we are using keys, we are using names, we ar nt using index

2) for each , can we create two,  t2 medium  bc the example we used, we actually t2 micro, t2 medium 

ANS:
bc as longs it reads frm the list right??

*********he tried to practicalise it bt his system was nt working, his IDE is frozen
he said just play with it by yourself and see what happens 


3) if we used a life cycle to prevent the instance not to be deledte bt if the instance was in that particular vpc , can we delete the vpc

ANS: 
NO U CAN NEVER DELETE A VPC THAT HAS A RESOUREC IN IT 
if a vpc has a resoure, it cannot be deleted, if u try to delete it it gives u an error 


4) when u were working on meta argument, i saw a place u mentioned abt the element bt u didnt emphasize more on it, bt i was wondering, how does element factor in the count
or meta argument

ANS:
 there are things i didnt mention bc those are part of trfrm functions, as u realise i ddint cover much on function, an element is a function, we can use a function called 
element to extact a value  , or lememe jst call it an element to extract an obj frm a list, that is a function that will go and pick a value frm a list
so u can use a function to get those values frm a list bt  we still hv  a topic to talk abt tfrrm function , and thats when we will talk abt loop n all that n thats why i 
didnt mention it



so keep practising with thos meta arguments bc 
once u have a good mastery of them then ur all good to wrk eith terfrm once u knw the backend and all that, u guys shud be gud
then when we transition to bootcamp we would now igve you more advanced ocncept 
for now this is jsudt the  foundation of trfrm
you need to undertsnd the trfrm blocks, the workflow, the backends , n jsut some basic concept for u to b able to run tfrm
but once w ego tru the bootcamp, we start puting in some advanced concept in trfrm 




TERFRM4
 INTERVIEW QUESTION
 What is a null resource ??
this is just a place holder, its a dormy/empty resource, it doesnt create anything.. bt we can use it to enable us establish our connection n we ar connecting to this 
particular inastance we are creating :aws_eip.ip.public_ip



   WHAT IS THE part.module 

  ANS: 
i explained taht on saturday
its just a terfrm way, the syntax that they use saying that this is within the path of the module 
where we are here, the part of your module is your pwd , so when u evalute this (path.module) it always returns your pwd 
(path.module) this is your pwd 
 so u can easily pass as  :   file = ./mykey   .... bc it will evaluate to the pwd
OR u can pass it as : file = mykey/ansible

  ***************all these are the same 




What is tainting in terfrm

   ANS:
this is marking a resource for destruction
when a resource needs to be destroyed in terrafrm, you taint it 
when its tainted, during the next apply , this particular resource will be destroyed

 ************ so our EIP has been tainted therefore it must be replaced 




  ************ QUESTION 
REMOVE WHAT YOU DNT WANT TERFRM TO RECREATE FROM THE CONFIGURATION FILE

1) if there are situations wher i have to go to the console to delete a resource, how does that impact the statefile ???
   ANS: 
   Remember when you run terfrm plan , terfrm wil always compare your configuration, this is your desired state, this is what i desire to have
then it goes to aws and looks at your actuall state
so when my desired state say i need to have an instance here bc i hv it in my configuration but when it goes to aws it doesnt find the instance bc i deleted it 
then terfrm wil see that as a decrepracy , a drift, so it will recreate it so that this always matches what is actual 
so if you run apply once you deleted it as long as its still in your configuration file terfm wil go and recreate it
so if you dnt want whatever u deleted, if dnt want it here then remove the configuration file bc this is your desired state

2) is there a function/argument  or the moment we change mayb an argument, it could just taint the former one 

  ANS:
 thers a terfr command to taint a resource
  terraform taint 
terraform taint aws_instance.my_instance              .... so if youv created such an instance ,its in your statefile ,bt u wnat to terminate it or to dstroy it 
you can taint it, tainting just means u acn just mark it for destruction and when nxt you run terfrm aply, it wil automatically terminate it bc the resource is tainted
 


3) am asking bc when i look at the template, sometimes it awes me, in our working env do we have some template that we can be modifying or we hv to write the code all the 
time

   ANS:     2:49:00
I did say it might look overwhelming but as you keep working at it especially if u rely on documentation
trfrm is very easy to work with bc ther are templates, ther are modules, u go to the documentation you copy and you paste and you just modify
you will be asked to create modules, custom modules , so custom modules you can use the official module to copy examples bt ur customizing them menaing your makiing them
to suit your env, your changing the names , the variables , the regions, changing hw users are interacting with that, bt the base code ur basically reusing it
in terfrm thers smt we call dry , like dnt repeat youself, terfrm supports that concept wherby we use modules so that our code is reuseable
like now how hv created this vpc here, hv create it as a module so i dnt anticipate  that whenever i wnat a vpc anytime that i will go back and rewrite this code, like 
what i did , i copied this code form somewher else bc i had already wirtten that so am just reusing the code and you will see a lot of that
so craete your infrastructure as modules and just reuse them , just make changes as as you deem fit and just reuse them


4) AM still confused as to how the documentation works , like how do we access all the documentations just to pick it back up when we are doing our work in the real world??

5) before WE apply our code is there a way to check our work,like a test run or thers like a backend checkover our work before we do terfrm apply n hv it show up on our 
 instance as for d real world work

  ANS:
   and thats why you see evrytime am runing trfrm plan, what does the do??
if u have configuration , before u create anything u wnat to check the plan, is this what you wnat? this is the test that your doing, you generating a plan
terfm wil generate for u a plan and the plan will ask you, is this what you wnat to do , 
in most of the cases you will jsut be runing trfrm plan, bc you dnt have to everytime run the pipeline bc once you apply it means you are goin to trigger a bill
most time u can do a local plan to see if your configuration is valid, once the plan exists success and u check the plan and its good then u can go ahead and run apply
just rely on terfrm plan to check that what your intending to do is captured in the plan before you run apply 




    QUESTION
1) whats basicaly the function of the dashboard, is it for monitoring the app??  APN, app performance monitoring 

   ANS:  3:09
yes for monitoring it 
right here you can monitor diff ,like right now its me metrics based on the OP that is running on 
so if i wnat host name , type, its giving me that,  am running this on a linux , platform its ubnuntu the sever its giving me that 

so u can cfeate a dashboard that will show u evertin , so frm this metrics you can create a dashboard that wil be able to display those metrics 


   INTERVIEW QUETSION
How do you work with terfrm in a distributed env , IN A COllaboartive env
 
    ANS:
we sue s3 for stste storage , we use it as our backend , wherby evertin that we do , we run here (our local env), its been captured in the s3 bucket 



                                  PULLING STATE FILE
       QUESTION 
1) IN the event that i start working in a cop and the have their statefile in the backend, and i wnat to start working in collaboration with th eother team, 
how do i migrate it to my own local env so that i can work with the statefile that is already in existence

   ANS:
that will not be the case because once you pull it , we only have 1 ststefile  
normally they prevent you form pulling unless you are pulling it and pushing it back
but theres a command for that :
 terraform state pull

 ********  it pulled our stsefile and you can tell by the lineage number , it doesnt change 

******** so if i wanted to inspect the ststefile, like what is there , then i can easily do a state pull
 then look at it, work on it then i can do a state push to push it back but i think its asking for a path  or u can use -force

  ##### but basically you can use terfform state commands  
      terrafrm state -h
it shows you the sub commands, how to manipulate terfrrm state
 the state commands are only used when you are working with a statefile, but the statefile is very sensitive so you need to know what your doing bc if u mess up/corrupt 
the staefile then your entire infrastructure is corrupted bc the ststefile can no longer be used to compare what is desired and what is present
therefore thats why if you want to do anything on the statefile use the state commands , you dnt go on the statefile itself and try to edit/change it
if i need to remove a data source instaed of goin on the statefile to delete it ill use a state remove



  ***STATE LOCKING/LOCKING THE STATE FILE
dynamond db table to lock our ststefile in the s3 bucket


QUESTION: VIDEO 2;ANsible  .... PUTTING LOCK ON THE STATE FILE USING DYNAMOND DB

3) hOW do 2 or more engrs work on the same repo without causing a clash , in git we have branches , is it the same in terrafrm
ANS:
in your office you mayb be asked to create a module e.g for a vpc then someone else is given a ticket to also work on the same vpce.g enable login or  the flow logs
it means you both wil clone the same repo , your doing yours he doing yours , when both of you are done , u wil commit he code, depsnding on who commited first the other
person may need to rebase bc now the commit has moved, we stated on the smae commit, w eboth clone frm the main but now bc we hv multiple commit that has come in, it just 
mean that my commit is behind, my main branch has already moved ahead, the commits have moved( am goin tru thsi bc u guys usnderstand hw git work) so it just mean that il
need to rebase my branch before i match my cod e, once i do that w eare colaboarting the that particular repo, now if we are running pipelines, as we run the pipelines the
pipeline wil be bulding our vpc, id changes have happened then those cahnges are goin to be effected, but now the issues comes whne we are running terfrm commands at the same 
time

    *********24:49
 if we are doin in a pipeline then how pipelines hv been set up is pipeline has beeen queued, thers a queue 2piplines will nt run at the same time, once i commit and i do my
push if anoda engr is also pushing at the same time what the pipeline, the CI will do is it will queue the jobs and queing the jobs just means that whover committed the job 
first that pilpline is goin to run, and how it runs is bc am running terfrm, it is goin to put a lock on the state file. this is wher we are talking abt using dynamond DB to 
lock our statefile  and our tstefileis in the s3 bucket, the lock ensuresatha t no other terfrm activity can tk place on that statefile apart from the activity that this 
pipeline is running , so once this psieline is done and the statefile has been updated, it realease the lock and then anoda job can be submeitted
thast what happens terfrm alwsy puts a lock
for us bc we are running it on our local env we hv to configure that lock, we hv to create/configure a dynamond DB thta we will use to lock our ststefile in the s3 bucket
to prevent 2engrs or more running terfrm workflow  on the statefile at the sametime.

********26:28
if u are using terfrm cloud, its done automatically, trfrm cloud wil lock the statefile anytime your doing trfrm workflow so you can only have 1 wrkflow running, after the job 
the lock is is done the lock is taken off , the statefile is unlocked for any other workflow to run

27:00
###### but all this will become clearer when you go furtehr, right now as long a you just understand the basic trffrm concepts, the blocks , the workflow ,jst basic functions
so that you can create a resorce/ data source then that is sufficient, it becomes the building blk for u to build onto that b4 u start thinking of more advanced concept in 
terfrm



   *******Terfrm/Ansible Nov26


     EXPLAIN YOUR EXPERIENCE USING TERRAFORM
1:10:39                                
********** our env is heavy on aws 
HV USED AWS to provision resources in aws :

AWS:
   ec2-instances  \ VPC / EKS  / S3  / ETC.  
  my experience in terfrm is nt just provivisng ec2 instances , i hv also :
installing terraform on windows and linux systems 
hv been able to mg8 terfm workflow
    define scope / 
    init / 
    writing & modifying terraform scripts [ vars.tf main.tf modules]   

1:13:53  ********* u can pnly initialize in a dir where you hv terfrm code/file

************* 1:18:37nn

2. Ansible  is a Configuration Management tool  
for e.g w ehv 5clusters we are managing 
we can use ansible to mg8 this configuration

    appServers = 50    ,,, without ansible yo will be deploying the application manually one after the other on the 50 app servers bt with the help of ansible w can automate
    webservers = 40  
    dbservers  = 44 
    kubernetes = 5  

******1:25:50    apart frm ssh pluggin, anisble can also use winrm plugging to connect to the windws server and ssh for the linux servers
********* 1.27.28   fr ansible, we can mg8 our k8 cluster 

    deployment of applications in the 50 appServers 
    commissioning the 50 appServers 
    securing the  50 appServers
    installing tomcat in the 50 appServers

Ansible control server is able to mg8 all the servers

tasks that ansible can perform/run on its hosts:( all these fall under configuration)
    FileMGT  
    userMGT  
    deployment  
    securityMGT  
    system monitoring  
    patching  
    packageMGT  


How to perform the tasks and ansible workflow:
   modules 
   playbooks:
     variables  
     plays   
     tasks  
     handlers   
     modules [ yum / copy / service / template / apt / package / shell ] 
               command / setup / systemd / 








QUESTION

1)import commnad is used to bring in infrastructure that was not created by terraform to the mgt of terraform

2)we can create resources/ servers using the console or commands


1:57:00
3)******** she validated but when she applied there was an error

*** 1:57:40
2:01:00

4)i did a virtual machine on my system and i had several nodes bt bc i was running it on a vm and all my ip addresses were assigned dynamically when i shut down the vm and 
restrted it i noticed that the controller node cud no longer reach out to the other nodes probably bc th eip addresses hv chnaged and there were so many errors so i had to
destroy it start all over again

ANS
for the k8s wher did you launch your k8s servers was it on the vm or they were deployed from aws cloud???
No i deployed on my hyperv on my laptop

ANS:
when u deploy on hyperv on your laptop you are nt deploying how the class is deployed thats the first thingbc we are nt deploying using hyper v bc hyperv and virtualiztion
technology most coy hv moved into cloud computing and if u ha dto deploy in the cloud, the mgt is easier, the complication is not as much as whne you either virtual box, vm 
ware hypervisor to deploy your virtual machines and then use them to create a cl8

**** i will come bk to this 

5) am not here to teach you all the options for destroy THE reason you are been taught terrafrm is how have you applied terraform in your env


destroy options :
  terraform destroy --auto-approve  
  terraform destroy --target local_file.test      
  terraform destroy --target aws_instance.web         

the question is How have you applied terraform in your environment/landmark??? 
thats the interview question 
the interview question is not how many options do we hv for terfrm destroy, that has no business to do with you
you cud hv so many options for terfrm dsetroy but what hv done in my env hv not been able to look at those options 
these are the options hv been able to see


2:04:50
***********

6)  what else can we output in terfrm, apart from ip addresses and if there are templates we can use to put out those things that need ot be outputed

ANS:
when you run your trfm command there are times that we need to get some result frm the trfrm command and and if we need to get those result, theose result can only be gotten
frm the output block

eg
if you create an ec2instance  what are some meta data abt the ec2 instance, what info can we get as meta data, meta abt data
you can create an instace and decide to output the id of the instance 

output block:
   create ec2-instances :
      metadata = 
        ipAddresses
        az/ami/dns/
        Instance ID


we can also output :
   vpc : vpc-id  / cidr block   
   eks cluster:
     kubeconfig    :: we can output this bc we this to be able to get authenticated to get access and mg8 the cluster

contd NO4
********** kubernetes vm  .. u see all my servers i stopped started immediately i started them, 
so the issue you were having is bc of the vm , hyperv 


7)  would you say the info we hv in linux is enough in the curri to say we hv studied linux adminstration  ??

ANS:
     what we hv for linux which includes the command and bash shell scripting is sufficient for the tinz we need to use in terms of working as a devops engr
bc again you ar ent been trained to work as a traditional linux adminstrator
secondly most systems are now in the cloud and their mgt is now in the cloud computing side of things so the commands that we hv covered and the ones we will continue to
cover is sufficient for you to work effectively as a devops engr

********** well i was just asking cos i saw a job and they were playing empasis on linux system adminstrator as part of the devops tin so thats why i was asking, if its
enough to say we hv got that training 

ANS: YES please its enough


8) FILE MIGRATION;
how to migrate from onprem to aws , i saw it in a job vacancy as well

ANS: 
yes for you to migrate, you knw when we did aws like in s3 we were able to move files frm onprem to aws via an s3 bucket  , 
so u can use an s3 to copy file nad that is data migration, we can us es3 bucket and we can also use other vol concepts  to migrtae files 
and ther are equally other mgrtaion options that are available which we are goin to be seeing as we get into bootcamp


9) my question is on terfrm, if for example you hv 2 projects, project 1 and project2 
and for project1 uv developed the variables the data source and all the tvalues, so when am working with project2, is there a way i can refer to those data files and 
variables or i hv to copy them to th eproject2

ANS:
The fact is if you understand how to clone right bc whatever project you are working on you can store them in a scm like github so if my project1 is in github, i can clone
that project1 or i can just create a branch and i use branching concept to be able to modify the code for project2 bc if u create a branch all the code in, lets assume 
project 1 bransch will be in project2 branch, so all you hv to do is change certain values parametres and apply then and your good to go 
and thats why we say that terfrm is very good bc it can be versioned controlled










                                          ANSIBLE

INTERVIEW QUESTION
 what other configurtaion mgt tools do you know ??

ANS: 
 Apart from ansible, puppet , chef is also a configuration magt tool 
this two uses agent on the remote host while ansible uses a push mechanism and thats why ansible stands out 
ansible pushes modules so its your controller that initiates, it pushes out modules so it doesnt need agents in your seververs
unlike puppet and chef that need an agent bc the use a pull module an dbc they use a pull mechanism as such you have to install a puppet agent of a chet agent inside of 
each node bc it is the agent that will initiate that particular push bc your controller will be pulling. it uses a pull mechansim when it does the configuration
so ansible stand sout bc it doesnt need an agen , it uses a push mechanism as such its agentless 



   INTERVIEW QUESTIONS 

Benefits of using Ansible

It is a free open -source Automation tool and simple to use.
It Uses existing OpenSSH for connection ,    most of THE servers that we provision already hv open ssh configure so ansible jsut uses this  for it to do configuration
Agent-less                              No need to install any agent on Ansible Clients/Nodes
Python/YAML based                        .... playbooks are written in yaml language n yaml are very easy, so ansible is easy to use 
Highly flexible and versatile in configuration management of systems.
Large number of ready to use modules for system management  ,  ansible community and redhat they hv a whole lot of module that are ready to be used
                                so u dnt need to create or re write anything just use the existing module, we hv a lot of modules that comes preloaded when u install ansible
Custom modules can be added if needed        


**** just knw what ansible is, knw how it works, once you knw how it works you are able to present this in an interview , you are able to easily talk abt ansible just
based on this and the benefits

so if your wroking for a coy that doesnt use ansible yet and you wnat to convince them , may they are alwys patching servers, alwasy sshing into servers manually to do
configuration then you can introduce a configuration mgt tool as ansible 



 USING 2 DIFF SCRIPT AS USER DATA ON AN INSTANCE

    INTERVIEW QUESTION
How can you pass 2scripts as user data on an instance ???? 

ANS:

 because of how very flexible terfrm is, it gives you that opportunity to do that through what is called a datasource called :
this data source is called:
    "template_cloudinit_config"    , its a template that you can use , he called it (name) 'user-data'



 INTERVIEW QUESTION
if you are to cd into   /etc/ansible, what do you expect to see if you have worked with ansible what do you expect to see in that dir 

  ANS:

in the etc ansible, this is the dafault ansible home, ansible lives in /etc/ansible, this is default
in the /etc/ansible, you will seee what is called anisible.cfg, this is the ansible config file, everything ansible does, it has to read this configuration file 
this is the dafault :anisible.cfg 
default meaning you can create anoda file and override this, you can create another configuartion file and override this bc this is default
apart it has a hostt file
this is wher we put the ip address of the host that we are trying to manage 
so if i wnat to manage like the 2 or 3 ubuntu servers it just mean iil go to the host file and do an entry inside of the host file , once that is done ansible wil be reading
the host file to know which servers it has to configure




  INTERVIEW QUESTION
What is the precedence or priority of ansible when its searching for the configurtaion file 
mee... 
Because ansible has the option of creating the config file in a diff location other than the default location, so when searching for the file it follows an order of precedence

 bc whenevre we run ansible, the 1st tin is ansible will read the configuration file it wil look for the config file based on the priorities, it wil check if we pass env 
variable/mee..reference it, if its nt there it wil check the pwd so if i created a playbk where my ansible.cfg is , ansible wil use the file to read the configuration thats 
the priority, if it doest find it it wil go to the home of the user and then the default 

 ANS : 

1ST : ANSIBLE_CONFIG        env varibale.,, this takes priority , if not seen it goes to 
2nd:  ./ansible.cfg         pwd dir 
3rd:  ./ansible.cfg         home  of that particulr user 
and if not it goes to the 
default : /etc/ansible/ansible.cfg

 if u have ./ansible.cfg, in your pwd , then its the file ansible will be looking at to read d configuration
    thefor with al that, it just means that if am working on the dev env, i can create a  dev ansible.cfg for the dev env within my working dir and thats wher ansible will 
get its configuartion


                                HOST KEY CHECKING/AUTOMATING HOST KEY CHECKING
1)DISABLE THIS FEATURE INSIDE THE ANSIBLE CONFIG FILE
     
           host_key_checking = False                                 *****1:02:22
                     or                          ****export an env var called ** and set it to false with this ansible wil nt ask you to confirm the authenticity of this var
2)      $ export ANSIBLE_HOST_KEY_CHECKING=false



  QUESTION
1) FRM WHAT YOU SHOWED US TODAY U USE Ansible to provision the conrol and nodes server , my quation is can anisble also deploy in servers that already exist that you dnt hv
to provison and how do you do that???

  ANS:
YES AND WE WILL SEE HOW TO DO THAT, all that ansible needs is an ip address 

2)in creatin the control and managed nodes, i knw we added the user to the sudoers grp, is there any sg that you are creating in the background for any other connctions
mayb for other applictaions

ANS:    NO
 THE only sg that comes by default is ssh , ansible only needs ssh 


3)why do we output does attributes , do you have any use cases?? 
  ANS:
i outputted these ip addresses bc i wnat to configure my host file, in my host file i need ip addresses, i dnt wnat to go on the aws console to try and copy those addresses,
i can dynamically get them here like reading an output so am just trying to avoid goin to AWS
and this is smt you will find as engrs u wanna mk life easy for yourself, if terfrm can do the wrk for u let it do the work for you
i dnt have to log into aws to try and copy the ip address fo rthe diff instances bt i can just create an output and get my ips here so when i, go to create my host file 
and thats why i put this screen right here,(host file screen) i say ill need it ,ill just copy my ip adress for my ansible host n remote ,ill just copy n put in my host file



  lets tk a break now , when we cm bk at 9, we ll see hw we can configure our managed nodes using the PW and using ssh keys and probbaly be able to run the command 





                                                     VIDEO 2; class 33

2:30 
in the real world 
it wil be a diff scenario that will REQ you to use that basic understaning 
e.g they may say we hv a k8 cl8 that is runing in aws we nid to get a the cluster certificate form that cl8 bc we need it somewhere, if given that task so how wil you do it??

ANS: 
so now bc you understand the concept of, if you have a resource that exists you can get inf/attribute frm that resource, so u knw that i can use a data source, i can employ a 
data source that can specifically go into my eks cluster and return for me the certificate, so nmw thats a concept you understand so now you go to the documentation to see if
you have a datasource that acn access your k8 cluster and when it accesses what can it return, how many attributes can it return
again you just look at the documentation, if you use a datasource for eks cluster it will show you the tins that it can return, the inf it can get, so now when you have that 
concept you are goin to employ a data source to get all the inf that you need

but as we go further into the bootcamp everytin will be clearer the concept will solidify , and everything will make more sense

   ************6:00    importance of boot camp

preparation for transitioning into the bootcamp
cv  and linkedin preparation 
focused on project base so that everyting youv been doing al these concept is solididfied whereby you are doing projects
and we have other advanced concept thay you will be able to tour
helping you prepare for interviews, how your suppose to response


            QUESTION
1)  CAN WE create our resources in global instaed of regions and having issues,??

ANS:     ..NO
when your configuring you cant use global, we use a default , you must pass a region
e.g you want to create an ec2 instance , can you create that using the global option , it needs to be created in a specific region bc it needs to be created in a vpc
so the reason why we hv to pass a default is just incase in our configuration we are not specifying a region atleast aws is proactive to mk sure that your resource will be 
created somewhere, so thats why you hv to set that default region bc resources, most of the resources in aws are region based, we hv very limite global services like IAM
thats why when you go to create a user or a role it doesnt ask you for a region , S3 bucket is a global service bc you can access it frm any region but howeevr, when u go to 
create a bucket youhv to specify a region , bc most of the service are region specific so your reQ to pass a default region so that when a region is nt set in your 
configuration then either terfrm or whatever appli your using wil fall back on a region that has been set as a default
so thats why we cannot global aws configure set globally, we must have a region 


14:00
2) dnt understand how you use count.index and slice in the previous video 
ANS:
we use count index whenever we hv count set 
ITS a meta argumnet which alters the behaiour of how a resource works
when you look at the element function ,that is the first element that we are passing to count, element function  wil always return a number 
when you look at the documentation its just goin to select an item fmr a list , so we are combing those functions
e.g
i hv 10 elemebts in a list and now am using a slice function so am goin to implore that slice function on this list, it has 10 things and i need a slice of 3, or i need to 
slice it so that i get 4 elemet out of the list so ill give the first like a 0, so it menas the slice wil count frm the first elemet to and count 4slices and return taht to 
me so that wil be the value of the count
*********i can also slice frm the middle but say ill say the first slice/elent is 3, so it is goin to start frm 3 and count the 4 elements , 4,5,6,7 , so it will take that
port as the slice and return it to count

*********** the whole reason why we use count and (it becomes clearer in the both camp) we wnat to make our code more flexible, we dnt wnat to hard code values, we want such
that am nt harding coding anything, by putting a value as 2 or 3 on the count bc when i do that i have basically hardcoded it but i want that value of count to be detr by 
mayb d number of subnet so if i hv 5avaialbilty zones then i want the slice to be 5, if i have 3 availabity zones then i want the lsice to be 2 bc its dependent on something
else.

3) hOW do 2 or more engrs work on the same repo without causing a clash , in git we have branches , is it the same in terrafrm
ANS:
in your office you mayb be asked to create a module e.g for a vpc then someone else is given a ticket to also work on the same vpce.g enable login or  the flow logs
it means you both wil clone the same repo , your doing yours he doing yours , when both of you are done , u wil commit he code, depsnding on who commited first the other
person may need to rebase bc now the commit has moved, we stated on the smae commit, w eboth clone frm the main but now bc we hv multiple commit that has come in, it just 
mean that my commit is behind, my main branch has already moved ahead, the commits have moved( am goin tru thsi bc u guys usnderstand hw git work) so it just mean that il
need to rebase my branch before i match my cod e, once i do that w eare colaboarting the that particular repo, now if we are running pipelines, as we run the pipelines the
pipeline wil be bulding our vpc, id changes have happened then those cahnges are goin to be effected, but now the issues comes whne we are running terfrm commands at the same 
time

    *********24:49
 if we are doin in a pipeline then how pipelines hv been set up is pipeline has beeen queued, thers a queue 2piplines will nt run at the same time, once i commit and i do my
push if anoda engr is also pushing at the same time what the pipeline, the CI will do is it will queue the jobs and queing the jobs just means that whover committed the job 
first that pilpline is goin to run, and how it runs is bc am running terfrm, it is goin to put a lock on the state file. this is wher we are talking abt using dynamond DB to 
lock our statefile  and our tstefileis in the s3 bucket, the lock ensuresatha t no other terfrm activity can tk place on that statefile apart from the activity that this 
pipeline is running , so once this psieline is done and the statefile has been updated, it realease the lock and then anoda job can be submeitted
thast what happens terfrm alwsy puts a lock
for us bc we are running it on our local env we hv to configure that lock, we hv to create/configure a dynamond DB thta we will use to lock our ststefile in the s3 bucket
to prevent 2engrs or more running terfrm workflow  on the statefile at the sametime.

********26:28
if u are using terfrm cloud, its done automatically, trfrm cloud wil lock the statefile anytime your doing trfrm workflow so you can only have 1 wrkflow running, after the job 
the lock is is done the lock is taken off , the statefile is unlocked for any other workflow to run

27:00
###### but all this will become clearer when you go furtehr, right now as long a you just understand the basic trffrm concepts, the blocks , the workflow ,jst basic functions
so that you can create a resorce/ data source then that is sufficient, it becomes the building blk for u to build onto that b4 u start thinking of more advanced concept in 
terfrm



   QUESTION
Since ansible is a configuration mgt tool, the code we ran today , we created them on servers
but hw use same on containers, is it possible to use ansible to configure conatiners

ANS:
i dnt think we can use that to manage conatainers bc what mangrs containers is docke r , its a runtime, docker run time 
so we really cant use that to manage our conatainers
bt am sure thers a way that we can use ansible to run playbks that will craete docker
what is actually managing those containers we are creating is docker bt we are using ansible to run a playbk that wil do that , I BLIV he meant that will create docker


i asked that bc conatiners are super fats fast to spin up and they are more effective than virtual machines

ANS :
definitely what ur saying is true bt even if u hv those virtual machines u can configure them to run on servers
so you need smt to be able to configure servers, if i need to patch the server bc my server is holding virtual machines, then that server needs to be patched 
i need a configurtaion mgt tool thats why we use ansible 

**i bliv by virtual machine he doesnt mean cloud computing bc i bliv cloud instances is virtual machines running on  servers
In layman's terms, a virtual machine (VM) is like a computer inside your computer, created by software, that acts as a completely separate, independent machine
with its own operating system (like Windows or Linux) and apps, all running on your physical hardware without interfering with your main system


***i checked online, google says yes we can use Ansible to manage docker & kubernetes



 WHAT IS ADHOC COMMAND 
https://github.com/LandmakTechnology/Ansible-mc-series/tree/main/03-adhoc-commands
An Ad-Hoc command is a one-liner ansible command that performs one task on the target host(s)/group(s).

Unlike playbooks  which consist of collections of tasks that can be reused  ad hoc commands are tasks that you dont perform frequently,
such as restarting a service or retrieving information about the remote systems that Ansible manages 

This command will only have two parameters,
the group / target of a host that you want to perform the task and
the Ansible module to run.
eg:
ansible 172.19.16.16    -    ** i could pass an ip addresss , that becomes my target 
ansible 172.19.16.16    -m ping                   *********** i can  passa amodule, mayb i wnat to ping that server 




  MODULES
https://github.com/LandmakTechnology/Ansible-mc-series/blob/main/04-Ansible%20modules/Type_module.md

They are the main building blocks of Ansible and are basically reusable scripts that are used by Ansible Ad-hoc and playbooks.
These are small programs that do some work on the server.
E.G  we hv a module called ping 
this is a small program, this is an ansible program that is written in ptyhon

Ansible comes with a number of reusable modules.
it is synonmous to modules on tefrm registry, u can all a vpc module, an ec2 module, u can reuse them as many times AS POSSIBLE
it has already been written, so you only need to referene them and it will be called frm the registry
so its the sam ething that hasppens here 




  TERRARY OPERATOR

how do you mk your script, your interpreter you hv to select it here, based on your selection, the ? is called itinerery,terrary operator, its what always evaluates the
condition,
so am saying , this  var.os == "windows" ? , is it equals to windows, if this is true then your interpreter will be powershell, then it is goin to run that windows scripts
then if it is nt then your interpreter wil be bash, 
so this ? is called a itenery operator, it always evaluates, its a bullion variable, it always evalustes either true or false




    INTERVIEW QUESTION                                ****   1:37:27
why do we need a -k option when ruuning an ansible  adhod command

ANS:
 we use it when run ssh AND so that we will be prompted for an ssh PW 



     ITEM POTENCY
Item potency of ansible, ansible will not recreate what it has already created


ANSIBLE FACT       (pingpong, meaning success)


this hostfile is still the password for ansible and when ansible goes ther its looking to see if ansible has been installed in those servers
   'discovered_interpreter_python": /usr/binpython3
it uses what is called a module called anible fact, it goes and FINDS the interpreter there, it discovers python is availabeland thefroe it says ping pong meaning it is a 
success


HOST LEVEL PRECEDENCE

##########################  we knw that host variable has the hightest priority
if variables are defined at host level, it takes precedence over varaibles defined at group level


INTERVIEW QUESTION
why do you need PW authentication enabled on your ansible remote node when your doing configuration

   ANS:
so that you are able to transfer the key , able to transfer your ssh key, public key, we wll need that PW


  INTERVIEW QUESTION
default home of ansible is /etc/ansible 

This directory will consist of:
a) ansible.cfg
b) hosts    and 
c) roles dir

  bc whenevre we run ansible, the 1st tin is ansible will read the configuration file & ansible has the option of creating the file in other locations other than the default
location, & so whenever we run ansile,
it wil look for the config file based on the priorities, it wil check if we pass env variable/mee..reference it, if its nt there it wil check the pwd
so if i created a playbk where my ansible.cfg is , ansible wil use the file to read the configuration thats the priority, if it doest find it it wil go to the home of the
user and then the default 





VIDEO 2; class 34

  QUESTION:
1)i was goin tru an env and i noticed they deveolped a docker file that had lifecycle when developing it
so is there a way that terfrm options can enter docker files bc when you were doing file i didnt see smt like life cycle in it but i saw that meta argument of life cycle which
is in terfrm in docker file

ANS:
ITS TRUE THAT there are some things which you will see more of it based on what we have covered already bt talking abt life cycle that is in terfrm in docker file , you may 
hv to share that with me personally let me lookat it 
but the base line is that if you have  a mastery of what is expected of frm which is the basics there are a lot of docker files you can play around with in dockerhub bc all
the images in docker hub has an associated docker file 
and for the official images ther docker file are mostly in github wher you will see other options there

2)
what is the better way to provison on aws
bc i went tru the aws solution architech course and they were heavily on cloud formation 
so is it terrafrm or cloudformation

ANS:
MOST ATTIMES YOU MAY BE exposed to both of them based on what you want to provision but generally companies are serching fo terfrm 
bc it cloud alcostic meaning you can use the same code across multiple cloud just by changing the variables






VIDEO 3, CLASS 33

INTERVIEW QUSETION
can you configure your ansible so that it execute task on remote host in series , ie one host after another

ANS:
yes you can do that by setting your forks number to 1 so that ist only executing task on one SERVER AT A TIME 

bt right now by default its set to 5, so its able to configure 5servers at the same time, basically in parrarrel 



 INTERVIEW QUESTION
WHAT is idem potency 

ANS:
Anisble does not reinstall  a package thats already insatlled or run a task that has already been completed 



  INTERVIEW QUESTION
when you  pass an option like :gather facts= to false what module are you disabling

ANS: 
You are diasbling the setup module


 QUESTION
1) when u did the ssg ken gen, you configured for 3servers, what if you want to do it for 50 servers , is that what you will do for all 50?
also when you created a user you first created a PW, but to do it for 50 users, is ther a way to do it in bulk

ANS:
you can hv a script that can loop tru your invnetory , bt ryt now we v been do in it maunally bc we nid to understand the concept, bt as we go further Tomao we wil look at 
dynamic inventories if i hv 50servers ders no way ill go into the 50 serevers to get the inventories, the i[p addresses  so i can configure it, with dynamic invnetory, i 
dnt need to configure my host file, instaed ill use a script taht wil go into aws , even if its 100servers, based on the script am using it goes and and get all the servers
ips and return it as an inventories then ansible will just use that as an inventory to go and do your configuration

   CUSTOM AMIs
but what ull do here, esp in your prod env goin forward, most companies wil use what is called custom AMIs, ie an AMI that hv created bc am using it for a coy, this AMI will
be managed by ansible b4 i craete the AMAI, i already exchange the key i pass in the key, the ssh key, you load it with all the software b4 you craete an instance for it
so when i introduce ansible to mg8 it i dnt nid to worry abt ssh keys anymore bc this custome AMI alraedy has a user called ansible baked in, a pecific key has already been 
exchanged so my ansible usser is already set so ill create ec2 instances based on this AMi, i can spin up a 100 instances based on this AMi.when i use that AMI , it will 
already hv been configured to be maintained by ansible but if i just hv insatnces that exists in aws then we ll see hw i can use dynamic inventory script that wil go into aws 
search tru d region, get all d inastnces and return that as an inventory then ansible wil use the ip addreses and go out and just do configuration.




 QUESTION
1) when u did the ssg ken gen, you configured for 3servers, what if you want to do it for 50 servers , is that what you will do for all 50?
also when you created a user you first created a PW, but to do it for 50 users, is ther a way to do it in bulk

ANS:
you can hv a script that can loop tru your invnetory , bt ryt now we v been do in it maunally bc we nid to understand the concept, bt as we go further Tomao we wil look at 
dynamic inventories if i hv 50servers ders no way ill go into the 50 serevers to get the inventories, the i[p addresses  so i can configure it, with dynamic invnetory, i 
dnt need to configure my host file, instaed ill use a script taht wil go into aws , even if its 100servers, based on the script am using it goes and and get all the servers
ips and return it as an inventories then ansible will just use that as an inventory to go and do your configuration


                                            CUSTOM AMIs
but what ull do here, esp in your prod env goin forward, most companies wil use what is called custom AMIs, ie an AMI that hv created bc am using it for a coy, this AMI will
be managed by ansible b4 i craete the AMAI, i already exchange the key i pass in the key, the ssh key, you load it with all the software b4 you craete an instance for it
so when i introduce ansible to mg8 it i dnt nid to worry abt ssh keys anymore bc this custome AMI alraedy has a user called ansible baked in, a pecific key has already been 
exchanged so my ansible usser is already set so ill create ec2 instances based on this AMi, i can spin up a 100 instances based on this AMi.when i use that AMI , it will 
already hv been configured to be maintained by ansible but if i just hv insatnces that exists in aws then we ll see hw i can use dynamic inventory script that wil go into aws 
search tru d region, get all d inastnces and return that as an inventory then ansible wil use the ip addreses and go out and just do configuration.

*** 1:43:20
then for users, u can use loops , loop tru instances, if i hv maultiple packages that i need to install, i can use like foreloops that will loop tru all the variables that i 
hv, it will loop tru all the packages and one by one & those wil be installed & the same thing again for users,we can set a foreloop so that it  loops tru all our inventories
and for each server its configuring a user

as we go tru play bks and roles then things will start to mk more sense bt now we are focusing on the conept and the basics of hw actually things works




     INTERVIEW QUESTION
1) What programming lang can you write???
   
ANS:     YAML
when you writing playbk, its written in yaml so ull hv an experience writing YAML language



 2) whats the diffERNECES btw ansible and terfrm 
ANS:

ansible wil run frm top to bottom, that concept we say that ansible is procedural , it follows a certain procedure ,frm top to bottom so you hv to order the task 
so when a task fails ansible will stop, if it encounters smt that fails your play will stop and it will stop right der bc it runs frm top to bottom

whereas
terrafrm is declarative, is nt procedural, it doesnt matter hw the files are organized, doesnt matter hw u put the mainfest files in a dir, bc ist declarative, it will read
everytin firt then create  a plan , so it doesn tmatter the order of the files 
thats one of the key differences when it comes to our automation tools

 ADHOC COMMANDS vs PLAYBOOKS
so for ansible we hv adhoc commands but if we hv multiple commands that we want to pass tru ansible then we can write a play bk 



what command do you use to find your present working dir?  
ANS : pwd
now if i ask you to find your pwd, i dnt expect you to write a script, to put that inside of a script bc its a one line command, its a command that u can directly execute on
ur CLI
BUT if you have a multiple/series of commands that you want to execute then you can put them in a script 
so thats what an adhoc command is and we do that bc its smt that we dnt execute often 
adhoc command is a one line command and its smt we dnt execute often but if we hv multiplr command that we always run all the time then 
it is best practice to put multiple commands in a sript( **me,playbk) and its less prone to error and all the time you run the script, you are runing the same command so its
less prone to error if you execute it as a script as oppose to u typing 10 commands bk to bk on the cli, you are prone to make mistakes
so for one line you can run it directly on the cli but for multiple commands u put it in a script thats what automation is about 

 


  INTERVIEW QUESTION
the first playbook that i ran to install httpd, we see that we we installed httpd then we started it, 
now the 2task are dependent on each other , installation had to be executed before "start" is executed, if the first state is absent the 2nd task will fail 
so bc the task are dependent on each other then we can create dependencies, ansible gives you a way of creating dependencies btw task & for the dependencies we can use handlers



 QUESTION
1) you said playbook is procedural  so is it realy necesssary  for us to use handler to notify the next process if it would run anyway??

    ANS: 
Yes, as we look at what handlers do, they only run when notifed bc if lets say ders no change in the httpd, its already running, ders no need to run this task (handler part
so ansible shud just go ahead and run the nxt play, ders no need to run this task if its nt necessary so this is just a way of cleaning up our playbk so that task only runs
when necessary and therfore handlers is a way of doin that, creating depencendies btw task and they will run run if they are notified, if there is a change then handler is
notified if ders no change then thers no need for it to run 



### just go over playbooks bc this is the really key of ansible, ie you knowing how to write  a playbk and executing using the diff modules
and if you go to interview you can say that you hv experience, you use diff modules to do config 
you v used like cariable to craete conditions so u are able to write a playbk that if you hv a fleet of 100instances but some of them windows and some are linux bt u just 
want to patch ununtu servers or you hv multiple patches for ubuntu , windows and redhat, so you can hv a playbk that uses condition it says 
if its a windows server then this is the patch ur to apply and vice versa, then your using the same playbk but with conditions then ur just running one playbk and ansible is
goin out into all your servers and bc of the gathering facts, it finds out on its own which server is what,if its windows then it applies the appriopriate patch vice versa
thats what automation is then u dnt hv to be logging into servers, this is my windows then running commands there

so as an engr, you sit bk tk your coffee and let ansible do the work for you  but the task is you to understand how to write the playbk, how to create the conditions , hw to
create depencies, when to run what, you can automate so that it runs on a schedule and those things are now like some advanced concept of ansible 
by the end of your bootcamp then u guys wll be expert in automation, now combining terfrm and ansible 




vIDEO3b, Class 34, v

My xperince in ansible includes writting and maintaining ansible playbks, and the playbks that hv written comprises of 1 to 8


IQ: How can specific tasks be runned in a playbooks?
1. ansible-playbook apache.yml  --step
2. assign tags to tasks 
  ansible-playbook apache.yml --tags 'install, start'   *********** 41:00      to run A SPecific task in a playbk
  ansible-playbook apache.yml --tags 'copy'
How can tasks/handlers be skipped in an playbooks?  :    ********  to skip task
  ansible-playbook apache.yml  --skip-tags 'install,start'

What is the difference between tasks and handlers?:
Tasks are executed by default from top to bottom.
Some tasks has to notify handlers. 
Handler will be executed only if the tasks changes.

Difference b/w copy and template module? :  
  template module copy both static and dynamic file content 
  copy modules copies only static file content 

Which module does gather_facts uses? setup module
We can refer variables in ansible-playbook using this ginger template -- {{ varName }}  


 
what is verbose mode in ansible? -v -vv -vvv
ansible-palybook apache.yml -v
ansible kops -m command -a "free -m" -vvv      **********
you can run ansible command in verbose mose, it will give you a lot of content like call logs   ******** 59.24
so if ur trying to troubleshoot a problem in ansible, u can run it in verbose mode, it will display logs 


1:03:07
************ this is whY integrating ansible with k8 and jenkins is very powerful 
what can we do to fix a problem wher we want to copy a file but it doesnt allow and as a result other task is nt executed
ANS:
we pass ignore error= true

we can also run -vvv
ansible-playbook tag.yml -e "index=dEVOPS"  -vvv
it will give us more info about the problem 
it will troubleshoot your ansible playbk




VIDEO4, Class33


1:54:08
QUESTION

is there a command to pass so that capital for the RedHat stays correct
ANS:
we hv what is called build in or already predefined variables that you cant change bc these are already values that come with your system
so if its RedHat that show the variable has already been defined, so we cant alter it to make it case insensitive
i dnt think there is a way to do it , but you as an engr, you can use ur scripting so that you can mayb give it multiple options
am just thinking of ways cud do it
you cud say mayb if the name is this or this then it shud still work, get that particular name but then its now  prone to error bc what if you hv 2files that hv diff 
content (in one the name is capitalized and the other is lowercase then you are prone to introduce errors,) so thats why they just do it that way
so you a an engr, having worked with ansible you can predetermined what variables you want to use, so bc uve already predetermined if you just want to use os family, so now
you can run your setup module and check how to pass them
am sure theres a way around it but as of now we just stick at what we have

     QUESTION


**** 1:56:40          USING src & destination

1) frm the last eg 5v) above, u did src and dest and i know when u were teaching us b4 abt copying and all that thers a place "  remote source =yes
does it apply in this case, if not wher is it applicable??

i think he was refrering to the Tomcat playbook in video3, (3:00:1)

  unarchive:                                                                
          src: "/usr/local/apache-tomcat-{{req_tomcat_ver}}.tar.gz"          
          dest: /usr/local                                                       
          remote_src: yes    

ANS:

its not bc our index.html file is right here, its on our control node, we are like copying frm where we are , our conrol node &its goin to a remote, so thats why we are not 
doing a remote src but if this index.html (src: index.html) was inside of this server,i think (web host), then we could say the src, we will give it the path to where it is 
then the dest, then we ll put "remote src = yes" bc all this action is happening in this server (web host) bt in our case we created our index.html in our local host(i bliv 
control node) which is running our ansible 


     ###### after break we go to dynamic inventory

                                    CODE FOR FROMATTING SYNTAX
1:58:55
2) is there an automating tool to format the yaml file just the way we hv with tefrm format instaed of this manual setting  and evrtime trying to set the yaml for the syntax

ANS:

i wouldnt say there is but on the vscode, i think you shud hv , you can install the extension (look video ,1:59:12) on your vscode
i think you can instal a yaml extensin that helps with fromatting or syntax
but am nt sure ther is one as we run terfrm format to automatically format, thats why its imp you understand/knw how to write yaml
i dnt think ders a short cut out of that, but yaml it is veryeasy u just need to pay attention to indentation
but otherwise wverything is just writing it as a map or a list , with just identation and you can always just use a tab or auto spaces for indentation



3) whats the sequence for execution when it comes to the roles file
Also, i was wondering lets say its an xml file, will like just get the original xml file and mk certain tins variable and just put a .j2 and that will be enough to create a 
a your templat file??

ANS:  2:02:21

any template file will be a jinger2. bc we are using templating, so u can get any template file bt we knw when u hv like an http or apache server, we hv what is called an 
index.html page, that is the index.html that always deplays on the site once you logon , that is your default page, so as long as that file is called index.htm then yes
that is the file that we are using, so u can get any file but as long as its called that name and it has that extension then u can pass in your variables, if this file
  src: index.html.j2  (index.html.j2) was static then ill just pass it as index.html, now here i can either use template or copy, it doesnt matter cos template can transfer 
both static or dynamic, it doesnt matter but if i want to pass a dynmaic i must use template, if i use copy, i can only pass static, if i was using copy then this file will
just be index.html, i dnt need to put .j2, bc .j2 is making it a template file 

******* then for sequence, it will follow the task, everything always executes the tasks, the actual work that is been done is base on the task, evrything is written as a 
task so it will follow the sequence of the task... 
if a task calls a handler then it is executing that task 
so the order it is procedural but it always follows your task if u just hv one task then that is the task that is goin to run 
if i hv multiples roles, like if i hv 2roles that am calling then it will be ordered in the sequence of hw i hv placed, it will be role1, then role2 
but in role1, it will always be task1, task2, task3 till the end , then when that is done it goes to role2 and repeats the same process
but if i hv those 2 roles but smt in role1 fails then the playbk will fail bc ansible is procedural




 QUESTION                    CREATING THE IAM ROLE

1) i was wondering hw u wud hv used the iam role to acces the instance since we didnt do that , dnt knw if its in our notes

ANS:
 an i am role, we normally acttach a role to an instance , u can create a role and attach your instance bc your instance it needs credentials  bc we are quering an API in 
aws, aws api, so bc of that we need to configure a role, we need credentials but i dnt knw why my role wasnt working in aws


3:09:14

when you create a role it creates what is called an instance profile, it gives ur instance a certain profile with permission, so am giving it permission to access other
aws services bc by default services hv , bc of IAM, all services hv deny policy, they are denied to access any other services unless you explicitly allow them 
so thats why now for ec2, bc am running my workload frm an ec2 instance so this ec2 instance (my ansible control node) it needs permission bc its accsesing other instances 
to find their meta data so bc of that it needs permission and that permisson u can create a role but as we saw the role wasnt working thats why i created a secret and access
key but that is a security issue, we are nt suppose to store accsess key and secret key inside of your instance so instaed u always use a role bc role are short lived 
they are nt like permanent, bc they hv sessions, i think a section by default will last for an hour then that session will be over 

3;14;00
but if i was running that on my local env(terrfrm) and bc am running it on mac which is linux, therfpre am able to run ansible, am able to install ansible on mac bc its a 
linux env 
i run ansible , i see i hv ansible installed

3:19:00  ;;....   bla bla bla 
he passed a custom AMi


QUESTION
    CAN ANSIBLE PING INSTANCES NOT CREATED WITH TERFRM

DID We at any stage exchange key with my other instance(d once we created directly on aws) that we created bc hw is ansible master able to ping my other instance since it
wasnt created frm terfrm 

ANS:
the way inventory works is its not necessary that it needs to be created with tefrm, it just goes in your aws, whatever inatance it finds there it gets it ip adress
so whather u created it with tefrm or nt it will just go to a region and find all instances and return them 

                       PASSING TAGS IN PLAYBOOKS TO SPECIFY INSTANCES FOR CONFIGURATION
but if you wnat to select specific ones that swhy ull now use tags 
if i want to configure only instances with a specific tag then i can pass that in my playbk, i can say this playbk will run on instances with a tag called tomcat so thats
how dynamic inventory will wrk , its now goin to use tag to filter to deter which instances to use 
so thats why whenevr u are provisioning instances you hv to tag them give them a name in prod that show it works
if its a server that belongs to prod , it has to be tagged ad prod  etc, cos when ansible goes to do configuration it is looking for those particular tag for it to be able 
to configur

2) in prod do we actiually shut down servers??
ANS:
NO , it depends , if its in prod thats a downtime , ur instances dont, but if its goin for maintenance
but then in prod we use elasic ips bc u asign an elastic ip on your instance so that even if your instance stops or it goes down,it will stil maintain the same ip bc of dns
we always want your ip not to change so must of ur instances in prod will hv elastic ips 



      *****INTERVIEW QUESTION.....

                  WHAT NEXT AFTER MANAGER APPROVES A LINE OF CODE
3) i was aksed this question in an interview and i was really kind of confused 
in production if for example the manger approves a line of code, how does the process go ???  frm the tesing to prod and now the manager has approved that the code is fine
how do we proceed frm ther

ANS:

if its approved , if its anything that has to do with code then that particular code is merged depending on whatever line 
if ur working on dev branch or staging branch or a bug fix that always needs somebody to look at or if its a new feature that you are developing then once the code has been 
written and ther has a peer review, it has been reviewed , and the pipelines have run and they hv succeeded and they hv run all the test and it has been approved
once it has beeen approved then you can click the button to merge bc your pull request has been approved now now you can click the button to merge and your now merging it 
into main or your merginging it into whatever branch that ur suppose to be merging into once that is merged then basically the pipeline wil tk over and run & create a build 
frm there
but its always that once you do your work your peers or manager has to approve nad thats why you will craete a pull req, ie ur asking for other to review your wrk before its 
merged bc u cannot approve your own work, someone else has to and for that to ahppen they hv to look at what uv done 

the main is what is in prod so ur creating a branch out of your main & if its just a hot fix,u do ur modification/changes and do your pull reQ which is tour requeting for 
your hotfix to be merged back into main cos your just modifying, it was a bug or smt so you create a pull REq , ur peer or peers will look at it if its approved ,you obviusly 
once you create an hotfix branch as you creater apull REq A PIPELINE WILL RUN TO ENUSRE THAT DOES WHTAT SSUPPOSE TO DO , once it passes and its approved then u can merge 
directly into the main and that resolve the issue that was running in production



   Proff ken:  what are the limitation of the command module
The Ansible command module directly executes commands on remote servers, offering high security but lacking intelligence. It cannot use shell features like pipes (|), 
redirects (>), or environment variables. Critically, it is not "idempotent"it runs every time, never knowing if the system is already configured, and cannot handle 
complex logic or verify results. 

Here are the key limitations in simple terms:
No Shell Features: You cannot use |, <, >, &, or $VARIABLE. It simply runs the binary, not a shell script, making it useless for chained commands or complex logic.
"Dumb" Execution (Lack of Idempotency): The command module doesn't check if a change is actually needed. If you tell it to create a file, it will try to create it every 
single time, even if it already exists, which can break system configuration.
Requires Full Path: You often need to specify the absolute path to a command (e.g., /usr/bin/echo instead of just echo) because it doesn't load user environment settings.
Limited Error Handling: It often returns a "changed" status, making it difficult to differentiate between a successful task and a failure.
Best Used Rarely: It is considered poor practice to use it for complex tasks because it defeats the purpose of automation, which is to ensure a consistent state. 

When to use it: Only for simple commands where no shell features (pipes, variables) are required, such as triggering a specific binary. For anything more complex, use the 
shell module or dedicated modules like apt, yum, or file. 


CLASS 33 Summary fEB18

prof diff btw task and handler 
task is executed in the playbook chronlogically frm top to bottom, also task notifies handlers while 
handlers is a task that can only be executed if there are changes in a particular task 


1:01:57
Diff btw copy and template module?
Template module copy both static and dynamic file content
copy modules copies only static file content

WHICH module does gather_facts uses?   >>>>>>>> setup module 

WHICH template do we use for referring variables ??   >>>>>>>>  ginger template -- {{ variable }} variable name



VIDEO4, Class34

 in our bootcamp, we are goin to see, as a matter of fact i already created a github repository for jenkins ansible intergration 
we will see hw jenkins will work with ansible
this configurtaion that we ddid here we are goin to do it frm ansible, so evrythin will be automated 
we will est full automation wher we are goin to hv a cp
we are goin to use terfrm to create our tomcat server and once the server is created, dynamically it will trigger a downstream job that will configure the tomcat server
once that is done , the ci job also will be triggerred, we will see all of that in bootcamp 


