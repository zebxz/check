  
LINUX , scripting, git , maven, tomat/ngnix tomcat, sonar, nexus, jenkins , aws, docker, k8, terraform, ansible, k8/helm 
In our env, we use LINUX OS for file,process,package,security mgt, GIT FOR VERSIONING, MAVEN FOR BUILD, JENKINS automates the end to end process, 
AWS for cloud computing services/resources, DOCKER for containerization ie deploying light weight containers, KUBERNETES for container orchestration/MGT  

 

so from warheouse associate to customer rep to business teacher, every single one of those led me to this point, i didnt trow any of those skills away & trust
mw when i say you gonna get a slice of every piece of it.


*** if you think what uve have heard so far is interesting then you should let me introduce you to k8
AT this point you would think thats where the magic ends but no, you havnet heard about ansible & terraform.


In docker applications run as containers   
QUESTIO .. 2:27:00
is docker a replacement for tomcat
no its nt , its a newer version to deploy appl by containerizing the appl including the appl code n all its components ,, with docker u can deploy any type of appl including
python,nodejs, tomcat, docker can containerize any form of apllication

docker5&6
 MULTI STAGE DOCKER FILE
 multi stage docker file  :  it has multiple 'FROM' keyword


************************************************




***********   SUMMARIZED FOR INTRVIEW**********************
Am good with docker 1&2, 5&5b, docker7 & some of 3&4 videos

**********the problem with docker is that when you create containers, you cannot scale BUT with kubernetes we can.

In tomcat applications are deployed in heavy weight virtual machines VMs:
In docker applications are deployed in lightweight containers:
****i bliv the short fall of tomcat is resolved by docker & the short fall of docker is resolved by k8... 
 so when we containerize with docker we ar going to manage those containers with kubernetes, dats what we wil be seeing

Docker is all about 
Docker files for creating docker images, Images for creating containers, containers are the run time of a docker image.
ONCE THE applications are containerized, we ar able to run containers
Once developers commit the code, we can create/modify docker files, build images & push to SCM & we are able to do all this running docker commands like 
docker clone, docker build, docker run, also we can make use of docker compose file or docker swamp, depending on what the job is meant to achieve. eg what
are the type of services we need to deploy eg login,cart, checkout, we need data base to capture data, we need to ochestrate the containers, the containers
also need networks for communication
networks--- (default bridge,custom bridge, host or null network) for the containers to communicate with each other communication[internal or
external] 
And finally volumes;volumes = storage/mounts for the containers, the storage can be stateless (does not store data), or stateful needs to maintain its state,
to prevent data loss,therefore it needs a piece of storage to capture data, data nids to be stored.this include data bases , applications like jenkins
   databases / jenkins, we use jenkis to be able to deploy a lot of works, so we hv jobs in jenkins.
so we can create a mountpoint and mount it wit the dockerhost server,he is the guy hosting all our containers.


..1:31:18                                 TWO WAYS TO DEPLOY IN DOCKER
1) We can deploy the database and the aplication either using command ...... OR 
docker run --name mongo -d --network tesla \        
-v ebs3:/data/db -e MONGO_INITDB_ROOT_PASSWORD=devdb@123 \                                                                        
-e MONGO_INITDB_ROOT_USERNAME=devdb mongo
 
docker run --name springapp -d -p 8080:8080 --network tesla \
-e MONGO_DB_HOSTNAME=mongo -e MONGO_DB_USERNAME=devdb \
-e MONGO_DB_PASSWORD=devdb@123  mylandmarktech/sping-boot-mongo

2) using Decompose' ie using 'a file' ..1:31:18
Docker Compose is a tool for defining/declaring and running multiple  containerised micro-services applications.

   ALSO******Docker 5&6, we saw multi-stage file
ALSO****docker10***  Docker swamp is simply a container oschestrator, good for scaling
so we v docker swap , kuberDockernetes , openshift ..all of these are ochestrators
Docker Compose is a tool for defining/declaring and running multiple containerised micro-services applications.
***i realised that inside 'docker compose, we dnt put docker commands like 'docker run, docker build' to deploy the container.
i bliv  its bc of the docker client, its docker the build and run command.
 *** docker 8&9.. 
 when you check the docker file in github, this is when u install dis docker file, it does a build while running the docker file
we get the docker file when we clone the appl & inside the file;  we see the 'pip command.; we hv docker file for wildfly for e.g
RUN pip install --no cache dir -r requirements-prod.txt      2.00  .... this is the command that wil do a build , its doing a build wit pip installed 
                                                              whil creatin the image, pip instal is like deploying the application


***docker8&9***
dnt freak out bc this code is part of our project already, if u go to the project that we v for this stuff we already v d code ther
                  is oin our github repository, this docker compose file is already ther, most of the time it wil b part of ur source code bt u nid to understand if u nid 
               to change anytin like the image version and so on 


****this particular ci/cd ticket in docker 3/4 & docker 8&9,  includes jenkins/docker integration
 *** for this ticket, i bliv we can ignore no.1 to 4, & start from no.5, then we can run 'docker scan' to scan the image after we build the image. 

*** i may only need docker, kubernetes, ansible & Terraform for my job.







START

In my environment we use docker to containerize applications & deploy applications ie applications run as containers
Docker: Is a containerization multi-platform software use to;create, build, ship, share and deploy containerize applications 
Compared to the Traditional/Physical Deployment & Virtualized Deployment options, containers are light weight,containers starts up very fast like a cheetah n the integration is very 
easy. containers also resolved the issue of repetitive task associated with the virtualized system, eg(having to install in the dev en & also testing env ) & thats because 
Docker containers: Is a Run time instance of a docker image, docker image contains everything required for an application to run create or evry tin req for appl/process to run 
(App Files (code), Dependencies, (Softwares +Libraries), ENV vars & Other Configuration files) & we are able to build images from a docker file
we have a docker file from the file we build an image from the file n the process of building the image is called containerization IE PACKAGING THE APPL with all its DEPENCIEDS IN AN IMAGE N FRM DER WE AR
ABle to run containers/ deploy applications
1:42 we v a build server,with the server we executing mvn package, creating package , we ar also building images  using docker file, for docker build n once dat is done we
can distribute these images , we can use dockerhub or nexus ,or amazon ecr or jfrog to distribute the images .. these images are push into the image registry and frm
the registery a lot can be made to happen ,, we can pull the images and deploy appls in any enev, therfore ,we can pull the images for deployment, docker pull to pull
d image n docker run to deploy the application, in the deployment server we v the pull n run and dats where deployment is taking place and this can take place in
multiple env like dev, stage and prod,, the same image ,, we can deploy in any number of env... once they ar built these images are shared/distributed 
meaning it can be easily reused as many times as possible.
with the docker exec command we can interact with containers, get into containers troubleshoot issues and do a lot of other stuffs as well.
with docker cp command we can copy copying files and directories between a Docker container and the local filesystem of the Docker host without needing to 
restart or rebuild the container 
** mee*** and this is very useful bc the uptime of we appl is very important. we dnt hv to keep restarting our appl everytime we nid to mk changes so thats a 
plus with docker
when the process in a container is running der ar obviously some changes
when u deploy a container, der wil b some changes btw the container n the image wich was used to create the cointainer e.g we v copied some files into the 
container, and to know the changes that have occured in the container we use the 'docker diff command' docker diff container name/id
its the diff btw the image used to create the container n the changes tahe hv occurred in the container and inorder to maintain
the changes in future application , we can commiit the changes using the 'docker commit command' which enables us to extract an image frm a running container
ONCE THE applications are containerized, we ar able to run containers
 and we need  networks--- (default bridge,custom bridge, host or null network) for the containers to communicate with each other communication[internal or
external] 
And then we have volumes;volumes = like bind mounts ie we can attach a piece of storage to the containers mount point because our data base can either be 
stateless (does not store data)or stateful needs to maintain its state,to prevent data loss,therefore it needs a piece of storage to capture data, data nids
to be stored.so we can create a mountpoint and mount it wit the dockerhost server,he is the guy hosting all our containers.so we can create a mountpoint &
mount it with the dockerhost server,he is the guy hosting all our containers.
FInally, in our env we create Monitoring dashboards, that monitors and report if there are issues, so if there is a problem, we are aware before the customers
starts complaining 

***meee*** however k8 is a more powerful tool that knocks docker out the water & i'd be glad to expand more on that.



PROFF
#####u guys dnt nid to freakout, the big issue of docker u can explain it in less than 2minutes in an interview
my experience inclues writing docker files, maintaining docker files, to containerize our applications, we generally have java base application in our envr, 
we run those application on a CI/CD platform and before we run the application , we nid a docker file to containerize it as part of the CI/CD process , so i am 
task to write and modify docker files and when i do so, i mk use of key Docker words like: FROM for my base image , COPY to copy files from the host to the 
container,RUN to execute commands ,CMD and ENTRYPOINT instruction to execute command while creating d container and it is important for me to make mention tht 
in writing my docker file in our env we mk use of docker file  best practices and these practices that we implement in our env include ensuring that our images 
are light weight and for that to happen, we are adviced nt to download unnecessary packages, nt to copy unnessary files into our containers, nt to use multiple
run instructios, so we use the && instructor as much as possible and that reduces the number of layers and by so doing we are going to be creating light weight
portaBLE images.also we only make use of official images as our base images, so docker official images those are the ones that we choose. in an attempt to keep
our images as light weight as possible, in our env we use alpine images as much as possible bc linux alpine is the lightest OS for linux that u can get bc docker
is intended to be light weight
In Passing our CMD and ENTRYPOINT Instructions, we are using executable form over shell form this will be enable our application process to run as a parent 
process and as such, it cannot be killed unnoticed
We also enabled scanning in our env, such that everytin is auto scanned and if thers any issue, pull REQ are been deployed for us to take remedial action 
*************docker is intended to be light weight bt then u nid to do smt to mk it light weight by making the right choices



WHAT IS SOME OF THE PROBLEMS U HAVE FCAED IN UR ENVR RECENTLY AS A DEVOPS ENGR.
we hv some of our docker files which we hv used to build and deploy applications and we had issues wher  applications were failing, endusers were complaining,
so i had to TS , ha sto check hw the docker files were written and i realized that our CMD were been passed in shell form and as a result, the applications 
process were been killed unnoticed and i quickly modify the dockerfile, changed it from shell form to executable form such that the application will run as a
parent process. 

there are some things u are going to explain, that will be the end of the interview , they wil knw that u knw, bc ther are some intricacies that somebody who is at work
whose hands are dirty like yours, u ar going to be making mention of them n they will knw this guy his hand is really dirty when it come sto containerization 
with docker





**meee****
STEPS FOR HOW APP IS DEPLOYED USING DOCKER AFTER CODE IS COMMITTED
1)the first step is developers commiting codes to scm , then we use mave to build the application and deploy either in the tomacat or docker
for tomcat deployment takes place in the opt/webapps dir target dir and the packages are created in the the target dir of our project & also uploading the artifacts into the maven local/default
repo OR custom repo
2)But in our evn we use docker for deplyment therefore;
Once developers commit the code to github The next step is to clone & build, the appl is cloned as a micro application, ie instead of deploying the appl as
a monothelic appl, we decouple it and then we have a micro service appl with stanalone entities, a repo for login, a repo registrtaion, a repo to cretae a
cart, a repo for returns,these ar the diff repo we hv created when it comes to this projeect, so when we pull this image we ar creating multiple packages e.g
login.jar up to retruns.jar, etc.
3)Next step is the netwrk, ie a group of servers communicating with each other using a specific netwrk, therfore we create a netwrk to make sure communication 
is established btw the servers using the container name. so with the images of these stanalone entities built into containers we deploy our containers in 
these deployment server using a custom bridge ntwrk so that the login container can communicate with registaration container ,cart container needs to 
communicate with pay container, we also v a db container etc and all of this is captured in the dabase container (i bliv the inf needed frm endusers to access
the other containers will be captured in the db)
4)Next is volume, ie attaching a piece of storage to the container. it can be statless or stateful. and this takes us to the volume concept in docker;
bind mount our app container which is a STATELESS does nt store data, bt d database is STATEFUL n needs to maintain its state, to prevent data loss,therefore
it needs a piece of storage to capture data, so we can create a mountpoint and mount it wit the dockerhost server,he is the guy hosting all our containers.






*************** END**********************



 



DOCKER1&2
  CONTAINERS HAVE ALL THE REQUIRED DEPENDENCIES 
 *****now the quetsion is what can be done to be able to package d appl plus its dependencies n kip such dat if i wnt to deploy d appl all i hv to do is ship the app to 
the deployment env and it comes wit the dependencies already packged meaning ders no nid going to the dev enev n carry out conf, test n run , we can easily ship dis appli
how can we mk this process simplified?
therefore we will be usisng :
Containerization Software/Runtime --> : with this we can package an appl code plus its dependencies
 these ar the dif dockerization software
Docker, = over 80% usage , in the industry, it has the most level of trust

Docker:  
  Is a containerization multi-platform software use to;    
  create, build, ship, share and deploy containerize applications  


Dockerfiles are input use to build docker images, docker file is a simple text file that consists of instructions to build/assemble a docker image
Docker can build images by reading the instructions from a docker file

Docker images are input needed/used to create containerised applications  

Docker containers : Run time instance of a docker image 
                  if you execute: Docker run, a container is created from the image
******DOCKER8&9
In tomcat applications are deployed in heavy weight virtual machines VMs:
In docker applications are deployed in lightweight containers:
****i bliv the short fall of tomcat is resolved by docker & the short fall of docker is resolved by k8... 
 so when we containerize with docker we ar going to manage those containers with kubernetes, dats what we wil be seeing

we use the docker client / wich is the cli, command line interface that permits u to run docker commands   
 = docker build/push/pull/run  (and this docker client generally starts with
   docker 

WE also have docker daemon
docker daemon/service =  this enforces the command that is run, e.g when u run docker build, the docker deamon knowns he has to create an image etc                                                                                   
  docker registry  , wher we ship images to and make them distributable0      = ship/share[dockerhub/ecr(elastic container registry) /nexus]    




DOCKER 5&6
******Online
Multi-stage Dockerfile:
A multi-stage Dockerfile is a single Dockerfile that uses multiple FROM instructions to define different build stages. Each FROM instruction starts a new 
stage,potentially with a different base image. The key benefit is the ability to selectively copy artifacts from one stage to another, leaving behind 
unnecessary build tools and dependencies in the final image. This results in smaller, more secure, and more efficient production images.
Docker Compose File:
A Docker Compose file (typically docker-compose.yml) is a YAML file used to define and run multi-container Docker applications. It allows you to configure
multiple services (containers), networks, and volumes in a single file. Docker Compose simplifies the management of complex applications by enabling you to
start, stop, and manage all your services with a single command (docker compose up).
Key Differences:
Purpose:
Multi-stage Dockerfile: Primarily focused on optimizing the build process of a single Docker image, making it smaller and more efficient.
Docker Compose File: Primarily focused on orchestrating and managing multiple interconnected Docker containers that form an application.
Scope:
Multi-stage Dockerfile: Operates within the context of building a single Docker image.
Docker Compose File: Operates at the application level, defining and linking multiple services (containers).
Content:
Multi-stage Dockerfile: Contains Dockerfile instructions (e.g., FROM, RUN, COPY, CMD) organized into stages.
Docker Compose File: Contains YAML definitions for services, networks, volumes, and other configuration options for a multi-container application.
Relationship: A Docker Compose file can reference a multi-stage Dockerfile to build an image for a specific service. For example, a build section in a 
Compose service definition can point to a Dockerfile and even specify a target stage within that Dockerfile.
In essence, multi-stage builds help you create optimized individual container images, while Docker Compose helps you manage and run those images as a 
cohesive application.



 
DOCKER7                  

                              APPLICATION ARCHITECTURE

********DOCEKR8&9 we can use docker to containerize both monolithic and microservices application
' ... bt docker will shine more when it comes to micro services bc they ar very light weight & docker containers are light weight


 1)           MONOLETHIC APPLICATION
MONOLITHIC ARCHITECTURE: ebay webapp
this type of appl wher everytin is coupled together is called monolithic architecture, so it is developed using monolithic architecture
   if u wnt to acces thhis appl, it has cm with diff component : so when u acces this appl, u can login/registration/cart/pay/order/ 

 
PROBLEMS OF MONOLETHIC ....................... 
1)DIFFIcult to scale
eg if thers a spike  eg if 10million people are registering but only 1million is making payment, we will hv to scale up both the entire appl even though only 
1m is making payment,so thers a lot of waste with resource usage in monolethic design 
2)also micro service appli are light weight but monelethic are larger and thats a problem
3)lots of lines of code not easy to mange 
e.g if ur dealing wit a monolitic appl and ur code is in github we hv a single repository wich is for ebay only
if ur using just ebay, ebay could hv 
monolithic = ebay / 20,000 lines of code 


2)  MICROSERVICE APPLICATON
bt now we can decide to use microservices , with microservices, the monolethic application is decoupled, we hv each of theses appl  components decoupled
 ebay.war  = decoupled  , they are now stanalone entities
   login.jar/registration.jar/cart.jar/pay.jar/order.jar  return.jar  
************** so we hv decouples these appl from a monolithic to a micorservice appli
**meee*** Therfore beacuse the monlethic application has been decoupled into diff stanalone applications, when we deploy these seperate entities as
containers, we use network to ensure comunnication between them as well as our data base. we will be deploying these application with a data base 


WHY IS IT IMP TO DECOUPLE
1)A Micor service architecture is easy to manage, 
2)its also easy to scale
microservices architecture wich ebay has been decoupled: login 2,000 lines of code, registration 4,000 lines of code, cart 2,000 lines of code, pay 6,000 lines of code 
order 2,000 lines of code, return 8,000 lines of code   
******* therfore micro services is easier to manage:

3)better/efficient resource usage : 
********e.g in a case wher the login increases frm 20m to 28m then i nid to scale login only, i dnt v to scale/modify the entire appl
    so micro services leads to better use of resource bc when u scale jst a component, it means the overall resources to be used wil be smaller 

4)****************versioning is easier to manage with microservices (bc we hv to modify some codes)
also if thers a new version of the appl that relates with only the login e.g we ar changing the login interface, then we dnt nid to modify the the entire 20,000 line of 
codes as with monolithic but only the 2,000lines of code for login for microservices



               NETWORKS
### when we talk abt a ntwrk its ensuring the devices are able to communicate with each other using containerIPS,under same docker network.
If Containers are in two different networks. They can't communicate/access each other.
types of netwrk
1) Default bridge network 
2) host network
3) none network

PROBLEMS WITH DEFAULT BRIDGE NTWRK
containers communicate with only the containerID
e.g  if a container was dead and it comes bk to life  e.g smt goes wrong n its recreated, it comes bk to life with a new IP address and as such communication
will be broken bc containers communicate with the ip adress in the default bridge ntwrk so for that reason, we use what is called custom bridge ntwrk. 



**************28:30     CUSTOM BRIDGE NTWRK
containers communicate with IPS and the container name, so even if a container is recreated, it will be recreated with the same container name so it can stil communicate wit
each other.


  DIFF BTW THE TYPES OF NETWORKS

 ********docker 3&4 .... 1:25:00 to deploy a 2nd conatiner using the d same image, 
2 containers cannot v the same host port number& name  , bt can v the same conatainer number
host port must change ie we hv to choose a port nmuber for each contaoiner but container port is nt changing
**** docker7; networks:  
We can't create more than one container with d same container port in host network, except in the default bridge netwrk, bt we hv to do port publishing


1)DEFAULT BRIDE NETWRK
containers communicate with only the containerID
e.g  if a container was dead and it comes bk to life  e.g smt goes wrong n its recreated, it comes bk to life with a new IP address and as such communication
will be broken bc containers communicate with the ip adress in the default bridge ntwrk so for that reason, we use what is called custom bridge ntwrk. 
****mee*** in the default bridge netwrk, We can create more than one container with d same container port , bt we hv to do port publishing

Depploying in the bridge ntwrk: we dnt need to create a netwrk like we do for custom bridge netwrk
the normal way we  create & run containers, it now in the bridge ntwrk 
docker run --name app -d -p 8000:8080  mylandmarktech/java-web-app


2)CUSTOM BRIDGE NETWRK
 docker network create tesla        ***to create the tesla netwrk
containers communicate with IPS and the container name, so even if a container is recreated, it will be recreated with the same container name so it can stil communicate wit
each other.
  ***Deploying in the custom bridge ntwrk: we have to create a netwrk 
   create a ntwrk for tesla :  docker network create tesla
docker run --name app -d -p 8000:8080 --network tesla mylandmarktech/java-web-app

3)HOST NETWRK  
We can't create more than one container with d same container port in host network, except in the default bridge netwrk, bt we hv to do port publishing
         2.21.20   docker logs apps .. we see binding exception

Docker Host Network.:
If we create containers in host network. 
Container will not have IP Address. and we cant do port forwarding -p
 Container will be created in a system network.

Deploying in the host netwrk
docker run --name javaapp -d --network host mylandmarktech/java-web-app   
   ContainerPort: 8080                                                        ************ IT HAS ITS CONTAINER PORT 8080  
*********i can deploy the application & access this first application/container but if i create another container with the same container port 8080, it will 
nt run the first container 'javaapp' 8080 is in use & bc this is host netwrk, WHATEVA PORT the container is having dats what wil be assigned to the host 
We can't create more than one container with d same container port in host network, except in the default bridge netwrk, bt we hv to do port publishing


4)NONE/NULL NETWRK
If we create container in none/null network. Container will not have IP Address.
We can't access  containers in this network  Internal or external 
****************** we can use this ntwrk to isolate containers

Deplying in the Null/None network
docker run --name hello -d --network none mylandmarktech/hello   
cant be accessed Internal or external 



We support Java based applications:
===================================
We support nodeJS and .net based applications: (also some python base appl, like we saw when we were doing container, we deployed a python base application)

 For our micro service applications, in our environment, we hv legacy appl that are running as monolectic which as a devops engr we are now decoupling 
monoletic appl into micro services


 Deploy a microservice application for tesla.     *** WE need the spring appl & a database 
we are using an application developed using the spring boot framework
 while deploying this appl, it need a database
we need to deploy the springapp and the database - mongo   ... (the database in this case is mongo) as we can see it defined in github: data: 
  ie developers are  recommending the mongo database           
########copied frm the file in github:      ****** this application is developed using the spring boot framework
spring:
  data:
    mongodb:
      host: ${MONGO_DB_HOSTNAME} .. this is dynamic bc no values were given
      port: 27017
      username: ${MONGO_DB_USERNAME}
      password: ${MONGO_DB_PASSWORD}
      database: users
      authentication-database: admin
server:
  port: 8080

bt we want to write images that dynamic, images that can run in any environment, so we can jst use diff envr variables 
this is an application that is being deployed and some inf needs to be captured so a database is needed.

                                          THIS APPLICATION NEEDS 2THINGS
*****meee****
1) the applictaion image , in this case its; springapp
2) the database image, in this case its; mongo

we need to deploy the springapp and the database - mongo   ... (the database in this case is mongo) as we can see it defined in github: data: 
  ie developers are  recommending the mongo database                                                                                     mongodb:

For thIS APPLICATION, we hv our images so we can pull bt i also nid a mongo image for db (he doesnt hv any mongo image in his repository) bt ders a mongo image in dockerhub
so once i pull this 2images am able to deploy my application


#####meeee**QUESTION, HW DO WE KNOW WHICH ENV VARIABLE VALUE TO USE ,, BC here both the applictaion(spring) has its env var & the database has its own env ???????
i bliv the the devolopers indicate the 'env var' in the code/document they commit in the repo

        ****i bliv this is like a key (the variable) & padlock (env variable)situation...
******he said this in the nxt video , the application passes the variable,ie its like having to be authorized to access the db, thefore the 
input used for the application shud b d same used for the db
e.g  the password we assigned/decide to use should be the same for both the appl & the db but the hostname comes from the db 
     *** i got to realise that what i used to think is the application name is called the hostname, or mayb its called 'applictaion host name.
  *** eg, (hostname 'mongodb')  docker run --name mongodb --network tesla -e MONGO_INITDB_ROOT_USERNAME=devdb -e MONGO_INITDB_ROOT_PASSWORD=devdb@123 mongo 




DOCKER 8&9
docker run --name mongo --network tesla -e MONGO_INITDB_ROOT_USERNAME=devdb -e MONGO_INITDB_ROOT_PASSWORD=devdb@123 mongo
1.08.05    ****Mongo, is the host database hostname...  .... i didnt knw thats hostname, thought it was application name, or mayb its application hostname.

TYPES OF VOLUME
1)BIND POINT:   Not persistent. BUT BindMount: can store your data anywherE;  /tmp/mongo:/data/db ,  /home/ubuntu/data:/data/db

Bind mounts may stored data anywhere on the host system. They may even be important system files or directories. 
In the host server, data is stored in  /data/db, and this is the dir that was created /tmp/mongo
if we ls /tmp , we can see the mount point mongo that we created
this volume concept is nt a persistent volume example, bc this process is nt manged by docker, when we run ll /tmp/mongo/ , we see the
process is not managed by docker and as such it could be accidentally deleted. so what happens if its deleted by a non docker process??

eg lets assume that our database is down, the mongo db is down n so we can enter data e.g docker stop mongo , bt when we retsrat .. docker 
start mongo we ar now able to access the data
now lets kill the database ,,  lets completely remove this database, docker rm -f mongo n so the db is no longer running 
now lets redeploy the database, then we can still see the the previous data..this is because of bind mount 
******27:00  what happens is that any data that is being captured by this database is also sychronizing wit our docker host such that if the database is 
destroyed completely
if we recreate the database with the same mount point, the data will syn with the data that is on this piece of storage that is on our docker host.
BUT DERS A PROBLEM HERE 
if we remove the container again   .... docker rm -f mongo      ... now the container is no longer running , so our database is killed , we cant access it again
at the moment our vol is nt managed by the docker process ,
lets assume the mount point is accidentally deleted  sudo rm -rf /tmp/mongo/
if we recreate our data base, we wont be able to access our data anymore bc the mount point is nt managed by the dcker process n as such , it can be deleted 
unnoticed


 2)   PERSISTENT VOLUME
Non-Docker processes on the Docker host can modify Bind Mounts volumes at any time unnoticed, so we nid to introduce persistence vol 
NOW LETS SEE THE DOCKER VOLUME CONCEPT THAT IS being mMANGED BY DOCKER, we want to talk abt persistence vol 
Docker Volumes
Volumes stored data in a part of the host filesystem managed by docker
Docker home dir :  DHD = /var/lib/docker


DIFF BETWEEN BIND POINT & DOCKER VOL & External volume
****** i realised that for external vol we had to run; docker volume craete ebs3 -d rexray/ebs before deploying the database BUT for bind mount & docker vol
we dont, we only need to deployed the command to deploy the database with the volume
also we need access & sacred key for extenal vol BUT for bind mount & docker vol, we dont.

1)BIND POINT
a)Not persistent. BUT can store your data anywherE
b)for the BIND mount, we nid to create a mount point for the host server  eg from  docker7; /tmp/mydata
we have to create a mount point; 
/tmp/mydata to pair with the contianer mount point of the image form the official doc is /data/db  /tmp/mydata: paired to /data/db
  Storage name is 'tmp, while 'mydata' is the customized mount point  
*****
create the database with the vol
docker run --name mongodb --network tesla -e MONGO_INITDB_ROOT_USERNAME=devdb -e MONGO_INITDB_ROOT_PASSWORD=devdb@123 -V /tmp/mydata: paired to/data/db mongo

****docker10****
we did another eg with bind mount
we created anoda vol called mongo & i realised that /tmp/ is the name of the vol, /mongo, is the customized mount point, while /tmp/mongo, the dir we created
In the host server, data is stored in  /data/db, and this is the dir that was created /tmp/mongo:
ubuntu@docker:¬ $  ls /tmp/mongo/



2)FOR DOCKER VOL   
a) Persistent but Docker home dir  DHD = /var/lib/docker    so ******* in this home dir we ar given the mountpoint point of our volume ie cant store anywhere
b)dnt need to create a mount point, what i need to do,just the vol name: eg mongodata  ,data33                                                i bliv dats what it means
create volume: 
eg docker volume create mongodata -d local  ..... ( -d for driver)  ... here am passing a driver 
eg docker volume create mongo33   ............ here am nt, when u dnt pass ur driver, it will create under local
docker volume ls to see the diff drivers  ... ryt now we can see local, so both drivers are local even though when i created mongo33 i didnt indicte for d driver to be local
so when u DNT CHOOSE driver it is going to crate under the local driver 

create the database with the vol (i bliv data33);
docker run --name mongo -d --network tesla -v data33:/data/db -e MONGO_INITDB_ROOT_PASSWORD=devdb@123 -e MONGO_INITDB_ROOT_USERNAME=devdb mongo
need to create a mount point, just the volume name  data33 & pair it data/db from the official document.

3)FOR external vol
a)dnt need to create a mount point, what i need to do,just the vol name: eg ebs33
b) -d is for driver, the driver is rexray/ebs 
c) docker volume craete ebs3 -d rexray/ebs;   to create the vol
d) Access & sacred key
create the database with the vol;
so he used the file app.sh  and paste the below command 
docker volume craete ebs3 -d rexray/ebs
docker run --name mongo -d --network tesla -v ebs3:/data/db -e MONGO_INITDB_ROOT_PASSWORD=devdb@123 -e MONGO_INITDB_ROOT_USERNAME=devdb mongo   (mongo is image name )
then  deploy: sh app.sh




3) EXTERNAL VOLUMES          ===== Network Volumes Using AWS EBS ==========
ther are external vol pluggin ,that can be installed in docker
we can craete external vol using sometin like AWS ebs , also efs: elastic file system, s3 simple storage system, google persistent disk, also sdsure disk,
rexray plugin for docker storage etc 
https://rexray.readthedocs.io/en/v0.8.2/user-guide/docker-plugins/  , here we also see d docker plugins installatin command, if u dnt choose d version it wil
so when i hv my container mount point : /data/db/, i can mount it with an external vol 
 we can create a mount point , frm this point, we could have external mounting , we can mount it externally
 we can also hv multiple mount points

STEP1)
now we deploy this pluging :
docker plugin install rexray/ebs \
  EBS_ACCESSKEY=AKIAUGAKLPXTXI23EAXH \
  EBS_SECRETKEY=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


STEP2)    
 Authentication & authorization    ;#this process cannot happen if u dnt hv the right IAM authorization
EBS_ACCESSKEY=AKIAUGAKLPXTXI23EAXH \
  EBS_SECRETKEY=/1

step3)
am creatin a vol called ebs33
docker volume create -d rexray/ebs ebs33 .. ...........d is for driver, the driver is rexray/ebs 
if u dnt pass driver, it wil create it under local driver 
docker volume ls
   ******he tried to run multiple volume mount point but it didnt work 
******************* he said it seems the vol property does nt permit multiple mount point

****Also he tried to deploy the container with the vol but didnt work, we couldnt access the data 

3) ############# so he used the file app.sh  and paste the below command 
docker volume craete ebs3 -d rexray/ebs
docker run --name mongo -d --network tesla -v ebs3:/data/db -e MONGO_INITDB_ROOT_PASSWORD=devdb@123 -e MONGO_INITDB_ROOT_USERNAME=devdb mongo   (mongo is image name )
then  deploy: sh app.sh

************* successful,  he checked in aws and we can see the  volume we see both ebs3 and ebs33 that we created n we can access the application n write data
docker volume ls and we see ebs33 n ebs3


DOCKER COMPOSE
==============
Docker Compose is a tool for defining/declaring and running multiple containerised micro-services applications.
***docker10*** and docekr compose file is a .yml file; docker-compose .yml

                               TWO WAYS TO DEPLOY IN DOCKER
1) We can deploy the database and the aplication either using command ...... OR 
docker run --name mongo -d --network tesla \        
-v ebs3:/data/db -e MONGO_INITDB_ROOT_PASSWORD=devdb@123 \                                                                        
-e MONGO_INITDB_ROOT_USERNAME=devdb mongo

docker run --name springapp -d -p 8080:8080 --network tesla \
-e MONGO_DB_HOSTNAME=mongo -e MONGO_DB_USERNAME=devdb \
-e MONGO_DB_PASSWORD=devdb@123  mylandmarktech/sping-boot-mongo

2) using Decompose' ie using 'a file' ..1:31:18
***docker10*** and docekr compose file is a .yml file; docker-compose .yml
docker-compose.yml 
version: '3.1'             
services: 
  springapp:
    image: mylandmarktech/sping-boot-mongo   
    restart: always



In real time one application can have more than 7 micro-services:
E.G ebay.com: 
An e-commerce java based web application for ebay or ebay-web-application:
docker run --name login -d -p 80:8080 --network ebay login          
docker run --name reg -d -p 80:8080 --network ebay registration 
docker run --name cart -d -p 80:8080 --network ebay cart
docker run --name checkout -d -p 80:8080 --network ebay checkout
docker run --name pay -d -p 80:8080 --network ebay payment 
docker run --name order -d -p 80:8080 --network ebay order 
docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-admin -d --network ebay mysql  

******so we ar deploying this database: Mysql,  bt we hv to put it in the same ntwrk 'ebay',
if u hv an aplication like dis n u nid to deploy it, hw many run command are we going to execute , very long command,we ar going to v 
multiple run command:   **i bliv therfore we use a 'docker compose file'

******* in trying to deploy this application what are the objects that ar ivolved ? 

docker objects:
  - u must use the right images   , each of the micro services must be deployed with the right image 
  - it must be in the right networks ,  so if we nid this containers to comm with each oda they must be in the same ntwrk
  - must assign volumes accordingly ,  if any of the containers nids to maintain its state we must introduce docker volume
  - environmental variables   ,   for us to deploy it in any env , we must use env variable 


                               INSTALL DOCKER COMPOSE
ubuntu@docker:~$ docker-compose
Command 'docker-compose' not found, but can be installed with:
sudo snap install docker          # version 20.10.24, or
sudo snap install docker          # version 20.10.24
sudo apt  install docker-compose  # version 1.29.2-1
See 'snap info <snapname>' for additional versions.

 ubuntu@docker:~$ sudo apt  install docker-compose -y      ... successful


since we are using docker compose, we can deploy in multiple env, like dev, uat, prod using diff env variables

             dev     uat      prod   
USERNAME   devdb     uatdb    proddb   
hostname   mongo     mymongo  mongodb  
password   dev@123   uat@123  admib@#123  


                                        HOW THE DOCKER COMPOSE VERSION HAS AN IMPACT

click on the menu box , select doc, click on references , compose cli, compose file references and legacy versions then version 3
when we look at docker compose, u want to look at the  docker compose like version3 references and hw it relates to the docker engine with the docker software


Docker version 20.10.12  .. this is my docker version
docker-compose.yml  

we can use version 3.1 for docker engine  1.                                   ..... 1:26:44
what dis means is that  ...       Docker version 20.10.12  .. this is my docker version and this is the docker compose version 1.29.2
 so if i wnat to write a docker compose file, this is hw it goes 
------------------      ####################### if ur trying to deploy ur application using docker compose it cud fall under stuff like dis 1:27:40



                                       CREATING DOCKER COMPOSE FILE
1) Creating the docker compose file
   .....the rule says when u wnat to write a docker compose file, that file starts with version
                 the default name for docker compose file is, ie docker compose.yml


  THIS IA A .yml file                                                                       1:33:00
so in deployn dis spring app, i nid d image, smt like restart,if d container stops wot hapens, i nid ports, its a list i pair port 6000 to 8080, ntwrks too is a lis, env var
                             for d db service , i nid the image, retsrt if it stops, ntwrks, vol, env.
                                WE AR CREATING OUR VOLUMes, netwrks and services




version: '3.1'             
services: 
  springapp:
    image: mylandmarktech/sping-boot-mongo   
    restart: always    
    ports:
    - 6000:8080     
    networks:
    - fintech     
    environment:
    - MONGO_DB_HOSTNAME=mongodb
    - MONGO_DB_USERNAME=proddb
    - MONGO_DB_PASSWORD=proddb@123         
  mongodb:
    image: mongo     
    restart: always     
                       (no space here, i jst did it like that) under port, we wnt be allocating any port for our db bc we wont be accessing it externally so i tk it out 
    networks:       ...... ..... ..... you cud hv multiple nterk cos a container cud be multiple ntwrk
    - fintech  k9                     ***** d 2application shud b in the same ntwrk bt i can place database in anoda ntwrk bc one container can b in more than one ntwrk)
    - tesla   
    volumes:
    - data11:/data/db:rw         *********** what is rw 1:40:43
    environment:
    - MONGO_INITDB_ROOT_USERNAME=proddb
    - MONGO_INITDB_ROOT_PASSWORD=proddb@123       
volumes:                                           i want the vol created, it doesnt exist, i want it craeted extrnally
  data11:
    driver: rexray/ebs     c
networks:
  tesla: 
    external: true    ...    true means the ntwrk is already available
  fintech:
    driver: bridge  

2)docker volume create -d rexray/ebs data11      ********************** we hv to also create dis volume and ntwrk fintech b4 we can start deploying the apl
3)docker network create fintech  
  ****************************  he didnt run 2&3 .... .. he deployed everytin with the file
1:38:20 ################################  you can use a file like this its easy bc this transaction can be easily repeated , now the docker compose.yml file is done
                                                                    we can deploy the containers ie springapp and mongo using dockr compose  .. 1:41;19
4) next
vi docker-compose.yml  and paste the file 


**************** nxt
5) check if the syntax are ok
docker-compose config      ................if ders an errror, it will tell you

6) create the containers/DEPLOY THE APPLICATION
docker-compose up -d 
       docker ps .... the containers are running
  
....... application deploy bt we cudnt acces it in goolge bt when we curl on the comandline it responds, he asaid ders no path to the appl, its suppose to
                                                              respond accordingly .......1:45:30 .... he even checked aws security groups, said all d ports are open




                 ***************************CREATING CONTAINERS USING DOCKER COMPOSE/ to DEPLOY THE APPLICATION  ***********************1:42:08

TO create the containers/DEPLOY THE APPLICATION

first: check if the syntax are ok
docker-compose config      ................if ders an errror, it will tell you

1) for docker compose file with default name [docker-compose.yml ]. 
  # Create Services/Contianers
   docker-compose up -d          ****** used when deploying an application using the default name, ie docker compose.yml

 
2) #docker compose file with custom name.  
docker-compose -f <CustomeComposeFileName>.yml <command>
Ex:
docker-compose -f docker-compose-springapp.yml config
docker-compose -f docker-compose-springapp.yml up -d
docker-compose -f docker-compose-springapp.yml down


# Remove Services/Contianers  .................  to destroy the containers
    docker-compose down



so we can create a docker compose file and call it: docker-compose-springapp.yml
i can also rename my docker compose file ie the file we deployed already  .... mv  docker compose.yml docker-compose-springapp.yml

then:  he changed port to 8080:8080 and dreiver:local
                              
version: '3.1'             
services: 
  springapp:
    image: mylandmarktech/sping-boot-mongo   
    restart: always    
    ports:
    - 8080:8080     
    networks:
    - fintech     
    environment:
    - MONGO_DB_HOSTNAME=mongodb
    - MONGO_DB_USERNAME=proddb
    - MONGO_DB_PASSWORD=proddb@123         
  mongodb:
    image: mongo     
    restart: always     
    networks:       
    - fintech                  
    volumes:
    - data11:/data/db
    environment:
    - MONGO_INITDB_ROOT_USERNAME=proddb
    - MONGO_INITDB_ROOT_PASSWORD=proddb@123       
volumes:                                           
  data11
    driver: local
networks:
  tesla: 
    external: true    ...    true means the ntwrk is already available
  fintech:
    driver: bridge  

then 
docker-compose -f docker-compose-springapp.yml config
with dis new file name to deploy will be  ........................... 1:53:20
docker-compose -f docker-compose-springapp.yml up -d

########################## now we were able to access the application online



DOCKER 10
dnt freak out bc this code is part of our project already, if u go to the project that we v for this stuff we already v d code ther
                  is oin our github repository, this docker compose file is already ther, most of the time it wil b part of ur source code bt u nid to understand if u nid 
               to change anytin like the image version and so on 

so docker compose is very imp, its easily manageable to use in deploying ur application and commands ############
e.g if u go to our github account we still v dis file, eveytin is der as it was originally defined, if u want to change smt u jsut go and change the version

*** in real time we ar going to be using dodcker compose file to do our deplyment 

******* 1:15:08  .....******************** u can deploy anytin using docker compose


EVEN THough we v been able to deploy our application in docker using vol, netwrk, env var n deploying multiple containers, we v realised that it is very easy fro us 
to put all of those containers in a particular file called docker compose file BUT the question is can i scale my application when i use docker? ,, w ecannot do that n
this is bc docker does nt support  multi hosting , a multi host netwrk architecture, n so we nid to see hw we can use the option that support multi hosting bc we wnt a
situation were we can hv severs in a cluster n for us to that we will look at docker swamp 
""""""""#####################################################:::' READUP POWERPOINT""""""""""\|||||#########################################


DOCKER SWAMP

THE TRADITIONAL MODEL;
traditional docker is running with independent servers n each of the independent docker machine v their docker cli where we run d diff docker commands Bt if u v a docker
server like this that goes down then that means ur applications are down, so ders no option to resolve such issue so what are we going to do ?
we v a swamp , a swamp is going to be a cluster

A swamp is a cluster that comprises of master node and worker node 1:20:00
so far we v been using docker as our containerization tool bt we realized that whan we craete containers, wecannot scale,  e.g w ev sooo much traffic, that traffic we
cannot scale n dats why we talk abt docker swamp
Docker swamp is simply a container oschestrator
so we v docker swap , kuberDockernetes , openshift ..all of these are ochestrators
**********************************FEATURES ............................ readup
SWAMP FEATURES
1)Cluster mgt : here we v
masternodes + worker nodes... ..  here we can v like 3 masters and 3 workers in one swamp,, so all these workers are together in one cluster n onces we put dem together,
doskser runs on all of them, what tk place is dat u can deploy ur app n it is goin to be scahedules in any of thses nodes,






*******************************************************************************





DNT PRINT


DoKCER 5&5B      **** lol**** THIS is a wrong interpretation i did in 2024


**************** this is wher he now applied this command 
##########copy from a container:  
copy from a container:
docker cp webapp:/usr/local/tomcat/webapps/tesla/jsps/home.jsp . 
docker cp webapp:/usr/local/tomcat/webapps .

docker exec webapp ls webapps/tesla/jsps
home.jsp
i wnt to copy this file called home.jsp n modify smt 
docker exce webapp ls webapps/tesla/jsps/home.jsp
now ders a command called docker cp
we wnt to copy the filw called home.jsp to the container called webapp
this relative path didnt wrk :.   docker cp webapp webapps/tesla/jsps/home.jsp .   ,, i bliv webapps is wher deployment takes place in tomcat
 WE run decker exec webapp pwd ,, we can see,  /usr/local/tomcat   ,   wich is the absolute path

1:40;34 **** so we use the absolute path ,, here we copied the webapps dir to the container webapp
docker cp webapp:/usr/local/tomcat/webapps .
and it says successfully copied to /home/ubuntu/.   ,, we wre unable to copy bc we wre using the relative path
ls n we can see /webapps  ...... i bliv this was copied to the pwd ,, i understand bt i can still ask ??????????????????????????????????????????????????????
now if i wnt to copy that exact file that is inside webapps (/tesla/jsps/home.jsp)
  docker cp webapp:/usr/local/tomcat/webapps/tesla/jsps/home.jsp . 
successfully copied , we can see /home.jsp ,,,, i bliv this was copied to the pwd ,, i understand bt i can still ask ??????????????????????????????????????????????????????
COPIED TO  /home/ubuntu/ .
#######  vi /home.jsp    , to change one line
we change the line but when we check on the browser , we stil cant seee the change
we wnt the change to b part of the container so i v to copy file to my container 

*******1:45:28 copy file to a container:
docker cp home.jsp webapp:/usr/local/tomcat/webapps/tesla/jsps/ 
now we v successfully copied it to the webapp conatiner 
it shows :COPIED TO webapp:/usr/local/tomcat/webapps/tesla/jsps/ 
and when we check in the browser we can see the line that we changed





