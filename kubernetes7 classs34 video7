

Deployment is making the application available in pods/containers
by using different kubernetes objects:  
  Pods or    
  ReplicationControllers / 
  ReplicaSets/ 
  daemonsetS
  StatefulSets / 
  deployments

ServiceDicovery is exposing the applications to endUsers 
by using Kubernetes services such as:  
  ClusterIP / 
  NodePort  / 
  LoadBalancers / 
  HeadLess Service  
  ExternalName
We can also use kubernetes add-ons/plugins to expose 
applications such as:  
   ingress    ( as well as netwrk policy)
   serviceMesh & Istio     


11:22
this is like a multi tier appl
this is my aplli,runing in my cluster, webapp pod deployed in the cluster, i also have a database(mysql) pod deployed both runing on node5 n node 7.. its a multi tier appl
such that end users need to acces the appl, this is a k8 cluster bt end user trafic cant go straight to the pod, so bc its an external access, we v diff typ of services
to mk the appl discovery (endusers can be external or internal) ,  for external user w euse the nodeport service such that when trafic gets to the cluster frm enduser, it 
gets to the node so we v to use a nodeIP, a webSVC , To accept extrnal traffic, therfore when the traffic gets to the node , we v kubeproxy&kubelete running in the node 
these are all services that ae runin in the node,they are diff agents in my node,we also v coontainer runtime, container dp  ... bt let me focus on this guy for now bc i 
want to talk abt smt imp, netwroking ,,,... which of these agents is responsible for netwroking, its kubeproxy
kubeproxy is responsible for networking , so when traffic comes into any of the node , kubeproxy identifies that this trafffic from xternal users, (cos when we create dis 
nodePort service we assign a pod number, and a clusterip as well, we nid the nodeport service for trafic to get into the cluster   ) once it is inside the cluster, service 
will be routed frm the nodeport service to the the webapp .
also he web appl need sto talk to the datbase pod, (rmember that the node service is doin lb), its routing trffic to all the pods , 
now webapp nid to talk to db pod , bc it neeed sto write data, the webapp is the enduser of the database appl, it needs to write data in the database,  how will that happen?
evn thouh they ar in the smae cluster they cant talk to each other like that,, we need another service , so we create another service for database
because it is internal communication, we create a clusterIP service, so our database is clusterIP, SO for webapp to talk to database(pod), it wil be via the database service
and database service wil talk to the database pod.
19:44 ...let us assume  we v anoda database that is out of our cluster , we wnta situation wher this appli shud persist data, therfor it shud hv multiple replica of data
such that my webapp need to also write data in the external database outsid emy cluster , so i have anoda database msql,outside my cluster, how wil this work???
for this to work, we will create an externalName service (externalName SVC) in our cluster .. THERFORE frm this ectrnal service am creating a service for the external msql
so for my webapp to talk to the external msql, my webapp wil talk to the external service contrling the database once the communication is established, frm my webapp this 
communication is being established, therfore from this external service we can communicate with the external database msql.

22:28   .......INTERVIEW QUSTION
IS IT POSSIBLE TO HV APPL OUT of YOUR CLUSTER COMMUNICATING WITH APPLications INSIDE YOUR CLuster
*************so YES, it can be exposed via the externalName SVC, its very possible to have appl outside communiacting with appl insdie the cluster

23:09   LETS TRY TO DEPLOY ONE OF OUR LEGACY APPL in github
remember, one of our legacy appl has a github repository,  as a matter of fact all our projects have a github repo
the application with the database, requiring the database ,, if you type docker, it is going to appear, spring boot docker, here is our legacy appl

in our env,
We manage java based projects written in java programming 
language by our team Developers. 
We also use the spring-boot framework in Developement.  

so w eare using a framwrk called springboot framewrk , so developers are deveeloping this appl using the springboot framewrok, thats why its called spring boot docker bc of
the framewrk used in developin this project.


its a spring appl, a spring java appl,  or a javabased appl developed via the spring boot framewrk
we need to deploy this appl in k8
this is an example of a microservice appl, when uhv many appl that you need to deploy and those appl has to comm with each other 
so we wnt to see hw to manage this appl with relative ease

************this project has 2micro services, w ehv the springapp and mongo database (mongodb)..........27:38


spring:
  data:
    mongodb:
      host: ${MONGO_DB_HOSTNAME}
      port: 27017
      username: ${MONGO_DB_USERNAME}
      password: ${MONGO_DB_PASSWORD}
      database: users
      authentication-database: admin
server:
  port: 8080

This comes with 2 microserivces (springapp and mongodb)   
-use deployment kubernetes object for springapp deployment  
-use replicaset kubernetes object for mongodb deployment
-springapp is stateless while mongodb is stateful  , therfore 
-We can also use StatefulSets with HeadLess service for mongodb bt for now we will use replica set and cm bk to this later on.
-write deployment manifest files for springapp and mongodb  , we can aslo use existing manifest file , most of the time u ar goin to use existin manifest file, if i alrady 
hv a manifest file exitsing then i shudnt write a new one..... i can also use an IDE to simplify hw to write my manifest file.if u hv vscode, i type deployment n delete 
the lines i dnt need, since it is stateless i dnt need volume so ill tk it out., am nt assignin resources for now.


=============================================================     .... we want to deploy a legacy appl
springapp.yml 
============
apiVersion: apps/v1  
kind: Deployment
metadata:
  name:  springapp
  labels:
    name:  spa     
spec:
  selector: 
    matchLabels:
      name: spring  
  replicas: 2
  template:
    metadata:
      labels:
        name:  spring
    spec:
      containers:
      - image:  mylandmarktech/spring-boot-mongo  
        name:  springapp
        env:
        - name:  MONGO_DB_HOSTNAME
          value:  mongosvc              *********** form communication btw the databse and the the sprinapp the hostname has to be the same as the database servicename 
        - name:  MONGO_DB_USERNAME
          value:  devdb
        - name:  MONGO_DB_PASSWORD
          value:  devdb@123
        ports:
        - containerPort:  8080


      host: ${MONGO_DB_HOSTNAME}
      port: 27017
      username: ${MONGO_DB_USERNAME}  ... this is nt hardcoded
      password: ${MONGO_DB_PASSWORD}   ... this is nt hardcoded

apiVersion: apps/v1  
kind: ReplicaSet  
metadata:
  name:  mongors     
spec:
  selector: 
    matchLabels:
      name: mongo    
  template:
    metadata:
      labels:
        app:  mongo  
    spec:
      containers:
      - name:  mongoapp    
        image:  mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME             ....hw do i knw the env variable for this image?, where the image is found in docker hub, so i can go to dockerhub
          value:  devdb                                                                          and search for mongo
        - name:  MONGO_INITDB_ROOT_PASSWORD
         value:  devdb@123
                                      ********************ther wil b no container port, am nt passing any container port,,...44:15
---
kind: Service       ****************if i dnt define my service type, the default service :clusterip will be created ....51:40
apiVersion: v1  
metadata:
  name: mongosvc      ..to expose appl in k8 we use the database SERVICE NAME...........  48:20
spec:
  selector:
    app: mongo    
  ports:
  - port: 80  
    targetPort: 27017      ***********i cud also check thsi info frm dcokerhub,HOW TO USE THE IMAGE, if ders any port that its expose on, he checked n we see
                           the mongoDB server in the image listens on the standard amaongoDB port, 27017
---
springapp ----mongosvc---- mongodb  



i was goin to share a video with you guys explaining the concept of volume 
bc when we are deployin  a stateless appl like mongo or a database then the discussion around vol is very imp .

QUESTION
1:08:22 .... i keep seeing azure devops is it a replacement for aws?  ie instead of aws, u use azure?
thers also AZURE devops ,that u can use a pipe line in azure cloud  in the place of jenkins.. we are goin to look at that in the bootcamp

1:06:50
2) each time we are gettin an image k8 knws wher to get the image frm but we can hv images in ERC, nexus or some other place,, so is the image hardwire into k8 and if u are 
getting  an image fmr other place other than dockerhub, hw does the k8 knw wher to find thatimage .
ANS; 
the  fact is if it is a public registry once  you type the image url, its goin to be indicating wher that image is found, so most time when it is docker hub its easy , its 
just the name bt when it come stosomewher else, it wil giv eyou th entire image url, like wher it is found online, so u copy it and put it in that particular image spot .




running notes for kubernetes 8/9 video follows:
===============================================
app.yml  
=======
host

destroy options :
  terraform destroy --auto-approve  
  terraform destroy --target local_file.test      
  terraform destroy --target aws_instance.web         
How have you applied terraform in your environment/landmark??? 


output block:
   create ec2-instances :
      metadata = 
        ipAddresses
        az/ami/dns/
        Instance ID
       
   vpc : vpc-id  / cidr block   
   eks cluster:
     kubeconfig    

kubernetes
=========
  -- Kubernetes CLUSTER = Group of servers working together - k8s  
  -- k8s -- nodes -- pods[containers]  
pods for deployment:
  -- pods has short lifecycle      

kubernetes features:
   - scaling  
   - self healing capabilities  - dead pods are authomatically restarted 
   - service discovery & load balancing  :
         - ClusterIP / NodePort  / LoadBalancer  / ExternalName / HeadLess Service
         - ingress  

kubernetes objects:
   - pods[v1]  = we shouldn't use pod to deploy applications in kubernetes
                 use controllerManagers to deploy and manage pods/applications 
   - ReplicationControllers[v1]
   - ReplicaSets [apps/v1] 
   - DaemonSets  [apps/v1] 
   - Deployments [apps/v1] 

  selector supports:
   ReplicationControllers - old version = equality based selectors 
   ReplicaSets/DaemonSets/Deployments - selectors:
        - equality based selectors = key1:value1
        - set based selectors:
             key1:
               values:
                 value1  
                 value2  \
                 value3  

  kubectl scale deployment/rc/rs --replicas 10   = manual scaling   
  kubectl scale ds --replicas 10   = manual scaling   

IQ: How can scaling be automated in kubernetes? 
    By installing a guage API [ metric-server ] in our cluster and using 
       1. HorizontalPodAutoscaler[HPA] for pods   
       2. VerticalPodAutoscaler[VPA] for pods 
       3. CUSTER Autoscaler[CAS] for nodes  

metrics-server    
Resources / Limits / Requests / ResourceLimits  
==========================================================
https://github.com/LandmakTechnology/spring-boot-docker
-# Spring App & Mongod DB as POD without volumes
Stateless applications deployment doesn't require to maintain it state   :
  Use deployment as the choice kubernetes object  
  ReplicaSets/ReplicationController:  
Stateful applications maintained their state. Examples are  :
  databases, Jenkins, 

spring:
  data:
    mongodb:
      host: ${MONGO_DB_HOSTNAME}
      port: 27017
      username: ${MONGO_DB_USERNAME}
      password: ${MONGO_DB_PASSWORD}
      database: users
      authentication-database: admin
server:
  port: 8080

   ssh username@hostname [ It will request for the password ]  
    mongodb:
      host: mongo
      port: 27017
      username: devdb  
      password: devdb123  
  ssh devdb@mongo  

mongo database:
===============
  1. use StatefulSets as the choice kubernetes object for Stateful  
     application deployments   
  2. USE ReplicaSets/ReplicationController/deployment StateLESS  
     application deployments   

Stateless
--- springapp.yml/
Stateful
--- mongo.yml/

springapp.yml: kams   
kind: Deployment  
apiVersion: apps/v1   
metadata:
   name: spingapp    
spec:
   replicas: 2  
   selector:
     matchLabels:
       app: spring    
   template:  
      metadata:
         labels:
            app: springapp    
      spec:  
         containers:
         - name: app  
           image: mylandmarktech/spring-boot-mongo  
           ports: 
           - containerPort: 8080
           env:
           - name: MONGO_DB_HOSTNAME  
             value: mongosvc  
           - name: MONGO_DB_USERNAME 
             value: devdb   
           - name: MONGO_DB_PASSWORD
             value: devdb@123   
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080

mongo.yml  = kams:  
=========
kind: ReplicaSet   
apiVersion: apps/v1  
metadata:  
  name: mongors    
spec:
  selector:
    matchLabels:
      app: mongo     
  template:
    metadata:  
      labels:
        app: mongo    
    spec: 
      containers:
      - name: mongodbcontainer    
        image: mongo      
        ports:
        - containerPort: 27017    
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD 
          value: devdb@123
---
kind: Service 
apiVersion:  v1  
metadata:  
   name: mongosvc 
spec:
  selector:
    app: mongo    
  ports:
  - port: 27017 
    targetPort: 27017  

---
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
           env:
           - name: MONGO_DB_HOSTNAME  
             value: mongosvc  
           - name: MONGO_DB_USERNAME 
             value: devdb   
           - name: MONGO_DB_PASSWORD
             value: devdb@123   

1. image: mongo   
2. image: mylandmarktech/spring-boot-mongo     

repository to clone:
1. https://github.com/LandmakTechnology/kubernetes-notes
2. https://github.com/LandmakTechnology/kops-k8s
3. https://github.com/LandmakTechnology/kubernetes-manifests
=========================================================

kubernetes volumes:
===================
mongovol.yml  
kind: ReplicaSet   
apiVersion: apps/v1  
metadata:  
  name: mongors    
spec:
  selector:
    matchLabels:
      app: mongo     
  template:
    metadata:  
      labels:
        app: mongo    
    spec: 
      volumes:
      - name: mongodbhostvol    
        hostPath:
          path: /mongodata  
      containers:
      - name: mongodbcontainer    
        image: mongo      
        ports:
        - containerPort: 27017    
        volumeMounts:
        - name: mongodbhostvol  
          mountPath: /data/db 
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD 
          value: devdb@123

kubectl taint nodes node9 key1=value1:NoSchedule     [taint the node]
kubectl taint nodes node9 key1=value1:NoSchedule-    [untaint the node]

  hostPath volume will store data ONLY on the node where the POD is scheduled
  This can result data lost and data inconsistency 

  NFS 

      volumes:
      - name: mongodbhostvol    
        hostPath:
          path: /mongodata 
      volumes:
      - name: mongodbhostvol    
        nfs:
          path: /mongodata 


awsElasticBlockStore 
azureDisk
azureFile 
configMap 
emptyDir g
cePersistentDisk
gitRepo (deprecated) 
hostPath
nfs 
persistentVolumeClaim 
secret


Configuration of NFS Server
===========================
Step 1: 

Create one Server for NFS

Install NFS Kernel Server in 
Before installing the NFS Kernel server, we need to update our system’s 
repository index with that of the Internet through the following apt command as sudo:

nfs server: 3.17.179.30  / 10.0.0.76
---------------------
mongo-nfs.yml  
==================
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongors
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      name: myapp
      labels:
        app: mongo
    spec:
      volumes:
      - name: mongodbvol
        nfs:
          server: 10.0.0.76
          path: /mnt/share
      containers:
      - name: mongodbcontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
        volumeMounts:
        - name: mongodbvol
          mountPath: /data/db


kubectl taint nodes node9 key1=value1:NoSchedule     [taint the node]
kubectl taint nodes node9 key1=value1:NoSchedule-    [untaint the node]

Persistent volumes 
===================

deploy a kubernetes using Kops  


PV --> It's a piece of storage (hostPath, nfs,ebs,azurefile,azuredisk) in k8s cluster. 
PV exists independently from from pod life cycle form which it is consuming.

PersistentVolumeClaim -->
   It's request for storage(Volume).Using PVC we can request(Specify) 
   how much storage u need and with what access mode u need.

Persistent Volumes are provisioned in two ways, Statically or Dynamically.:

1) Static Volumes (Manual Provisionging)
    A k8's Administrator can create a PV manually so that pv's can be available for PODS which requires.
    Create a PVC so that PVC will be attached PV. We can use PVC with PODS to get an access to PV. 

2) Dynamic Volumes (Dynamic Provisioning)
     It's possible to have k8s provision(Create) volumes(PV) as required. 
     Provided we have configured A storageClass [sc].
     So when we create PVC if PV is not available Storage Class will Create PV dynamically.        

PVC: If pod requires access to storage(PV),it will get an access using PVC.
     PVC will be attached to PV.

PersistentVolume – the low level representation of a storage volume.
PersistentVolumeClaim – the binding between a Pod and PersistentVolume.
Pod – a running container that will consume a PersistentVolume.
StorageClass – allows for dynamic provisioning of PersistentVolumes.

PV Will have Access Modes:
============================
ReadWriteOnce=RWO – the volume can be mounted as read-write by a single node = EBS 
ReadOnlyMany=ROM  – the volume can be mounted read-only by many nodes
ReadWriteMany=RWM – the volume can be mounted as read-write by many nodes = NFS 


Claim Policies
================
A Persistent Volume Claim can have several different claim policies associated with it including
Retain – When the claim(PVC) is deleted, the volume(PV) will exists.
Recycle – When the claim is deleted the volume remains but in a state where the data can be manually recovered.
Delete – The persistent volume is deleted when the claim is deleted.

The claim policy (associated at the PV and not the PVC) is responsible for what happens to the data when the claim is deleted.

mongo-pv-hostpath.yml   
---------------------
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/kube"

---
# Mongo db pod with PVC
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
    path: "/kube"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-hostpath
spec:
  storageClassName: manual
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
---
# Mongo db pod with PVC
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongors
spec:
  selector:
    matchLabels:
      app: mongo
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongo
     spec:
       volumes:
       - name: mongodb-pvc
         persistentVolumeClaim:
           claimName: pvc-hostpath
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: mongodb-pvc
           mountPath: /data/db

mongo-pv-pvc-nfs.yml   
===================
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs-pv1
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: <nfs server ip>
    path: "/mnt/share"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nfs-pv1
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Mi
---
pv-pvc-mongo-hostpath.yml  
=========================
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
    path: "/kube"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-hostpath
spec:
  storageClassName: manual
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
---
pv-pvc-hostpath.yml   
==================
# Mongo db pod with PVC
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs-pv1
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 10.0.0.76
    path: "/mnt/share"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nfs-pv1
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Mi
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongors
spec:
  selector:
    matchLabels:
      app: mongo
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongo
     spec:
       volumes:
       - name: mongodb-pvc
         persistentVolumeClaim:
           claimName: pvc-nfs-pv1
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: mongodb-pvc
           mountPath: /data/db

kubernetes volumes  :
==========================
