
1. Terraform   -- Provisioning
-------------------------------
1) Infrastructure as Code (IaC)
o Understand Problems with Traditional way of Managing Infrastructure
    - GUI/console   
    - commands  
o How IaC with Terraform Solves them

o Infrastructure as Code (IaC) VS Configuration Management
o Install Tools on Mac OS, Linux OS and Windows OS
o Command Basics
    terraform init
    terraform plan  
    terraform validate   
    terraform apply   
    terraform destroy   
    terraform import   
    terraform fmt 
    terraform show/ cat terraformstate.tf    
       remote backend like s3 
       simon / 
       mary / 
       kelvin     


o HCL Language Syntax

2) Terraform Top Level Blocks
o Fundamental Blocks
o terraform block
o providers block
     aws / azure / github / local  
o resources block
o Variable Blocks
o Input Variables
o Output Values
o Local Values
o Referencing Block
o Data sources Block
o Modules Block

 AWS:
   ec2-instances  \ VPC / EKS  / S3  / ETC.  
   installing terraform on windows and linux systems 
    define scope / 
    init / 
    writing & modifying terraform scripts [ vars.tf main.tf modules]   


2. Ansible  is a Configuration Management tool  

    appServers = 50  
    webservers = 40  
    dbservers  = 44 
    kubernetes = 5  

    deployment of applications in the 50 appServers 
    commissioning the 50 appServers 
    securing the  50 appServers
    installing tomcat in the 50 appServers
  tasks that ansible can perform/run on its hosts:
    FileMGT  
    userMGT  
    deployment  
    securityMGT  
    system monitoring  
    patching  
    packageMGT  
 How to perform the tasks and ansible workflow:
   modules 
   playbooks:
     variables  
     plays   
     tasks  
     handlers   
     modules [ yum / copy / service / template / apt / package / shell ] 
               command / setup / systemd / 
     roles    
   roles    
   inventory/host file  
     [ appServer ]
       10.10.0.55
       10.10.0.91
     [ dbServer ]
       10.10.0.77
       10.10.0.100

     [ k8s ]
       10.10.0.10
       10.10.0.30

app.yml  
=======
host

destroy options :
  terraform destroy --auto-approve  
  terraform destroy --target local_file.test      
  terraform destroy --target aws_instance.web         
How have you applied terraform in your environment/landmark??? 


output block:
   create ec2-instances :
      metadata = 
        ipAddresses
        az/ami/dns/
        Instance ID
       
   vpc : vpc-id  / cidr block   
   eks cluster:
     kubeconfig    

                                   ##########  STARTED HERE ########################
*********************************************************************************************************************************************************
So we've been looking at k8 And one of the things that we discuss about k8, we realize that k8 is an orchestration.Is an open source orchestration too.
That's k8, open source, or ochectration  tool which is used to orchestrate containerized applications.
So when applications are containerized using order tools like docker, we use k8 to orchestrate the containers, we use k8 to manage the containers.
So we also realize that k8 is not a replacement for docker.Okay, because we need, we stil need docker to containerize our application.
But k8 could  be a replacement for order orchestrates, like Doctor Swan.
Now, we saw the architecture of k8, and we realize that when we talk about k8, we are talking about a system that has, you know, it has control plane, and work on node, that
is the architecture.So , you know, there is a server that is controlling other, okay, servers, and we have the workeer   node where the applications are running.
Now we went ahead to discuss how do we deploy work load in  k8.How is work load deploy?
So we realize that generally, when we talk about k8 and work load, okay, is a very sensitive aspect to take note of when we talk about k8 and workload.
So E.G., we are managing containers, so containerized applications are deployed in k8 . we are deploying containerized application inside our k8 cluster
And for that to happen, We saw stuff where .We have Worker node. And, you know, that is our application are deployed in import.That's where they are deployed.
But now, should we just deploy our application directly in a port?The answer is no. So k8. So in k8, we have a k8 cluster, And this is a group of one group of 
savers working together Okay, now we have a cluster, okay. And this cluster, we can can call it, k8s. Now, inside the inside the cluster, what do we have? We have nodes.
Inside the notes, we have pods.And inside the pots, we have one containers.So this is the diagram. The cluster is made up of a group of note.
The node is housing pods, and the pod is housing containers.
So now you know what is the smallest building block of k8 when it comes to objects and k8? What is the smallest building block? The smallest building block for k8 are pods?
that is the smallest building block they are  pods.Because our continentalized application are okay. Deployed in pods,bt  we  notice that if we deploy our application in
pods, there's going to be a problem.why? How would that problem occur, e.g., the life cycle of a pod, okay, port.Has a short life cycle i. So the life cycle of the pod is 
short.Now, the second thing about the pod is that, if a pot dies, okay, the pot can not be restarted.So the pot does not benefit from k8 Features. What are some advantages 
of k8?Some of the features of k8? What are some of the features of k8, k8 features that we saw?We realize that k8 has self healing.
What that means is that now, in k8, okay, now, dead port, dead ports are automatically.It's automatic, automatically restarted But this will Not happen if we're going to use PODS 
for deployment here.So if we use  pods for deployment now,there is a problem.pod has a short life cycle now, pod does not benefit from the entire features of k8.And we also
saw that in k8, there is load balancing.Okay, and load balancing comes with service discovery, service discovery.Now, services discovery n load balancing.
We can easily use a service to discover our applications that are running in the cluster.Now, by virtue of that, when we talk about service discovery and
load balancing, what are some of the examples of services to take note of?  .. 6:24

EXAMPLES OF SERVICE DISCOVERY

1)CLUSTER IP  2)  noodeport  service  3)loadbalancer  service  4)External name  
5)headless service

these Are the different services in communities.

So with these, we can use any of these  to discover our application.
Therefore, we saw again, this diagram, where, with the help of a service, am I able to access, you know,access some  pods that are deployed in  my cluster.
Look at this diagram,  Look at this service. I have a cluster I-P service here okay now Let's.
Assume this my web service for my web application is able to route traffic and load balance traffic to all of this pods, to the group of pods so this service acts as a 
single DNS resolver.(domain name service) .. 8:00
What that means is that, now, if I want to access this pod, I want to call, okay, like, this pod will have, like, maybe Web port one.
Did you see that? Web port one?Now observe something This web pod one.,This pod two.Okay. Now if you want to access all of this pod, web, pod three., web pod4
Now do I need to be calling Web for one Web, web port2,No, I-I don't have to do that so I can use one name to call all of this pod,
so wish Name would be using i will use, the name of the service.So, because my servicename is, is constant.So DNS domain name is resolver.So  it, it is going to achieve 
domain name resolution.Okay, to all of the set of parts that are linked to this service now.So how do we call this pods in  k8 that are linked to a service?
We call them What end points? Okay, so this service has How many end point? How many end points? How many end points are there now? They are four and point.
Now, for my scale, if need be can I scale?  if there is need to scale, am I able to scale to have maybe more pod, Can the number of end point increase?The answer is yes.
The number of end point, okay, can increase.and once it increases
The same service is going to be achieving DNS resolution. The same service because that's what services are all about It's good to be achieving DNS resolution, .

If need be? Can we have more end point?, yes, if need be.
We can scale to how many replicas , right now 6
Okay. So one service, one service is routing traffic, to how many pods of the application six pods at the moment .  thatâ€™s one service.
This is what is going to be happening. So this is the feature of k8, as we have seen, a cluster, IP noteport, load balancer, external name, headless service.
And we are going to look at the ingress  as well as part of the options we can use for service discovery.  11:36

Therefore, if we if we should not deploy our our application as pods, how then should we deploy application, even though application are running as podst?
Should we use pods as an object to deploy k8? Should not be used as an object because we have, what we call k8 objects.
Okay. What are some of the objects that we know?Object number one is pods. Pod is an object?
Okay. Now, recommendation.we shouldn't use pods to deploy applications in k8.
 what should we use?  if you're going to deploy, use controller managers?

So we saw some controller managers. What are some of the controller managers that we have seen, which are part of the k8 object, okay?
Now, if you look at all of this controller my managers like pod for e.g , what is the API version for pod? API version for Pod? Well, what are their API versions for ports?

API version for pod  is V1.
What is the API version for replication? Controller? V1.
What is the API version for replica? Sets.. apps/V1.
What is it API version for daemon set, app/V1,   for deployments, app/V1.

So do you realize that the replica set for pod and replication controllers, for E.G.,  differs from that of replica set?, daemon set n deployment? yes.

And this difference is captured in what we call Selector  support.
for  E.G. replication controller. Which kind of selector does it have?
it Has equality based selector,. So this is like the old version  â€¦.replication controller.- old version 
How about replica set on the others? Which kind of selector do they support Replica set?, daemon set ,  which kind of selector, and  deployment.
Which kind of selector does it have? Now this has what we call both equality base.
 They have two classes of selectors.They have equality Base  selectors What else do they have apart frm  the equality base?
They have equality base. Selectors = key:value .... (key1: value1) ie key1  is equals to value1
it also has set base selectors.
We can hv a situation wher  a particular key  e..g key1 can hv multiple values like: one key is having multiple values like 
 key1:
               values:
                 value1  
                 value2  \
                 value3  

we also spoke abt replica sets ,,  with replication controller, and replica set, if you deploy your application, what happens?
We can scale them.Same with deployment, we can scale them so we can decide to have any number of replicas. So giving k8 support, one as a feature of k8, we hv scaling.
Could we skill in Doctor? No, but in k8, we able to scale, so we can run command like, :
kubectl scale deployment/rc/rs --replicas 10   = manual scaling    .... 19:00
  kubectl scale ds --replicas 10   = .. but we can scale with daemon set bc daemon set ar used to deploy appl which we expect them to be hosted in each node or a selectd 
number of nodes, that when wwe use daemon set.
so wehv seen that in k8 we are able to sclae ... .. and we can also automae scaling
HOW CAN SCALING BE AUTOMATED?



kubernetes
=========
  -- Kubernetes CLUSTER = Group of servers working together - k8s  
  -- k8s -- nodes -- pods[containers]  
pods for deployment:
  -- pods has short lifecycle      

kubernetes features:
   - scaling  
   - self healing capabilities  - dead pods are authomatically restarted 
   - service discovery & load balancing  :
         - ClusterIP / NodePort  / LoadBalancer  / ExternalName / HeadLess Service
         - ingress  

kubernetes objects:
   - pods[v1]  = we shouldn't use pod to deploy applications in kubernetes
                 use controllerManagers to deploy and manage pods/applications 
   - ReplicationControllers[v1]
   - ReplicaSets [apps/v1] 
   - DaemonSets  [apps/v1] 
   - Deployments [apps/v1] 

  selector supports:
   ReplicationControllers - old version = equality based selectors 
   ReplicaSets/DaemonSets/Deployments - selectors:
        - equality based selectors = key1:value1
        - set based selectors:
             key1:
               values:
                 value1  
                 value2  \
                 value3  

  kubectl scale deployment/rc/rs --replicas 10   = manual scaling   
  kubectl scale ds --replicas 10   = manual scaling   

IQ: How can scaling be automated in kubernetes? 
    By installing a guage API [ metric-server ] in our cluster and using 
       1. HorizontalPodAutoscaler[HPA] for pods   
       2. VerticalPodAutoscaler[VPA] for pods 
       3. CUSTER Autoscaler[CAS] for nodes  

metrics-server    
Resources / Limits / Requests / ResourceLimits  
==========================================================
https://github.com/LandmakTechnology/spring-boot-docker
-# Spring App & Mongod DB as POD without volumes
Stateless applications deployment doesn't require to maintain it state   :
  Use deployment as the choice kubernetes object  
  ReplicaSets/ReplicationController:  
Stateful applications maintained their state. Examples are  :
  databases, Jenkins, 

spring:
  data:
    mongodb:
      host: ${MONGO_DB_HOSTNAME}
      port: 27017
      username: ${MONGO_DB_USERNAME}
      password: ${MONGO_DB_PASSWORD}
      database: users
      authentication-database: admin
server:
  port: 8080

   ssh username@hostname [ It will request for the password ]  
    mongodb:
      host: mongo
      port: 27017
      username: devdb  
      password: devdb123  
  ssh devdb@mongo  

mongo database:
===============
  1. use StatefulSets as the choice kubernetes object for Stateful  
     application deployments   
  2. USE ReplicaSets/ReplicationController/deployment StateLESS  
     application deployments   

Stateless
--- springapp.yml/
Stateful
--- mongo.yml/

springapp.yml: kams   
kind: Deployment  
apiVersion: apps/v1   
metadata:
   name: spingapp    
spec:
   replicas: 2  
   selector:
     matchLabels:
       app: spring    
   template:  
      metadata:
         labels:
            app: springapp    
      spec:  
         containers:
         - name: app  
           image: mylandmarktech/spring-boot-mongo  
           ports: 
           - containerPort: 8080
           env:
           - name: MONGO_DB_HOSTNAME  
             value: mongosvc  
           - name: MONGO_DB_USERNAME 
             value: devdb   
           - name: MONGO_DB_PASSWORD
             value: devdb@123   
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080

mongo.yml  = kams:  
=========
kind: ReplicaSet   
apiVersion: apps/v1  
metadata:  
  name: mongors    
spec:
  selector:
    matchLabels:
      app: mongo     
  template:
    metadata:  
      labels:
        app: mongo    
    spec: 
      containers:
      - name: mongodbcontainer    
        image: mongo      
        ports:
        - containerPort: 27017    
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD 
          value: devdb@123
---
kind: Service 
apiVersion:  v1  
metadata:  
   name: mongosvc 
spec:
  selector:
    app: mongo    
  ports:
  - port: 27017 
    targetPort: 27017  

---
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
           env:
           - name: MONGO_DB_HOSTNAME  
             value: mongosvc  
           - name: MONGO_DB_USERNAME 
             value: devdb   
           - name: MONGO_DB_PASSWORD
             value: devdb@123   

1. image: mongo   
2. image: mylandmarktech/spring-boot-mongo     

repository to clone:
1. https://github.com/LandmakTechnology/kubernetes-notes
2. https://github.com/LandmakTechnology/kops-k8s
3. https://github.com/LandmakTechnology/kubernetes-manifests
=========================================================

kubernetes volumes:
===================
mongovol.yml  
kind: ReplicaSet   
apiVersion: apps/v1  
metadata:  
  name: mongors    
spec:
  selector:
    matchLabels:
      app: mongo     
  template:
    metadata:  
      labels:
        app: mongo    
    spec: 
      volumes:
      - name: mongodbhostvol    
        hostPath:
          path: /mongodata  
      containers:
      - name: mongodbcontainer    
        image: mongo      
        ports:
        - containerPort: 27017    
        volumeMounts:
        - name: mongodbhostvol  
          mountPath: /data/db 
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD 
          value: devdb@123

kubectl taint nodes node9 key1=value1:NoSchedule     [taint the node]
kubectl taint nodes node9 key1=value1:NoSchedule-    [untaint the node]

  hostPath volume will store data ONLY on the node where the POD is scheduled
  This can result data lost and data inconsistency 

  NFS 

      volumes:
      - name: mongodbhostvol    
        hostPath:
          path: /mongodata 
      volumes:
      - name: mongodbhostvol    
        nfs:
          path: /mongodata 


awsElasticBlockStore 
azureDisk
azureFile 
configMap 
emptyDir g
cePersistentDisk
gitRepo (deprecated) 
hostPath
nfs 
persistentVolumeClaim 
secret


Configuration of NFS Server
===========================
Step 1: 

Create one Server for NFS

Install NFS Kernel Server in 
Before installing the NFS Kernel server, we need to update our systemâ€™s 
repository index with that of the Internet through the following apt command as sudo:

nfs server: 3.17.179.30  / 10.0.0.76
---------------------
mongo-nfs.yml  
==================
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongors
spec:
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      name: myapp
      labels:
        app: mongo
    spec:
      volumes:
      - name: mongodbvol
        nfs:
          server: 10.0.0.76
          path: /mnt/share
      containers:
      - name: mongodbcontainer
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb@123
        volumeMounts:
        - name: mongodbvol
          mountPath: /data/db


kubectl taint nodes node9 key1=value1:NoSchedule     [taint the node]
kubectl taint nodes node9 key1=value1:NoSchedule-    [untaint the node]

Persistent volumes 
===================

deploy a kubernetes using Kops  


PV --> It's a piece of storage (hostPath, nfs,ebs,azurefile,azuredisk) in k8s cluster. 
PV exists independently from from pod life cycle form which it is consuming.

PersistentVolumeClaim -->
   It's request for storage(Volume).Using PVC we can request(Specify) 
   how much storage u need and with what access mode u need.

Persistent Volumes are provisioned in two ways, Statically or Dynamically.:

1) Static Volumes (Manual Provisionging)
    A k8's Administrator can create a PV manually so that pv's can be available for PODS which requires.
    Create a PVC so that PVC will be attached PV. We can use PVC with PODS to get an access to PV. 

2) Dynamic Volumes (Dynamic Provisioning)
     It's possible to have k8s provision(Create) volumes(PV) as required. 
     Provided we have configured A storageClass [sc].
     So when we create PVC if PV is not available Storage Class will Create PV dynamically.        

PVC: If pod requires access to storage(PV),it will get an access using PVC.
     PVC will be attached to PV.

PersistentVolume â€“ the low level representation of a storage volume.
PersistentVolumeClaim â€“ the binding between a Pod and PersistentVolume.
Pod â€“ a running container that will consume a PersistentVolume.
StorageClass â€“ allows for dynamic provisioning of PersistentVolumes.

PV Will have Access Modes:
============================
ReadWriteOnce=RWO â€“ the volume can be mounted as read-write by a single node = EBS 
ReadOnlyMany=ROM  â€“ the volume can be mounted read-only by many nodes
ReadWriteMany=RWM â€“ the volume can be mounted as read-write by many nodes = NFS 


Claim Policies
================
A Persistent Volume Claim can have several different claim policies associated with it including
RetainÂ â€“Â WhenÂ the claim(PVC) is deleted, the volume(PV) will exists.
RecycleÂ â€“Â When the claim is deleted the volume remains but in a state where the data can be manually recovered.
DeleteÂ â€“Â The persistent volume is deleted when the claim is deleted.

The claim policy (associated at the PV and not the PVC) is responsible for what happens to the data when the claim is deleted.

mongo-pv-hostpath.yml   
---------------------
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/kube"

---
# Mongo db pod with PVC
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
    path: "/kube"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-hostpath
spec:
  storageClassName: manual
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
---
# Mongo db pod with PVC
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongors
spec:
  selector:
    matchLabels:
      app: mongo
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongo
     spec:
       volumes:
       - name: mongodb-pvc
         persistentVolumeClaim:
           claimName: pvc-hostpath
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: mongodb-pvc
           mountPath: /data/db

mongo-pv-pvc-nfs.yml   
===================
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs-pv1
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: <nfs server ip>
    path: "/mnt/share"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nfs-pv1
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Mi
---
pv-pvc-mongo-hostpath.yml  
=========================
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-hostpath
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
    path: "/kube"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-hostpath
spec:
  storageClassName: manual
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
---
pv-pvc-hostpath.yml   
==================
# Mongo db pod with PVC
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs-pv1
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 10.0.0.76
    path: "/mnt/share"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nfs-pv1
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Mi
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongors
spec:
  selector:
    matchLabels:
      app: mongo
  template:
     metadata:
       name: mongodbpod
       labels:
         app: mongo
     spec:
       volumes:
       - name: mongodb-pvc
         persistentVolumeClaim:
           claimName: pvc-nfs-pv1
       containers:
       - name: mongodbcontainer
         image: mongo
         ports:
         - containerPort: 27017
         env:
         - name: MONGO_INITDB_ROOT_USERNAME
           value: devdb
         - name: MONGO_INITDB_ROOT_PASSWORD
           value: devdb@123
         volumeMounts:
         - name: mongodb-pvc
           mountPath: /data/db

kubernetes volumes  :
==========================

















