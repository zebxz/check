
Okay.. So you're able to make a request as far as a resources is  concerned, based on what you are trying to realize. And that's a very key aspect.. That's a very key
aspect in k8, you know. And now we have seen that,  while deploying your pod, right? You can decide how much resources should be assigned to this pod. And if you do that, 
the scheduler  wil only , schedule, that port on, they node  that got sufficient resources to provide what the pod is requesting. So please take notice. Very important.It's
a very key aspect as far as k8 this is concerned. So this aspect is always considered. While Trying to deploy a pod.
 So when the scheduler is about to schedule a pod, hes is going to schedule that pod based on the node that got sufficient resources. That's what is being considered. You
see that. So you are node, should have sufficient resources before the pdt is being scheduled. And k8 must always check, you know, in terms of how much resources the node 
Got before it's able to schedule a port. On that note. So those are very key aspect in k8 to take note of. 

**************************************SCALING:
SCALING
 Now, let's assume we want to deploy an application. We have said one of the features of k8 is that if you deploy your application using a controller, we are able to scale.
Okay. So how do we scale scaling in k8? 
How do we scale?
 we have Manual scaling and automated Scaling
How do we scale manually in k8 If you deployin your application for you to scale manually, you can run the command:
kubectl scale deployment/rs/rc/sts
ie we can scale rs , deployment, rc, stateful set, ...... these ar the scalable objs in k8 and these scaling can be done maually
we can also scale automatically, in k8 using another object Callled HPA .

scaling in kubernetes:
   manual scaling:
      kubectl scale deployment/rs/rc/sts/ app --replicas 4      
   automated scaling:
      Horizontal Pod AutoScaling  - HPA  :
      ============================
POD AutoScaling --> Kuberenets POD AutoScaling Will make sure u have minimum number 
pod replicas available at any time & based on the observed CPU/Memory utilization
on pods, it can scale PODS automatically.
HPA Will Scale up/down pod replicas of Deployment/ReplicaSet/ReplicationController 
based on observerd CPU & Memory utilization base the target specified. 

What is difference b/w Kubernetes AutoScaling(POD AutoScaling) & AWS AutoScaling?
 when we did aws, we also saw how we can auto scale by creating auto scaling grp 
AutoScaling group in aws : so we can have min, desired , max 
     min     = 5          the max
     desired = 5      so these are the no. of servers that we epxect
     max     = 100              max, so it  cant create more than 100 ,,, so scalin policy hS a tARGET, THE target can b memory utilzation or cpu ultilization ...... 8:14

but then when shud it scal? .....  so we hv scaling policy
 scaling policies in aws : it can scale based on 
    memory utilization    ... we can say that if mem exceeds 70% add servers 
       mem > 70% add servers      
    cpu utilization   
       cpu > 70% add servers 

  ScalingPolicy:
     memory utilization
       cpu -gt 80% 
       cpu -lt 40%     
     cpu utilization
difference b/w Kubernetes AutoScaling(POD AutoScaling) 
                   & AWS AutoScaling?
8:50 
for us to be able to understand hw mch resources are been consumed, we nid to check hw much resources the pods are already consuming, similarly, we can do this in k8
we cus decide we wnt to scale our pod..

resources:
   requests:     under resources, a pod is able to reqest for this much 
     memory: "128Mi"   
     cpu: "500m" 
deployment:     lets assume this req is created undera controler manger wich is deployment an dthe deployment has replica as 5
   replicas: 5    if my replica is 5, hw much resources is each of d replica suppose to consume? for memoery 128mi and cpu:500m, thi sis hw much resources a pod can consume
hpa:                 ************* we deploy an hpa and we say min number of pod 5
   min: 5 + 1 =    and say min number of pod is 5      (so we hv a deployment we are deployin and an HPA) 
   max: 50           and max is 50 pod 
                      based on this deploymnt, the HPA wil maintain 5replicas, however if the 5replicas are all using 128 each, it will add anoda pod bc the vol of resources
                       being usedby the pod is much so it will add one pod :+1, it will keep ading and the process wil be automated  .. 12.06

kubectl top pods    : to knw  hw much the pods are consuming , but we dnt hv any operator in aws that is able to check hw much resources our pods are consuming
kubectl top nodes  

error: Metrics API not available     ..... we nid to instal a metric server bc it acts as a guage for example , it wil chaek ur pod to knw hw much cpu is assigned.
    kubernetes addons/plugins:
      Metrics Server

Configure a Metrics Server on our Cluster4??     14:29 .. ..    metric server is a k8 operator or pluggin
===========================================
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml


wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

*********OR CHECK OUR Github repository wher we hv metric server script already 
https://github.com/LandmakTechnology/metric-server
***********to clone the repository:
git clone https://github.com/LandmakTechnology/metric-server
***************** to create a metrics server:
kubectl apply -f metric-server/metrics-server-deploy.yml     
=====================================================

*************** TO deploy the metric server: we v a file, inside metric server ther is :metrics-server-deploy.yml, dis is a file this is going to deploy metric server in the 
                               kubectl namespace..... so it is creating a metrics account for metrics server.this service account wil b used to manage metrics server, also 
                               creating a cluster role for metrics server, using rbac, it wil ceate all this service.
    
ubuntu@master: vi metric-server/metrics-server-deploy.yml   

ubuntu@master:~$ kubectl apply -f metric-server/metrics-server-deploy.yml          **********all this objects were deployed to get the metrics server running
serviceaccount/metrics-server unchanged
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader unchanged
clusterrole.rbac.authorization.k8s.io/system:metrics-server unchanged
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader unchanged
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator unchanged
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server unchanged
service/metrics-server unchanged
deployment.apps/metrics-server configured
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io unchanged

to check if our metrics server is running;
ubuntu@master:  kubectl get po -A
********TO check the metrics of our pod:
ubuntu@master: kubectl top po 


18:36****************so all of this hv been created,der is s deployment for our metric server n this metrics server is functional and and it has been deployed such that it
is able to get into my cluster, we hv given him the permission using RBAC , we are permiting metric server to check all our node, all our pod and report the metrics.. so 
it can gather metrics and if u lok at some of the objects eployed, we v stuff like metric server can check ur node on yr pod 

metrics-server:
  nodes  
  pods 

RBAC objects:
  serviceaccount metrics-server : we can have 
     - user        ......*******so we can create a service account for a user , for a grp or  a pod 
     - groups 
     - pods     
  clusterrole             ************ we create a cluster role that is permoting  certain tasks to b done on pods n nodes
     - pods/nodes [get/watch/list] 
  clusterrolebinding             ....  all of this hv been deployed
     - 
  rolebinding      ....   all of this hv been deployed

****************************************DEPLOYING WITH METRICS SERVER:  ... we wnt to deploy an application that wil include metrics server  ...21:00 
Deployment with HPA
==================
vi hpa.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpadeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      name: hpapod
  template:
    metadata:
      labels:
        name: hpapod
    spec:
      containers:
        - name: hpacontainer
          image: k8s.gcr.io/hpa-example              ******* this is a cutom appl that has a k8 image  ... k8.io 
          ports:
          - name: http
            containerPort: 80
          resources:
            requests:
              cpu: "100m"
              memory: "64Mi"
            limits:
              cpu: "100m"
              memory: "256Mi"
   
******************************************* as part of thsi deployment, we wnt to auto scale the application.... ..... 21:59
---                                                   HW DO WE AUTO scale the application
apiVersion: autoscaling/v2                **************we are deploying  an autoscalin/v2
kind: HorizontalPodAutoscaler  
metadata:
  name: autoapp        
spec:
  scaleTargetRef:
    apiVersion: apps/v1               ************** this must  be the same name as above
    kind: Deployment                   ************** this must  be the same name as above
    name: hpaapp                  ************** this must  be the same name as above
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 40
  - type: Resource
    resource:
     name: memory
     target:
      type: Utilization
      averageUtilization: 40
---                              *************we are creating a srvice for our cluster autoscaler
apiVersion: v1
kind: Service
metadata:
  name: hpaclusterservice
  labels:
    name: hpaservice
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    name: hpapod
  type: ClusterIP
*********************************** to deploy our cluster autoscaler immediately
ubuntu@master:~$ kubectl apply -f hpa.yml                        ........ 25:00
horizontalpodautoscaler.autoscaling/autoapp created        .....  ******* resources created
ubuntu@master:~$ kubectl get hpa
we see hpa running 
ubuntu@master:~$  kubectl get deploy
ubuntu@master:~$ kubectl get po
pods are running 

since the pods are running lets see hw we ar going to the autoscaler is goin to wrk
********************************** so we hv deployed all the objects including the autoscaler
~~~~~~~~~~ next we create/generate a load    .. 25:53 

TO CREATE A LOAD:
-# Create temp POD using below command interatively and increase the 
-# load on demo app by accessing the service.

creatin the pod in interative mode:
ubuntu@master:~$ kubectl run -i --tty load-generator --rm  --image=busybox /bin/sh
If you dnt see  a command prompt, try pressing enter
/#
......the pod has been created, w ewnt to access d service so that we can generate load on this pod

-# Access the service to increase the load.
/# while true; do wget -q -O- http://hpaclusterservice; done     (we ar executin a why lo...**** idnt knw what he said ..
**********.... load is been generted     (it was printing ...OK! OK!)
min replica is 2, max is 5  and now we hv 3replicas ... the replica was created tru an automated process,  this entire process was automated, load has been genertad, this
is a test appl tha k8 provide for u to just run some testing on it.... so tk note thia ia a very imp aspect

TYPES OF AUTOSCALING:   ... WE ILL LOOK AT the rest of the autoscaling as far as k8 is concerned
Vertical Pod AutoScaling : 
Horizontal Pod AutoScaling  :
Cluster AutoScaling:

we will stil hv to cover 
kubernetes = 15 hours   :
   kops / helm  /    
   stateless and stateful applications   
   volumes  
   configMaps and Secrets 
   EFK  / 
   Prometheus and Grafana  
   AMAZON EKS   
   Kubernetes Security
of course probes will be covered as well, wich is health check in k8 , to ensure tins ar workin and nothin is broken


thers a high possiblty that we will cover this , bc ther are stuffs here we nid to knw b4 we get done wit k8
infrastructure and configuration MGT with Terraform and ANsible  , beging frm monday n by the time we ar done wit this we wil cm bk and get done wit k8

Date  Local time  Change

Nov 05, 2023   2:00 am  -1 hour to standard time
Days: Mondays/Tuesdays/Saturdays  
Times: 7pm - 11pm EST Toronto/NewYork  Time     
Nigeria/Cameroon: 1am - 5am   
UK: 12am - 4am   




