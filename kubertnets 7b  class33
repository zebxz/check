
Okay.. So you're able to make a request as far as a resources is  concerned, based on what you are trying to realize. And that's a very key aspect.. That's a very key
aspect in k8, you know. And now we have seen that,  while deploying your pod, right? You can decide how much resources should be assigned to this pod. And if you do that, 
the scheduler  wil only , schedule, that port on, they node  that got sufficient resources to provide what the pod is requesting. So please take notice. Very important.It's
a very key aspect as far as k8 this is concerned. So this aspect is always considered. While Trying to deploy a pod.
 So when the scheduler is about to schedule a pod, hes is going to schedule that pod based on the node that got sufficient resources. That's what is being considered. You
see that. So you are node, should have sufficient resources before the pdt is being scheduled. And k8 must always check, you know, in terms of how much resources the node 
Got before it's able to schedule a port. On that note. So those are very key aspect in k8 to take note of. 

**************************************SCALING:
SCALING
 Now, let's assume we want to deploy an application. We have said one of the features of k8 is that if you deploy your application using a controller, we are able to scale.
Okay. So how do we scale scaling in k8? 
How do we scale?
 we have Manual scaling and automated Scaling
How do we scale manually in k8 If you deployin your application for you to scale manually, you can run the command:
kubectl scale deployment/rs/rc/sts
ie we can scale rs , deployment, rc, stateful set, ...... these ar the scalable objs in k8

scaling in kubernetes:
   manual scaling:
      kubectl scale deployment/rs/rc/sts/ app --replicas 4      
   automated scaling:
      Horizontal Pod AutoScaling  - HPA  :
      ============================
POD AutoScaling --> Kuberenets POD AutoScaling Will make sure u have minimum number 
pod replicas available at any time & based on the observed CPU/Memory utilization
on pods it can scale PODS.
HPA Will Scale up/down pod replicas of Deployment/ReplicaSet/ReplicationController 
based on observerd CPU & Memory utilization base the target specified. 

What is difference b/w Kubernetes AutoScaling(POD AutoScaling) & AWS AutoScaling?
  AutoScaling group in aws :
     min     = 5
     desired = 5 
     max     = 100
 
 scaling policies aws :
    memory utilization   
       mem > 70% add servers      
    cpu utilization   
       cpu > 70% add servers 

  ScalingPolicy:
     memory utilization
       cpu -gt 80% 
       cpu -lt 40%     
     cpu utilization
difference b/w Kubernetes AutoScaling(POD AutoScaling) 
                   & AWS AutoScaling?


resources:
   requests:
     memory: "128Mi"
     cpu: "500m" 
deployment:
   replicas: 5  
hpa:
   min: 5 + 1 =   
   max: 50   
kubectl top pods  
kubectl top nodes  

error: Metrics API not available
    kubernetes addons/plugins:
      Metrics Server

Configure a Metrics Server on our Cluster4??
===========================================
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml


wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml


https://github.com/LandmakTechnology/metric-server
git clone https://github.com/LandmakTechnology/metric-server
kubectl apply -f metric-server/metrics-server-deploy.yml
=====================================================

ubuntu@master:
~$ kubectl apply -f metric-server/metrics-server-deploy.yml
serviceaccount/metrics-server unchanged
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader unchanged
clusterrole.rbac.authorization.k8s.io/system:metrics-server unchanged
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader unchanged
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator unchanged
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server unchanged
service/metrics-server unchanged
deployment.apps/metrics-server configured
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io unchanged

metrics-server:
  nodes  
  pods 

RBAC objects:
  serviceaccount metrics-server 
     - user  
     - groups 
     - pods     
  clusterrole
     - pods/nodes [get/watch/list] 
  clusterrolebinding
     - 
  rolebinding
Deployment with HPA
==================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpadeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      name: hpapod
  template:
    metadata:
      labels:
        name: hpapod
    spec:
      containers:
        - name: hpacontainer
          image: k8s.gcr.io/hpa-example
          ports:
          - name: http
            containerPort: 80
          resources:
            requests:
              cpu: "100m"
              memory: "64Mi"
            limits:
              cpu: "100m"
              memory: "256Mi"

---
apiVersion: autoscaling/v2 
kind: HorizontalPodAutoscaler  
metadata:
  name: autoapp        
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment 
    name: hpaapp  
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 40
  - type: Resource
    resource:
     name: memory
     target:
      type: Utilization
      averageUtilization: 40
---
apiVersion: v1
kind: Service
metadata:
  name: hpaclusterservice
  labels:
    name: hpaservice
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    name: hpapod
  type: ClusterIP

-# Create temp POD using below command interatively and increase the 
-# load on demo app by accessing the service.

kubectl run -i --tty load-generator --rm  --image=busybox /bin/sh

-# Access the service to increase the load.

while true; do wget -q -O- http://hpaclusterservice; done  

Vertical Pod AutoScaling : 
Horizontal Pod AutoScaling  :
Cluster AutoScaling:


kubernetes = 15 hours   :
   kops / helm  /    
   stateless and stateful applications   
   volumes  
   configMaps and Secrets 
   EFK  / 
   Prometheus and Grafana  
   AMAZON EKS   
   Kubernetes Security     


Date  Local time  Change

Nov 05, 2023   2:00 am  -1 hour to standard time
Days: Mondays/Tuesdays/Saturdays  
Times: 7pm - 11pm EST Toronto/NewYork  Time     
Nigeria/Cameroon: 1am - 5am   
UK: 12am - 4am   




